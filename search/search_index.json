{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+","tags":false},"docs":[{"location":"","text":"<p>Welcome to galexrt's docs!</p> <p>Tips, tricks, cheat sheets and documentation treasure trove chest for everyone to read, learn and improve.</p>  <p>This work is licensed under a Creative Commons Attribution 4.0 International License.</p> <p></p>  <p>Use the information on this page however you want, just remember no guarantees given.</p>","title":"Home"},{"location":"privacy-policy/","text":"<p>The Site Notice / Impressum can be found here: Site Notice / Impressum page.</p> <p>German version of the Privacy Policy / Datenschutzerkl\u00e4rung below.</p>","title":"Privacy Policy"},{"location":"privacy-policy/#privacy-policy","text":"","title":"Privacy Policy"},{"location":"privacy-policy/#1-an-overview-of-data-protection","text":"","title":"1. An overview of data protection"},{"location":"privacy-policy/#general-information","text":"<p>The following information will provide you with an easy to navigate overview of what will happen with your personal data when you visit this website. The term \"personal data\" comprises all data that can be used to personally identify you. For detailed information about the subject matter of data protection, please consult our Data Protection Declaration, which we have included beneath this copy.</p>","title":"General information"},{"location":"privacy-policy/#data-recording-on-this-website","text":"","title":"Data recording on this website"},{"location":"privacy-policy/#who-is-the-responsible-party-for-the-recording-of-data-on-this-website-ie-the-controller","text":"<p>The data on this website is processed by the operator of the website, whose contact information is available under section \"Information Required by Law\" on this website.</p>","title":"Who is the responsible party for the recording of data on this website (i.e. the \"controller\")?"},{"location":"privacy-policy/#how-do-we-record-your-data","text":"<p>We collect your data as a result of your sharing of your data with us. This may, for instance be information you enter into our contact form.</p> <p>Other data shall be recorded by our IT systems automatically or after you consent to its recording during your website visit. This data comprises primarily technical information (e.g. web browser, operating system or time the site was accessed). This information is recorded automatically when you access this website.</p>","title":"How do we record your data?"},{"location":"privacy-policy/#what-are-the-purposes-we-use-your-data-for","text":"<p>A portion of the information is generated to guarantee the error free provision of the website. Other data may be used to analyze your user patterns.</p>","title":"What are the purposes we use your data for?"},{"location":"privacy-policy/#what-rights-do-you-have-as-far-as-your-information-is-concerned","text":"<p>You have the right to receive information about the source, recipients and purposes of your archived personal data at any time without having to pay a fee for such disclosures. You also have the right to demand that your data are rectified or eradicated. If you have consented to data processing, you have the option to revoke this consent at any time, which shall affect all future data processing. Moreover, you have the right to demand that the processing of your data be restricted under certain circumstances. Furthermore, you have the right to log a complaint with the competent supervising agency.</p> <p>Please do not hesitate to contact us at any time under the address disclosed in section \"Information Required by Law\" on this website if you have questions about this or any other data protection related issues.</p>","title":"What rights do you have as far as your information is concerned?"},{"location":"privacy-policy/#analysis-tools-and-tools-provided-by-third-parties","text":"<p>There is a possibility that your browsing patterns will be statistically analyzed when your visit this website. Such analyses are performed primarily with what we refer to as analysis programs.</p> <p>For detailed information about these analysis programs please consult our Data Protection Declaration below.</p>","title":"Analysis tools and tools provided by third parties"},{"location":"privacy-policy/#2-hosting-and-content-delivery-networks-cdn","text":"","title":"2. Hosting and Content Delivery Networks (CDN)"},{"location":"privacy-policy/#external-hosting","text":"<p>This website is hosted by an external service provider (host). Personal data collected on this website are stored on the servers of the host. These may include, but are not limited to, IP addresses, contact requests, metadata and communications, contract information, contact information, names, web page access, and other data generated through a web site.</p> <p>The host is used for the purpose of fulfilling the contract with our potential and existing customers (Art. 6 para. 1 lit. b GDPR) and in the interest of secure, fast and efficient provision of our online services by a professional provider (Art. 6 para. 1 lit. f GDPR).</p> <p>Our host will only process your data to the extent necessary to fulfil its performance obligations and to follow our instructions with respect to such data.</p> <p>We are using the following host:</p> <p>Cloudflare Inc. 101 Townsend St, San Francisco, CA 94107 USA</p> <p>GitHub Inc. 88 Colin P Kelly Jr St, San Francisco, CA 94107 United States</p> <p>Hetzner Online GmbH Industriestr. 25 91710 Gunzenhausen Germany</p>","title":"External Hosting"},{"location":"privacy-policy/#execution-of-a-contract-data-processing-agreement","text":"<p>In order to guarantee processing in compliance with data protection regulations, we have concluded an order processing contract with our host.</p>","title":"Execution of a contract data processing agreement"},{"location":"privacy-policy/#cloudflare","text":"<p>We use the \"Cloudflare\" service provided by Cloudflare Inc., 101 Townsend St., San Francisco, CA 94107, USA. (hereinafter referred to as \"Cloudflare\").</p> <p>Cloudflare offers a content delivery network with DNS that is available worldwide. As a result, the information transfer that occurs between your browser and our website is technically routed via Cloudflare\u2019s network. This enables Cloudflare to analyze data transactions between your browser and our website and to work as a filter between our servers and potentially malicious data traffic from the Internet. In this context, Cloudflare may also use cookies or other technologies deployed to recognize Internet users, which shall, however, only be used for the herein described purpose.</p> <p>The use of Cloudflare is based on our legitimate interest in a provision of our website offerings that is as error free and secure as possible (Art. 6 Sect. 1 lit. f GDPR).</p> <p>For more information on Cloudflare\u2019s security precautions and data privacy policies, please follow this link: https://www.cloudflare.com/privacypolicy/.</p>","title":"Cloudflare"},{"location":"privacy-policy/#3-general-information-and-mandatory-information","text":"","title":"3. General information and mandatory information"},{"location":"privacy-policy/#data-protection","text":"<p>The operators of this website and its pages take the protection of your personal data very seriously. Hence, we handle your personal data as confidential information and in compliance with the statutory data protection regulations and this Data Protection Declaration.</p> <p>Whenever you use this website, a variety of personal information will be collected. Personal data comprises data that can be used to personally identify you. This Data Protection Declaration explains which data we collect as well as the purposes we use this data for. It also explains how, and for which purpose the information is collected.</p> <p>We herewith advise you that the transmission of data via the Internet (i.e. through e-mail communications) may be prone to security gaps. It is not possible to completely protect data against third-party access.</p>","title":"Data protection"},{"location":"privacy-policy/#information-about-the-responsible-party-referred-to-as-the-controller-in-the-gdpr","text":"<p>The data processing controller on this website is:</p>  Alexander Trost Postfach 11 75196 K\u00e4mpfelbach Germany  <p>E-mail: me@galexrt.moe</p> <p>The controller is the natural person or legal entity that single-handedly or jointly with others makes decisions as to the purposes of and resources for the processing of personal data (e.g. names, e-mail addresses, etc.).</p>","title":"Information about the responsible party (referred to as the \"controller\" in the GDPR)"},{"location":"privacy-policy/#storage-duration","text":"<p>Unless a more specific storage period has been specified in this privacy policy, your personal data will remain with us until the purpose for which it was collected no longer applies. If you assert a justified request for deletion or revoke your consent to data processing, your data will be deleted, unless we have other legally permissible reasons for storing your personal data (e.g. tax or commercial law retention periods); in the latter case, the deletion will take place after these reasons cease to apply.</p>","title":"Storage duration"},{"location":"privacy-policy/#information-on-data-transfer-to-the-usa","text":"<p>Our website uses, in particular, tools from companies based in the USA. When these tools are active, your personal information may be transferred to the US servers of these companies. We must point out that the USA is not a safe third country within the meaning of EU data protection law. US companies are required to release personal data to security authorities without you as the data subject being able to take legal action against this. The possibility cannot therefore be excluded that US authorities (e.g. secret services) may process, evaluate and permanently store your data on US servers for monitoring purposes.\u00a0 We have no influence over these processing activities.</p>","title":"Information on data transfer to the USA"},{"location":"privacy-policy/#revocation-of-your-consent-to-the-processing-of-data","text":"<p>A wide range of data processing transactions are possible only subject to your express consent. You can also revoke at any time any consent you have already given us. This shall be without prejudice to the lawfulness of any data collection that occurred prior to your revocation.</p>","title":"Revocation of your consent to the processing of data"},{"location":"privacy-policy/#right-to-object-to-the-collection-of-data-in-special-cases-right-to-object-to-direct-advertising-art-21-gdpr","text":"<p>IN THE EVENT THAT DATA ARE PROCESSED ON THE BASIS OF ART. 6 SECT. 1 LIT. E OR F GDPR, YOU HAVE THE RIGHT TO AT ANY TIME OBJECT TO THE PROCESSING OF YOUR PERSONAL DATA BASED ON GROUNDS ARISING FROM YOUR UNIQUE SITUATION. THIS ALSO APPLIES TO ANY PROFILING BASED ON THESE PROVISIONS. TO DETERMINE THE LEGAL BASIS, ON WHICH ANY PROCESSING OF DATA IS BASED, PLEASE CONSULT THIS DATA PROTECTION DECLARATION. IF YOU LOG AN OBJECTION, WE WILL NO LONGER PROCESS YOUR AFFECTED PERSONAL DATA, UNLESS WE ARE IN A POSITION TO PRESENT COMPELLING PROTECTION WORTHY GROUNDS FOR THE PROCESSING OF YOUR DATA, THAT OUTWEIGH YOUR INTERESTS, RIGHTS AND FREEDOMS OR IF THE PURPOSE OF THE PROCESSING IS THE CLAIMING, EXERCISING OR DEFENCE OF LEGAL ENTITLEMENTS (OBJECTION PURSUANT TO ART. 21 SECT. 1 GDPR).</p> <p>IF YOUR PERSONAL DATA IS BEING PROCESSED IN ORDER TO ENGAGE IN DIRECT ADVERTISING, YOU HAVE THE RIGHT TO AT ANY TIME OBJECT TO THE PROCESSING OF YOUR AFFECTED PERSONAL DATA FOR THE PURPOSES OF SUCH ADVERTISING. THIS ALSO APPLIES TO PROFILING TO THE EXTENT THAT IT IS AFFILIATED WITH SUCH DIRECT ADVERTISING. IF YOU OBJECT, YOUR PERSONAL DATA WILL SUBSEQUENTLY NO LONGER BE USED FOR DIRECT ADVERTISING PURPOSES (OBJECTION PURSUANT TO ART. 21 SECT. 2 GDPR).</p>","title":"Right to object to the collection of data in special cases; right to object to direct advertising (Art. 21 GDPR)"},{"location":"privacy-policy/#right-to-log-a-complaint-with-the-competent-supervisory-agency","text":"<p>In the event of violations of the GDPR, data subjects are entitled to log a complaint with a supervisory agency, in particular in the member state where they usually maintain their domicile, place of work or at the place where the alleged violation occurred. The right to log a complaint is in effect regardless of any other administrative or court proceedings available as legal recourses.</p>","title":"Right to log a complaint with the competent supervisory agency"},{"location":"privacy-policy/#right-to-data-portability","text":"<p>You have the right to demand that we hand over any data we automatically process on the basis of your consent or in order to fulfil a contract be handed over to you or a third party in a commonly used, machine readable format. If you should demand the direct transfer of the data to another controller, this will be done only if it is technically feasible.</p>","title":"Right to data portability"},{"location":"privacy-policy/#ssl-andor-tls-encryption","text":"<p>For security reasons and to protect the transmission of confidential content, such as purchase orders or inquiries you submit to us as the website operator, this website uses either an SSL or a TLS encryption program. You can recognize an encrypted connection by checking whether the address line of the browser switches from \"http://\" to \"https://\" and also by the appearance of the lock icon in the browser line.</p> <p>If the SSL or TLS encryption is activated, data you transmit to us cannot be read by third parties.</p>","title":"SSL and/or TLS encryption"},{"location":"privacy-policy/#information-about-rectification-and-eradication-of-data","text":"<p>Within the scope of the applicable statutory provisions, you have the right to at any time demand information about your archived personal data, their source and recipients as well as the purpose of the processing of your data. You may also have a right to have your data rectified or eradicated. If you have questions about this subject matter or any other questions about personal data, please do not hesitate to contact us at any time at the address provided in section \"Information Required by Law.\"</p>","title":"Information about, rectification and eradication of data"},{"location":"privacy-policy/#right-to-demand-processing-restrictions","text":"<p>You have the right to demand the imposition of restrictions as far as the processing of your personal data is concerned. To do so, you may contact us at any time at the address provided in section \"Information Required by Law.\" The right to demand restriction of processing applies in the following cases:</p> <ul> <li>In the event that you should dispute the correctness of your data archived by us, we will usually need some time to verify this claim. During the time that this investigation is ongoing, you have the right to demand that we restrict the processing of your personal data.</li> <li>If the processing of your personal data was/is conducted in an unlawful manner, you have the option to demand the restriction of the processing of your data in lieu of demanding the eradication of this data.</li> <li>If we do not need your personal data any longer and you need it to exercise, defend or claim legal entitlements, you have the right to demand the restriction of the processing of your personal data instead of its eradication.</li> <li>If you have raised an objection pursuant to Art. 21 Sect. 1 GDPR, your rights and our rights will have to be weighed against each other. As long as it has not been determined whose interests prevail, you have the right to demand a restriction of the processing of your personal data.</li> </ul> <p>If you have restricted the processing of your personal data, these data \u2013 with the exception of their archiving \u2013 may be processed only subject to your consent or to claim, exercise or defend legal entitlements or to protect the rights of other natural persons or legal entities or for important public interest reasons cited by the European Union or a member state of the EU.</p>","title":"Right to demand processing restrictions"},{"location":"privacy-policy/#4-recording-of-data-on-this-website","text":"","title":"4. Recording of data on this website"},{"location":"privacy-policy/#cookies","text":"<p>Our websites and pages use what the industry refers to as \"cookies.\" Cookies are small text files that do not cause any damage to your device. They are either stored temporarily for the duration of a session (session cookies) or they are permanently archived on your device (permanent cookies). Session cookies are automatically deleted once you terminate your visit. Permanent cookies remain archived on your device until you actively delete them or they are automatically eradicated by your web browser.</p> <p>In some cases, it is possible that third-party cookies are stored on your device once you enter our site (third-party cookies). These cookies enable you or us to take advantage of certain services offered by the third party (e.g. cookies for the processing of payment services).</p> <p>Cookies have a variety of functions. Many cookies are technically essential since certain website functions would not work in the absence of the cookies (e.g. the shopping cart function or the display of videos). The purpose of other cookies may be the analysis of user patterns or the display of promotional messages.</p> <p>Cookies, which are required for the performance of electronic communication transactions (required cookies) or for the provision of certain functions you want to use (functional cookies, e.g. for the shopping cart function) or those that are necessary for the optimization of the website (e.g. cookies that provide measurable insights into the web audience), shall be stored on the basis of Art. 6 Sect. 1 lit. f GDPR, unless a different legal basis is cited. The operator of the website has a legitimate interest in the storage of cookies to ensure the technically error free and optimized provision of the operator\u2019s services. If your consent to the storage of the cookies has been requested, the respective cookies are stored exclusively on the basis of the consent obtained (Art. 6 Sect. 1 lit. a GDPR); this consent may be revoked at any time.</p> <p>You have the option to set up your browser in such a manner that you will be notified any time cookies are placed and to permit the acceptance of cookies only in specific cases. You may also exclude the acceptance of cookies in certain cases or in general or activate the delete function for the automatic eradication of cookies when the browser closes. If cookies are deactivated, the functions of this website may be limited.</p> <p>In the event that third-party cookies are used or if cookies are used for analytical purposes, we will separately notify you in conjunction with this Data Protection Policy and, if applicable, ask for your consent.</p>","title":"Cookies"},{"location":"privacy-policy/#server-log-files","text":"<p>The provider of this website and its pages automatically collects and stores information in so-called server log files, which your browser communicates to us automatically. The information comprises:</p> <ul> <li>The type and version of browser used</li> <li>The used operating system</li> <li>Referrer URL</li> <li>The hostname of the accessing computer</li> <li>The time of the server inquiry</li> <li>The IP address</li> </ul> <p>This data is not merged with other data sources.</p> <p>This data is recorded on the basis of Art. 6 Sect. 1 lit. f GDPR. The operator of the website has a legitimate interest in the technically error free depiction and the optimization of the operator\u2019s website. In order to achieve this, server log files must be recorded.</p>","title":"Server log files"},{"location":"privacy-policy/#contact-form","text":"<p>If you submit inquiries to us via our contact form, the information provided in the contact form as well as any contact information provided therein will be stored by us in order to handle your inquiry and in the event that we have further questions. We will not share this information without your consent.</p> <p>The processing of these data is based on Art. 6 para. 1 lit. b GDPR, if your request is related to the execution of a contract or if it is necessary to carry out pre-contractual measures. In all other cases the processing is based on our legitimate interest in the effective processing of the requests addressed to us (Art. 6 Para. 1 lit. f GDPR) or on your agreement (Art. 6 Para. 1 lit. a GDPR) if this has been requested.</p> <p>The information you have entered into the contact form shall remain with us until you ask us to eradicate the data, revoke your consent to the archiving of data or if the purpose for which the information is being archived no longer exists (e.g. after we have concluded our response to your inquiry). This shall be without prejudice to any mandatory legal provisions \u2013 in particular retention periods.</p>","title":"Contact form"},{"location":"privacy-policy/#request-by-e-mail-telephone-or-fax","text":"<p>If you contact us by e-mail, telephone or fax, your request, including all resulting personal data (name, request) will be stored and processed by us for the purpose of processing your request. We do not pass these data on without your consent.</p> <p>These data are processed on the basis of Art. 6 Sect. 1 lit. b GDPR if your inquiry is related to the fulfillment of a contract or is required for the performance of pre-contractual measures. In all other cases, the data are processed on the basis of our legitimate interest in the effective handling of inquiries submitted to us (Art. 6 Sect. 1 lit. f GDPR) or on the basis of your consent (Art. 6 Sect. 1 lit. a GDPR) if it has been obtained.</p> <p>The data sent by you to us via contact requests remain with us until you request us to delete, revoke your consent to the storage or the purpose for the data storage lapses (e.g. after completion of your request). Mandatory statutory provisions - in particular statutory retention periods - remain unaffected.</p>","title":"Request by e-mail, telephone or fax"},{"location":"privacy-policy/#datenschutzerklarung","text":"","title":"Datenschutz\u00aderkl\u00e4rung"},{"location":"privacy-policy/#1-datenschutz-auf-einen-blick","text":"","title":"1. Datenschutz auf einen Blick"},{"location":"privacy-policy/#allgemeine-hinweise","text":"<p>Die folgenden Hinweise geben einen einfachen \u00dcberblick dar\u00fcber, was mit Ihren personenbezogenen Daten passiert, wenn Sie diese Website besuchen. Personenbezogene Daten sind alle Daten, mit denen Sie pers\u00f6nlich identifiziert werden k\u00f6nnen. Ausf\u00fchrliche Informationen zum Thema Datenschutz entnehmen Sie unserer unter diesem Text aufgef\u00fchrten Datenschutzerkl\u00e4rung.</p>","title":"Allgemeine Hinweise"},{"location":"privacy-policy/#datenerfassung-auf-dieser-website","text":"","title":"Datenerfassung auf dieser Website"},{"location":"privacy-policy/#wer-ist-verantwortlich-fur-die-datenerfassung-auf-dieser-website","text":"<p>Die Datenverarbeitung auf dieser Website erfolgt durch den Websitebetreiber. Dessen Kontaktdaten k\u00f6nnen Sie dem Impressum dieser Website entnehmen.</p>","title":"Wer ist verantwortlich f\u00fcr die Datenerfassung auf dieser Website?"},{"location":"privacy-policy/#wie-erfassen-wir-ihre-daten","text":"<p>Ihre Daten werden zum einen dadurch erhoben, dass Sie uns diese mitteilen. Hierbei kann es sich z.\u00a0B. um Daten handeln, die Sie in ein Kontaktformular eingeben.</p> <p>Andere Daten werden automatisch oder nach Ihrer Einwilligung beim Besuch der Website durch unsere IT-Systeme erfasst. Das sind vor allem technische Daten (z.\u00a0B. Internetbrowser, Betriebssystem oder Uhrzeit des Seitenaufrufs). Die Erfassung dieser Daten erfolgt automatisch, sobald Sie diese Website betreten.</p>","title":"Wie erfassen wir Ihre Daten?"},{"location":"privacy-policy/#wofur-nutzen-wir-ihre-daten","text":"<p>Ein Teil der Daten wird erhoben, um eine fehlerfreie Bereitstellung der Website zu gew\u00e4hrleisten. Andere Daten k\u00f6nnen zur Analyse Ihres Nutzerverhaltens verwendet werden.</p>","title":"Wof\u00fcr nutzen wir Ihre Daten?"},{"location":"privacy-policy/#welche-rechte-haben-sie-bezuglich-ihrer-daten","text":"<p>Sie haben jederzeit das Recht, unentgeltlich Auskunft \u00fcber Herkunft, Empf\u00e4nger und Zweck Ihrer gespeicherten personenbezogenen Daten zu erhalten. Sie haben au\u00dferdem ein Recht, die Berichtigung oder L\u00f6schung dieser Daten zu verlangen. Wenn Sie eine Einwilligung zur Datenverarbeitung erteilt haben, k\u00f6nnen Sie diese Einwilligung jederzeit f\u00fcr die Zukunft widerrufen. Au\u00dferdem haben Sie das Recht, unter bestimmten Umst\u00e4nden die Einschr\u00e4nkung der Verarbeitung Ihrer personenbezogenen Daten zu verlangen. Des Weiteren steht Ihnen ein Beschwerderecht bei der zust\u00e4ndigen Aufsichtsbeh\u00f6rde zu.</p> <p>Hierzu sowie zu weiteren Fragen zum Thema Datenschutz k\u00f6nnen Sie sich jederzeit unter der im Impressum angegebenen Adresse an uns wenden.</p>","title":"Welche Rechte haben Sie bez\u00fcglich Ihrer Daten?"},{"location":"privacy-policy/#analyse-tools-und-tools-von-drittanbietern","text":"<p>Beim Besuch dieser Website kann Ihr Surf-Verhalten statistisch ausgewertet werden. Das geschieht vor allem mit sogenannten Analyseprogrammen.</p> <p>Detaillierte Informationen zu diesen Analyseprogrammen finden Sie in der folgenden Datenschutzerkl\u00e4rung.</p>","title":"Analyse-Tools und Tools von Dritt\u00adanbietern"},{"location":"privacy-policy/#2-hosting-und-content-delivery-networks-cdn","text":"","title":"2. Hosting und Content Delivery Networks (CDN)"},{"location":"privacy-policy/#externes-hosting","text":"<p>Diese Website wird bei einem externen Dienstleister gehostet (Hoster). Die personenbezogenen Daten, die auf dieser Website erfasst werden, werden auf den Servern des Hosters gespeichert. Hierbei kann es sich v. a. um IP-Adressen, Kontaktanfragen, Meta- und Kommunikationsdaten, Vertragsdaten, Kontaktdaten, Namen, Websitezugriffe und sonstige Daten, die \u00fcber eine Website generiert werden, handeln.</p> <p>Der Einsatz des Hosters erfolgt zum Zwecke der Vertragserf\u00fcllung gegen\u00fcber unseren potenziellen und bestehenden Kunden (Art. 6 Abs. 1 lit. b DSGVO) und im Interesse einer sicheren, schnellen und effizienten Bereitstellung unseres Online-Angebots durch einen professionellen Anbieter (Art. 6 Abs. 1 lit. f DSGVO).</p> <p>Unser Hoster wird Ihre Daten nur insoweit verarbeiten, wie dies zur Erf\u00fcllung seiner Leistungspflichten erforderlich ist und unsere Weisungen in Bezug auf diese Daten befolgen.</p> <p>Wir setzen folgenden Hoster ein:</p> <p>Cloudflare Inc. 101 Townsend St, San Francisco, CA 94107 USA</p> <p>GitHub Inc. 88 Colin P Kelly Jr St, San Francisco, CA 94107 United States</p> <p>Hetzner Online GmbH Industriestr. 25 91710 Gunzenhausen Germany</p>","title":"Externes Hosting"},{"location":"privacy-policy/#abschluss-eines-vertrages-uber-auftragsverarbeitung","text":"<p>Um die datenschutzkonforme Verarbeitung zu gew\u00e4hrleisten, haben wir einen Vertrag \u00fcber Auftragsverarbeitung mit unserem Hoster geschlossen.</p>","title":"Abschluss eines Vertrages \u00fcber Auftragsverarbeitung"},{"location":"privacy-policy/#cloudflare_1","text":"<p>Wir nutzen den Service \u201eCloudflare\". Anbieter ist die Cloudflare Inc., 101 Townsend St., San Francisco, CA 94107, USA (im Folgenden \u201eCloudflare\").</p> <p>Cloudflare bietet ein weltweit verteiltes Content Delivery Network mit DNS an. Dabei wird technisch der Informationstransfer zwischen Ihrem Browser und unserer Website \u00fcber das Netzwerk von Cloudflare geleitet. Das versetzt Cloudflare in die Lage, den Datenverkehr zwischen Ihrem Browser und unserer Website zu analysieren und als Filter zwischen unseren Servern und potenziell b\u00f6sartigem Datenverkehr aus dem Internet zu dienen. Hierbei kann Cloudflare auch Cookies oder sonstige Technologien zur Wiedererkennung von Internetnutzern einsetzen, die jedoch allein zum hier beschriebenen Zweck verwendet werden.</p> <p>Der Einsatz von Cloudflare beruht auf unserem berechtigten Interesse an einer m\u00f6glichst fehlerfreien und sicheren Bereitstellung unseres Webangebotes (Art. 6 Abs. 1 lit. f DSGVO).</p> <p>Weitere Informationen zum Thema Sicherheit und Datenschutz bei Cloudflare finden Sie hier: https://www.cloudflare.com/privacypolicy/.</p>","title":"Cloudflare"},{"location":"privacy-policy/#3-allgemeine-hinweise-und-pflichtinformationen","text":"","title":"3. Allgemeine Hinweise und Pflicht\u00adinformationen"},{"location":"privacy-policy/#datenschutz","text":"<p>Die Betreiber dieser Seiten nehmen den Schutz Ihrer pers\u00f6nlichen Daten sehr ernst. Wir behandeln Ihre personenbezogenen Daten vertraulich und entsprechend der gesetzlichen Datenschutzvorschriften sowie dieser Datenschutzerkl\u00e4rung.</p> <p>Wenn Sie diese Website benutzen, werden verschiedene personenbezogene Daten erhoben. Personenbezogene Daten sind Daten, mit denen Sie pers\u00f6nlich identifiziert werden k\u00f6nnen. Die vorliegende Datenschutzerkl\u00e4rung erl\u00e4utert, welche Daten wir erheben und wof\u00fcr wir sie nutzen. Sie erl\u00e4utert auch, wie und zu welchem Zweck das geschieht.</p> <p>Wir weisen darauf hin, dass die Daten\u00fcbertragung im Internet (z.\u00a0B. bei der Kommunikation per E-Mail) Sicherheitsl\u00fccken aufweisen kann. Ein l\u00fcckenloser Schutz der Daten vor dem Zugriff durch Dritte ist nicht m\u00f6glich.</p>","title":"Datenschutz"},{"location":"privacy-policy/#hinweis-zur-verantwortlichen-stelle","text":"<p>Die verantwortliche Stelle f\u00fcr die Datenverarbeitung auf dieser Website ist:</p>  Alexander Trost Postfach 11 75196 K\u00e4mpfelbach Germany  <p>E-Mail: me@galexrt.moe</p> <p>Verantwortliche Stelle ist die nat\u00fcrliche oder juristische Person, die allein oder gemeinsam mit anderen \u00fcber die Zwecke und Mittel der Verarbeitung von personenbezogenen Daten (z.\u00a0B. Namen, E-Mail-Adressen o. \u00c4.) entscheidet.</p>","title":"Hinweis zur verantwortlichen Stelle"},{"location":"privacy-policy/#speicherdauer","text":"<p>Soweit innerhalb dieser Datenschutzerkl\u00e4rung keine speziellere Speicherdauer genannt wurde, verbleiben Ihre personenbezogenen Daten bei uns, bis der Zweck f\u00fcr die Datenverarbeitung entf\u00e4llt. Wenn Sie ein berechtigtes L\u00f6schersuchen geltend machen oder eine Einwilligung zur Datenverarbeitung widerrufen, werden Ihre Daten gel\u00f6scht, sofern wir keinen anderen rechtlich zul\u00e4ssigen\u00a0 Gr\u00fcnde f\u00fcr die Speicherung Ihrer personenbezogenen Daten haben (z.B. steuer- oder handelsrechtliche Aufbewahrungsfristen); im letztgenannten Fall erfolgt die L\u00f6schung nach Fortfall dieser Gr\u00fcnde.</p>","title":"Speicherdauer"},{"location":"privacy-policy/#hinweis-zur-datenweitergabe-in-die-usa","text":"<p>Auf unserer Website sind unter anderem Tools von Unternehmen mit Sitz in den USA eingebunden. Wenn diese Tools aktiv sind, k\u00f6nnen Ihre personenbezogenen Daten an die US-Server der jeweiligen Unternehmen weitergegeben werden. Wir weisen darauf hin, dass die USA kein sicherer Drittstaat im Sinne des EU-Datenschutzrechts sind. US-Unternehmen sind dazu verpflichtet, personenbezogene Daten an Sicherheitsbeh\u00f6rden herauszugeben, ohne dass Sie als Betroffener hiergegen gerichtlich vorgehen k\u00f6nnten. Es kann daher nicht ausgeschlossen werden, dass US-Beh\u00f6rden (z.B. Geheimdienste) Ihre auf US-Servern befindlichen Daten zu \u00dcberwachungszwecken verarbeiten, auswerten und dauerhaft speichern. Wir haben auf diese Verarbeitungst\u00e4tigkeiten keinen Einfluss.</p>","title":"Hinweis zur Datenweitergabe in die USA"},{"location":"privacy-policy/#widerruf-ihrer-einwilligung-zur-datenverarbeitung","text":"<p>Viele Datenverarbeitungsvorg\u00e4nge sind nur mit Ihrer ausdr\u00fccklichen Einwilligung m\u00f6glich. Sie k\u00f6nnen eine bereits erteilte Einwilligung jederzeit widerrufen. Die Rechtm\u00e4\u00dfigkeit der bis zum Widerruf erfolgten Datenverarbeitung bleibt vom Widerruf unber\u00fchrt.</p>","title":"Widerruf Ihrer Einwilligung zur Datenverarbeitung"},{"location":"privacy-policy/#widerspruchsrecht-gegen-die-datenerhebung-in-besonderen-fallen-sowie-gegen-direktwerbung-art-21-dsgvo","text":"<p>WENN DIE DATENVERARBEITUNG AUF GRUNDLAGE VON ART. 6 ABS. 1 LIT. E ODER F DSGVO ERFOLGT, HABEN SIE JEDERZEIT DAS RECHT, AUS GR\u00dcNDEN, DIE SICH AUS IHRER BESONDEREN SITUATION ERGEBEN, GEGEN DIE VERARBEITUNG IHRER PERSONENBEZOGENEN DATEN WIDERSPRUCH EINZULEGEN; DIES GILT AUCH F\u00dcR EIN AUF DIESE BESTIMMUNGEN GEST\u00dcTZTES PROFILING. DIE JEWEILIGE RECHTSGRUNDLAGE, AUF DENEN EINE VERARBEITUNG BERUHT, ENTNEHMEN SIE DIESER DATENSCHUTZERKL\u00c4RUNG. WENN SIE WIDERSPRUCH EINLEGEN, WERDEN WIR IHRE BETROFFENEN PERSONENBEZOGENEN DATEN NICHT MEHR VERARBEITEN, ES SEI DENN, WIR K\u00d6NNEN ZWINGENDE SCHUTZW\u00dcRDIGE GR\u00dcNDE F\u00dcR DIE VERARBEITUNG NACHWEISEN, DIE IHRE INTERESSEN, RECHTE UND FREIHEITEN \u00dcBERWIEGEN ODER DIE VERARBEITUNG DIENT DER GELTENDMACHUNG, AUS\u00dcBUNG ODER VERTEIDIGUNG VON RECHTSANSPR\u00dcCHEN (WIDERSPRUCH NACH ART. 21 ABS. 1 DSGVO).</p> <p>WERDEN IHRE PERSONENBEZOGENEN DATEN VERARBEITET, UM DIREKTWERBUNG ZU BETREIBEN, SO HABEN SIE DAS RECHT, JEDERZEIT WIDERSPRUCH GEGEN DIE VERARBEITUNG SIE BETREFFENDER PERSONENBEZOGENER DATEN ZUM ZWECKE DERARTIGER WERBUNG EINZULEGEN; DIES GILT AUCH F\u00dcR DAS PROFILING, SOWEIT ES MIT SOLCHER DIREKTWERBUNG IN VERBINDUNG STEHT. WENN SIE WIDERSPRECHEN, WERDEN IHRE PERSONENBEZOGENEN DATEN ANSCHLIESSEND NICHT MEHR ZUM ZWECKE DER DIREKTWERBUNG VERWENDET (WIDERSPRUCH NACH ART. 21 ABS. 2 DSGVO).</p>","title":"Widerspruchsrecht gegen die Datenerhebung in besonderen F\u00e4llen sowie gegen Direktwerbung (Art. 21 DSGVO)"},{"location":"privacy-policy/#beschwerderecht-bei-der-zustandigen-aufsichtsbehorde","text":"<p>Im Falle von Verst\u00f6\u00dfen gegen die DSGVO steht den Betroffenen ein Beschwerderecht bei einer Aufsichtsbeh\u00f6rde, insbesondere in dem Mitgliedstaat ihres gew\u00f6hnlichen Aufenthalts, ihres Arbeitsplatzes oder des Orts des mutma\u00dflichen Versto\u00dfes zu. Das Beschwerderecht besteht unbeschadet anderweitiger verwaltungsrechtlicher oder gerichtlicher Rechtsbehelfe.</p>","title":"Beschwerde\u00adrecht bei der zust\u00e4ndigen Aufsichts\u00adbeh\u00f6rde"},{"location":"privacy-policy/#recht-auf-datenubertragbarkeit","text":"<p>Sie haben das Recht, Daten, die wir auf Grundlage Ihrer Einwilligung oder in Erf\u00fcllung eines Vertrags automatisiert verarbeiten, an sich oder an einen Dritten in einem g\u00e4ngigen, maschinenlesbaren Format aush\u00e4ndigen zu lassen. Sofern Sie die direkte \u00dcbertragung der Daten an einen anderen Verantwortlichen verlangen, erfolgt dies nur, soweit es technisch machbar ist.</p>","title":"Recht auf Daten\u00ad\u00fcbertrag\u00adbarkeit"},{"location":"privacy-policy/#ssl-bzw-tls-verschlusselung","text":"<p>Diese Seite nutzt aus Sicherheitsgr\u00fcnden und zum Schutz der \u00dcbertragung vertraulicher Inhalte, wie zum Beispiel Bestellungen oder Anfragen, die Sie an uns als Seitenbetreiber senden, eine SSL- bzw. TLS-Verschl\u00fcsselung. Eine verschl\u00fcsselte Verbindung erkennen Sie daran, dass die Adresszeile des Browsers von \u201ehttp://\" auf \u201ehttps://\" wechselt und an dem Schloss-Symbol in Ihrer Browserzeile.</p> <p>Wenn die SSL- bzw. TLS-Verschl\u00fcsselung aktiviert ist, k\u00f6nnen die Daten, die Sie an uns \u00fcbermitteln, nicht von Dritten mitgelesen werden.</p>","title":"SSL- bzw. TLS-Verschl\u00fcsselung"},{"location":"privacy-policy/#auskunft-loschung-und-berichtigung","text":"<p>Sie haben im Rahmen der geltenden gesetzlichen Bestimmungen jederzeit das Recht auf unentgeltliche Auskunft \u00fcber Ihre gespeicherten personenbezogenen Daten, deren Herkunft und Empf\u00e4nger und den Zweck der Datenverarbeitung und ggf. ein Recht auf Berichtigung oder L\u00f6schung dieser Daten. Hierzu sowie zu weiteren Fragen zum Thema personenbezogene Daten k\u00f6nnen Sie sich jederzeit unter der im Impressum angegebenen Adresse an uns wenden.</p>","title":"Auskunft, L\u00f6schung und Berichtigung"},{"location":"privacy-policy/#recht-auf-einschrankung-der-verarbeitung","text":"<p>Sie haben das Recht, die Einschr\u00e4nkung der Verarbeitung Ihrer personenbezogenen Daten zu verlangen. Hierzu k\u00f6nnen Sie sich jederzeit unter der im Impressum angegebenen Adresse an uns wenden. Das Recht auf Einschr\u00e4nkung der Verarbeitung besteht in folgenden F\u00e4llen:</p> <ul> <li>Wenn Sie die Richtigkeit Ihrer bei uns gespeicherten personenbezogenen Daten bestreiten, ben\u00f6tigen wir in der Regel Zeit, um dies zu \u00fcberpr\u00fcfen. F\u00fcr die Dauer der Pr\u00fcfung haben Sie das Recht, die Einschr\u00e4nkung der Verarbeitung Ihrer personenbezogenen Daten zu verlangen.</li> <li>Wenn die Verarbeitung Ihrer personenbezogenen Daten unrechtm\u00e4\u00dfig geschah/geschieht, k\u00f6nnen Sie statt der L\u00f6schung die Einschr\u00e4nkung der Datenverarbeitung verlangen.</li> <li>Wenn wir Ihre personenbezogenen Daten nicht mehr ben\u00f6tigen, Sie sie jedoch zur Aus\u00fcbung, Verteidigung oder Geltendmachung von Rechtsanspr\u00fcchen ben\u00f6tigen, haben Sie das Recht, statt der L\u00f6schung die Einschr\u00e4nkung der Verarbeitung Ihrer personenbezogenen Daten zu verlangen.</li> <li>Wenn Sie einen Widerspruch nach Art. 21 Abs. 1 DSGVO eingelegt haben, muss eine Abw\u00e4gung zwischen Ihren und unseren Interessen vorgenommen werden. Solange noch nicht feststeht, wessen Interessen \u00fcberwiegen, haben Sie das Recht, die Einschr\u00e4nkung der Verarbeitung Ihrer personenbezogenen Daten zu verlangen.</li> </ul> <p>Wenn Sie die Verarbeitung Ihrer personenbezogenen Daten eingeschr\u00e4nkt haben, d\u00fcrfen diese Daten \u2013 von ihrer Speicherung abgesehen \u2013 nur mit Ihrer Einwilligung oder zur Geltendmachung, Aus\u00fcbung oder Verteidigung von Rechtsanspr\u00fcchen oder zum Schutz der Rechte einer anderen nat\u00fcrlichen oder juristischen Person oder aus Gr\u00fcnden eines wichtigen \u00f6ffentlichen Interesses der Europ\u00e4ischen Union oder eines Mitgliedstaats verarbeitet werden.</p>","title":"Recht auf Einschr\u00e4nkung der Verarbeitung"},{"location":"privacy-policy/#4-datenerfassung-auf-dieser-website","text":"","title":"4. Datenerfassung auf dieser Website"},{"location":"privacy-policy/#cookies_1","text":"<p>Unsere Internetseiten verwenden so genannte \u201eCookies\". Cookies sind kleine Textdateien und richten auf Ihrem Endger\u00e4t keinen Schaden an. Sie werden entweder vor\u00fcbergehend f\u00fcr die Dauer einer Sitzung (Session-Cookies) oder dauerhaft (permanente Cookies) auf Ihrem Endger\u00e4t gespeichert. Session-Cookies werden nach Ende Ihres Besuchs automatisch gel\u00f6scht. Permanente Cookies bleiben auf Ihrem Endger\u00e4t gespeichert, bis Sie diese selbst l\u00f6schen\u00a0oder eine automatische L\u00f6schung durch Ihren Webbrowser erfolgt.</p> <p>Teilweise k\u00f6nnen auch Cookies von Drittunternehmen auf Ihrem Endger\u00e4t gespeichert werden, wenn Sie unsere Seite betreten (Third-Party-Cookies). Diese erm\u00f6glichen uns oder Ihnen die Nutzung bestimmter Dienstleistungen des Drittunternehmens (z.B. Cookies zur Abwicklung von Zahlungsdienstleistungen).</p> <p>Cookies haben verschiedene Funktionen. Zahlreiche Cookies sind technisch notwendig, da bestimmte Websitefunktionen ohne diese nicht funktionieren w\u00fcrden (z.B. die Warenkorbfunktion oder die Anzeige von Videos). Andere Cookies dienen dazu, das Nutzerverhalten auszuwerten\u00a0oder Werbung anzuzeigen.</p> <p>Cookies, die zur Durchf\u00fchrung des elektronischen Kommunikationsvorgangs (notwendige Cookies) oder zur Bereitstellung bestimmter, von Ihnen erw\u00fcnschter Funktionen (funktionale Cookies, z. B. f\u00fcr die Warenkorbfunktion) oder zur Optimierung der Website (z.B. Cookies zur Messung des Webpublikums) erforderlich sind, werden auf Grundlage von Art. 6 Abs. 1 lit. f DSGVO gespeichert, sofern keine andere Rechtsgrundlage angegeben wird. Der Websitebetreiber hat ein berechtigtes Interesse an der Speicherung von Cookies zur technisch fehlerfreien und optimierten Bereitstellung seiner Dienste. Sofern eine Einwilligung zur Speicherung von Cookies abgefragt wurde, erfolgt die Speicherung der betreffenden Cookies ausschlie\u00dflich auf Grundlage dieser Einwilligung (Art. 6 Abs. 1 lit. a DSGVO); die Einwilligung ist jederzeit widerrufbar.</p> <p>Sie k\u00f6nnen Ihren Browser so einstellen, dass Sie \u00fcber das Setzen von Cookies informiert werden und Cookies nur im Einzelfall erlauben, die Annahme von Cookies f\u00fcr bestimmte F\u00e4lle oder generell ausschlie\u00dfen sowie das automatische L\u00f6schen der Cookies beim Schlie\u00dfen des Browsers aktivieren. Bei der Deaktivierung von Cookies kann die Funktionalit\u00e4t dieser Website eingeschr\u00e4nkt sein.</p> <p>Soweit Cookies von Drittunternehmen oder zu Analysezwecken eingesetzt werden, werden wir Sie hier\u00fcber im Rahmen dieser Datenschutzerkl\u00e4rung gesondert informieren und ggf. eine Einwilligung abfragen.</p>","title":"Cookies"},{"location":"privacy-policy/#server-log-dateien","text":"<p>Der Provider der Seiten erhebt und speichert automatisch Informationen in so genannten Server-Log-Dateien, die Ihr Browser automatisch an uns \u00fcbermittelt. Dies sind:</p> <ul> <li>Browsertyp und Browserversion</li> <li>verwendetes Betriebssystem</li> <li>Referrer URL</li> <li>Hostname des zugreifenden Rechners</li> <li>Uhrzeit der Serveranfrage</li> <li>IP-Adresse</li> </ul> <p>Eine Zusammenf\u00fchrung dieser Daten mit anderen Datenquellen wird nicht vorgenommen.</p> <p>Die Erfassung dieser Daten erfolgt auf Grundlage von Art. 6 Abs. 1 lit. f DSGVO. Der Websitebetreiber hat ein berechtigtes Interesse an der technisch fehlerfreien Darstellung und der Optimierung seiner Website \u2013 hierzu m\u00fcssen die Server-Log-Files erfasst werden.</p>","title":"Server-Log-Dateien"},{"location":"privacy-policy/#kontaktformular","text":"<p>Wenn Sie uns per Kontaktformular Anfragen zukommen lassen, werden Ihre Angaben aus dem Anfrageformular inklusive der von Ihnen dort angegebenen Kontaktdaten zwecks Bearbeitung der Anfrage und f\u00fcr den Fall von Anschlussfragen bei uns gespeichert. Diese Daten geben wir nicht ohne Ihre Einwilligung weiter.</p> <p>Die Verarbeitung dieser Daten erfolgt auf Grundlage von Art. 6 Abs. 1 lit. b DSGVO, sofern Ihre Anfrage mit der Erf\u00fcllung eines Vertrags zusammenh\u00e4ngt oder zur Durchf\u00fchrung vorvertraglicher Ma\u00dfnahmen erforderlich ist. In allen \u00fcbrigen F\u00e4llen beruht die Verarbeitung auf unserem berechtigten Interesse an der effektiven Bearbeitung der an uns gerichteten Anfragen (Art. 6 Abs. 1 lit. f DSGVO) oder auf Ihrer Einwilligung (Art. 6 Abs. 1 lit. a DSGVO) sofern diese abgefragt wurde.</p> <p>Die von Ihnen im Kontaktformular eingegebenen Daten verbleiben bei uns, bis Sie uns zur L\u00f6schung auffordern, Ihre Einwilligung zur Speicherung widerrufen oder der Zweck f\u00fcr die Datenspeicherung entf\u00e4llt (z.\u00a0B. nach abgeschlossener Bearbeitung Ihrer Anfrage). Zwingende gesetzliche Bestimmungen \u2013 insbesondere Aufbewahrungsfristen \u2013 bleiben unber\u00fchrt.</p>","title":"Kontaktformular"},{"location":"privacy-policy/#anfrage-per-e-mail-telefon-oder-telefax","text":"<p>Wenn Sie uns per E-Mail, Telefon oder Telefax kontaktieren, wird Ihre Anfrage inklusive aller daraus hervorgehenden personenbezogenen Daten (Name, Anfrage) zum Zwecke der Bearbeitung Ihres Anliegens bei uns gespeichert und verarbeitet. Diese Daten geben wir nicht ohne Ihre Einwilligung weiter.</p> <p>Die Verarbeitung dieser Daten erfolgt auf Grundlage von Art. 6 Abs. 1 lit. b DSGVO, sofern Ihre Anfrage mit der Erf\u00fcllung eines Vertrags zusammenh\u00e4ngt oder zur Durchf\u00fchrung vorvertraglicher Ma\u00dfnahmen erforderlich ist. In allen \u00fcbrigen F\u00e4llen beruht die Verarbeitung auf unserem berechtigten Interesse an der effektiven Bearbeitung der an uns gerichteten Anfragen (Art. 6 Abs. 1 lit. f DSGVO) oder auf Ihrer Einwilligung (Art. 6 Abs. 1 lit. a DSGVO) sofern diese abgefragt wurde.</p> <p>Die von Ihnen an uns per Kontaktanfragen \u00fcbersandten Daten verbleiben bei uns, bis Sie uns zur L\u00f6schung auffordern, Ihre Einwilligung zur Speicherung widerrufen oder der Zweck f\u00fcr die Datenspeicherung entf\u00e4llt (z.\u00a0B. nach abgeschlossener Bearbeitung Ihres Anliegens). Zwingende gesetzliche Bestimmungen \u2013 insbesondere gesetzliche Aufbewahrungsfristen \u2013 bleiben unber\u00fchrt.</p>","title":"Anfrage per E-Mail, Telefon oder Telefax"},{"location":"site-notice/","text":"<p>The Privacy Policy / Datenschutz\u00aderkl\u00e4rung can be found here: Privacy Policy / Datenschutz\u00aderkl\u00e4rung page.</p> <p>German version of the Site Notice / Impressum below.</p>","title":"Impressum"},{"location":"site-notice/#site-notice","text":"","title":"Site Notice"},{"location":"site-notice/#information-pursuant-to-sect-5-german-telemedia-act-tmg","text":"Alexander Trost Postfach 11 75196 K\u00e4mpfelbach Germany","title":"Information pursuant to Sect. 5 German Telemedia Act (TMG)"},{"location":"site-notice/#contact","text":"E-mail: me@galexrt.moe","title":"Contact"},{"location":"site-notice/#responsible-for-the-content-according-to-sect-55-paragraph-2-rstv","text":"Alexander Trost Postfach 11 75196 K\u00e4mpfelbach Germany","title":"Responsible for the content according to Sect. 55, paragraph 2 RStV"},{"location":"site-notice/#liability-for-contents","text":"<p>As service providers, we are liable for own contents of these websites according to Paragraph 7, Sect. 1 German Telemedia Act (TMG). However, according to Paragraphs 8 to 10 German Telemedia Act (TMG), service providers are not obligated to permanently monitor submitted or stored information or to search for evidences that indicate illegal activities.</p> <p>Legal obligations to removing information or to blocking the use of information remain unchallenged. In this case, liability is only possible at the time of knowledge about a specific violation of law. Illegal contents will be removed immediately at the time we get knowledge of them.</p>","title":"Liability for Contents"},{"location":"site-notice/#liability-for-links","text":"<p>Our offer includes links to external third-party websites. We have no influence on the contents of those websites, therefore we cannot guarantee for those contents. Providers or administrators of linked websites are always responsible for their own contents.</p> <p>The linked websites had been checked for possible violations of law at the time of the establishment of the link. Illegal contents were not detected at the time of the linking. A permanent monitoring of the contents of linked websites cannot be imposed without reasonable indications that there has been a violation of law. Illegal links will be removed immediately at the time we get knowledge of them.</p>","title":"Liability for Links"},{"location":"site-notice/#copyright","text":"<p>Contents and compilations published on these websites by the providers are subject to Creative Commons Attribution 4.0 International License.</p> <p>This work is licensed under a Creative Commons Attribution 4.0 International License. </p>","title":"Copyright"},{"location":"site-notice/#impressum","text":"<p>English version below Site Notice section.</p>","title":"Impressum"},{"location":"site-notice/#angaben-gema-5-tmg","text":"Alexander Trost Postfach 11 75196 K\u00e4mpfelbach Germany","title":"Angaben gem\u00e4\u00df \u00a7 5 TMG"},{"location":"site-notice/#kontakt","text":"E-Mail: me@galexrt.moe","title":"Kontakt"},{"location":"site-notice/#verantwortlich-fur-den-inhalt-nach-55-abs-2-rstv","text":"Alexander Trost Postfach 11 75196 K\u00e4mpfelbach Germany","title":"Verantwortlich f\u00fcr den Inhalt nach \u00a7 55 Abs. 2 RStV"},{"location":"site-notice/#haftung-fur-inhalte","text":"<p>Als Diensteanbieter sind wir gem\u00e4\u00df \u00a7 7 Abs.1 TMG f\u00fcr eigene Inhalte auf diesen Seiten nach den allgemeinen Gesetzen verantwortlich. Nach \u00a7\u00a7 8 bis 10 TMG sind wir als Diensteanbieter jedoch nicht verpflichtet, \u00fcbermittelte oder gespeicherte fremde Informationen zu \u00fcberwachen oder nach Umst\u00e4nden zu forschen, die auf eine rechtswidrige T\u00e4tigkeit hinweisen.</p> <p>Verpflichtungen zur Entfernung oder Sperrung der Nutzung von Informationen nach den allgemeinen Gesetzen bleiben hiervon unber\u00fchrt. Eine diesbez\u00fcgliche Haftung ist jedoch erst ab dem Zeitpunkt der Kenntnis einer konkreten Rechtsverletzung m\u00f6glich. Bei Bekanntwerden von entsprechenden Rechtsverletzungen werden wir diese Inhalte umgehend entfernen.</p>","title":"Haftung f\u00fcr Inhalte"},{"location":"site-notice/#haftung-fur-links","text":"<p>Unser Angebot enth\u00e4lt Links zu externen Websites Dritter, auf deren Inhalte wir keinen Einfluss haben. Deshalb k\u00f6nnen wir f\u00fcr diese fremden Inhalte auch keine Gew\u00e4hr \u00fcbernehmen. F\u00fcr die Inhalte der verlinkten Seiten ist stets der jeweilige Anbieter oder Betreiber der Seiten verantwortlich. Die verlinkten Seiten wurden zum Zeitpunkt der Verlinkung auf m\u00f6gliche Rechtsverst\u00f6\u00dfe \u00fcberpr\u00fcft. Rechtswidrige Inhalte waren zum Zeitpunkt der Verlinkung nicht erkennbar.</p> <p>Eine permanente inhaltliche Kontrolle der verlinkten Seiten ist jedoch ohne konkrete Anhaltspunkte einer Rechtsverletzung nicht zumutbar. Bei Bekanntwerden von Rechtsverletzungen werden wir derartige Links umgehend entfernen.</p>","title":"Haftung f\u00fcr Links"},{"location":"site-notice/#urheberrecht","text":"<p>Die durch die Seitenbetreiber erstellten Inhalte und Werke auf diesen Seiten unterliegen der Creative Commons Attribution 4.0 International License.</p> <p>Diese Arbeiten sind lizenziert unter der Creative Commons Attribution 4.0 International License. </p>","title":"Urheberrecht"},{"location":"databases/mysql/cheat-sheet/","text":"","title":"Cheat Sheet"},{"location":"databases/mysql/cheat-sheet/#re-create-the-debian-sys-maint-user","text":"<pre><code>mysqldump --complete-insert --extended-insert=0 -u root -p mysql | grep 'debian-sys-maint'\n</code></pre>","title":"Re-Create the <code>debian-sys-maint</code> User"},{"location":"databases/mysql/event_scheduler/","text":"<p>Ever wanted to run queries in a cronjob? Where to safely put the database credentials? MySQL / MariaDB can help out with that.</p>","title":"event_scheduler"},{"location":"databases/mysql/event_scheduler/#enable-the-event_scheduler-system","text":"<p>You must have the <code>event_scheduler</code> enabled, this can be done by running the following query:</p> <pre><code>SET GLOBAL event_scheduler = ON;\n</code></pre>   <p>Note</p> <p>It is highly recommended to use the config file(s) of your MySQL / MariaDB server to enable the <code>event_scheduler</code> feature. Another (hacky) way is to use the MySQL server <code>init_file</code> option which runs a SQL script on server startup.</p>","title":"Enable the <code>event_scheduler</code> System"},{"location":"databases/mysql/event_scheduler/#create-an-event-to-run-a-sql-query","text":"<p>The <code>CREATE EVENT</code> query below would run the query after the <code>DO</code> every day at <code>02:00</code> in the <code>exampledb</code> database.</p> <pre><code>CREATE EVENT `exampledb`.`my_cool_table_reset_userOption45` ON\nSCHEDULE EVERY 1 DAY STARTS CURRENT_DATE + INTERVAL 1 DAY + INTERVAL 2 HOUR DO\nUPDATE\n    `exampledb`.`my_cool_table`\nSET\n    userOption45 = ''\nWHERE\n    userOption45 != ''\n    AND STR_TO_DATE(userOption45,\n    '%Y-%m-%d') &lt;= NOW();\n</code></pre>","title":"Create an Event to run a SQL Query"},{"location":"databases/mysql/event_scheduler/#show-existing-events","text":"<p>Note</p> <p><code>exampledb</code> is the database name.</p>  <pre><code>SHOW EVENTS FROM `exampledb`;\n</code></pre>","title":"Show existing Events"},{"location":"databases/mysql/event_scheduler/#references","text":"<ul> <li>MySQL 5.7 Reference - <code>event_scheduler</code></li> <li>MySQL 5.7 Reference - <code>CREATE EVENT</code> Statement.</li> </ul>","title":"References"},{"location":"databases/mysql/init_file/","text":"<p>Add the following paramter to the <code>[mysqld]</code> or <code>[mariadb]</code> section of your <code>my.cnf</code> file (depending on the OS, at <code>/etc/mysql/my.cnf</code>, <code>/etc/my.cnf</code>, other path):</p> <pre><code>init_file = /etc/mysql/init.sql\n</code></pre>  <p>The <code>/etc/mysql/init.sql</code> file can contain \"any\" SQL queries.</p> <p>Example to enable / install the MariaDB Query Response Time Plugin plugin:</p> <pre><code>INSTALL SONAME 'query_response_time';\n\nSET GLOBAL query_response_time_stats = 1;\nSET GLOBAL query_response_time_flush = 1;\n</code></pre>   <p>Note</p> <p>There are other ways to install the plugin, but to \"initially\" flush the plugin's data the <code>SET GLOBAL query_response_time_</code> can be useful to be run.</p>","title":"init_file: Run SQL file on startup"},{"location":"databases/mysql/init_file/#references","text":"<ul> <li>https://mariadb.com/docs/reference/mdb/system-variables/init_file/</li> <li>https://dev.mysql.com/doc/refman/8.0/en/server-system-variables.html#sysvar_init_file</li> </ul>","title":"References"},{"location":"databases/mysql/pitfalls/","text":"","title":"Pitfalls"},{"location":"databases/mysql/pitfalls/#mysql-cli-console-password-character-limit","text":"<p><code>mysql</code> command only reads in 79 characters of \"password\" from TTY / \"console\". In my case I was copy'n'pasting / the password store autotyping database passwords which doesn't work then (leads to <code>Access denied for user</code> errors because the password is wrong). This \"limit\" doesn't seem to be documented anywhere I looked, so here we go. If you want to login to your users only use passwords 79 characters long when \"typing\" them on the console.</p> <p>Thanks to this Jira ticket (MXS-1766) to pointing to the code behind the \"limitation\"!</p> <p>I also need to give a shoutout to the people with which I reflected the issue and then stumbled upon the <code>mysql</code> CLI limitation.</p> <p>Code references:</p> <ul> <li>GitHub MariaDB/server - <code>10.3/client/mysql.cc</code> Line 1959</li> <li>GitHub MariaDB/server - <code>10.3/mysys/get_password.c</code> Line 63</li> </ul>","title":"<code>mysql</code> CLI Console Password Character \"Limit\""},{"location":"databases/mysql/pitfalls/#replication-user-password-character-limit","text":"<p>The password for a replication user must be a maximum of <code>32</code> characters long.</p>","title":"Replication User Password Character Limit"},{"location":"databases/mysql/pitfalls/#mysql-memory-storage-size-suffixes","text":"<p>TL;DR The suffixes are in \"Bytes\". E.g., 16M = 16777216 Bytes.</p>  <pre><code>mysql --max_allowed_packet=16777216\nmysql --max_allowed_packet=16M\n</code></pre>  <p>The first command specifies the value in bytes. The second specifies the value in megabytes. For variables that take a numeric value, the value can be given with a suffix of K, M, or G to indicate a multiplier of 1024, 10242 or 10243. (For example, when used to set max_allowed_packet, the suffixes indicate units of kilobytes, megabytes, or gigabytes.) As of MySQL 8.0.14, a suffix can also be T, P, and E to indicate a multiplier of 10244, 10245 or 10246. Suffix letters can be uppercase or lowercase.</p>  <p>Quoted from: MySQL 8.0 Reference Manual - 4.2.2.5 Using Options to Set Program Variables</p>","title":"MySQL Memory / Storage Size Suffixes"},{"location":"databases/mysql/slow-log/","text":"","title":"Slow Log"},{"location":"databases/mysql/slow-log/#enable-slow-query-log-to-file","text":"<pre><code>SET @@global.slow_query_log_use_global_control = long_query_time,min_examined_row_limit,log_slow_verbosity;\nSET GLOBAL slow_query_log_file = '/var/log/mysql/slow_log.log';\nSET GLOBAL min_examined_row_limit = 0;\nSET GLOBAL long_query_time = 0;\nSET GLOBAL slow_query_log = 1;\n</code></pre>","title":"Enable Slow Query Log to File"},{"location":"databases/mysql/slow-log/#disable-slow-query-log","text":"<p>It is important to log slow queries, so set it to something like <code>3</code> seconds at maximum.</p> <pre><code>SET GLOBAL long_query_time = 3;\n</code></pre>","title":"Disable Slow Query Log"},{"location":"general/tools-utilities/","text":"","title":"Projects, Tools and Utilites"},{"location":"general/tools-utilities/#automation","text":"<ul> <li>https://github.com/duncs/clusterssh - Not necessarily \"automation\" in the classical sense, but being to access multiple servers over SSH at the same time.</li> </ul>","title":"Automation"},{"location":"general/tools-utilities/#network","text":"","title":"Network"},{"location":"general/tools-utilities/#dns","text":"<ul> <li>https://github.com/DNS-OARC/flamethrower - \"a DNS performance and functional testing utility supporting UDP, TCP, DoT and DoH (by @ns1)\"</li> </ul>","title":"DNS"},{"location":"general/tools-utilities/#kubernetes-k8s","text":"","title":"Kubernetes (K8S)"},{"location":"general/tools-utilities/#client","text":"<ul> <li>https://github.com/kubernetes-sigs/krew/ - Find and install kubectl plugins - krew.dev</li> <li>https://github.com/stern/stern - Multi pod and container log tailing for Kubernetes with regex support for selection of Pods and Pods' containers.</li> </ul>","title":"Client"},{"location":"general/tools-utilities/#network_1","text":"<ul> <li>https://github.com/inovex/illuminatio - The kubernetes network policy validator.</li> <li>https://github.com/aquasecurity/kube-bench - Checks whether Kubernetes is deployed according to security best practices as defined in the CIS Kubernetes Benchmark</li> </ul>","title":"Network"},{"location":"general/tools-utilities/#monitoring","text":"","title":"Monitoring"},{"location":"general/tools-utilities/#prometheus","text":"<ul> <li>Exporters: See Monitoring -&gt; Prometheus -&gt; Exporters page.</li> </ul>","title":"Prometheus"},{"location":"general/tools-utilities/#databases","text":"<ul> <li>https://dbeaver.io/ - Great tool to access many popular databases. An enterprise edition which supports more databases (e.g., MongoDB) is available.</li> <li>https://www.mongodb.com/docs/compass/current/ - \"Basic\" GUI for querying and managing MongoDB servers.</li> </ul>","title":"Databases"},{"location":"general/useful-online-tools/","text":"","title":"Online Tools"},{"location":"general/useful-online-tools/#diagrams","text":"<ul> <li>https://app.diagrams.net/ - Previously named <code>draw.io</code>.</li> <li>https://isoflow.io/ - \"Create beautiful cloud diagrams in minutes\"</li> </ul>","title":"Diagrams"},{"location":"general/useful-online-tools/#image-resizing","text":"<ul> <li>http://waifu2x.udp.jp - Free variant (less zoom).</li> <li>https://mng.waifu2x.me - Paid (not much, billed per minute).</li> </ul>","title":"Image Resizing"},{"location":"general/visual-studio-code/","text":"<p>If you are using Visual Studio Code here's some recommended extensions for it.</p>","title":"Visual Studio Code"},{"location":"general/visual-studio-code/#recommended-extensions","text":"<ul> <li>EditorConfig</li> <li>Better Comments</li> <li>Color Highlight</li> <li>Encode Decode</li> <li>GitLens</li> <li>Markdown All in One</li> <li>Markdown Preview Mermaid Support</li> <li>ShellCheck</li> <li>YAML</li> </ul>","title":"Recommended Extensions"},{"location":"kubernetes/cheat-sheet/","text":"","title":"Cheat Sheet"},{"location":"kubernetes/cheat-sheet/#update-statefulset-volumeclaimtemplates-and-other-uneditable-sections","text":"<ol> <li>Get the latest YAML of the StatefulSet you want to update (e.g., change size of a <code>volumeClaimTemplates</code> entry).</li> <li>Make the changes to the YAML file.</li> <li>Run <code>kubectl delete statefulset STATEFULSET_NAME --cascade=orphan</code></li> <li>The important thing here is the <code>--cascade=orphan</code> flag, it stops the <code>ControllerRevisions</code> objects (+ the Pods) to not be deleted (no downtime).</li> <li>Now just apply your StatefulSet YAML and if needed trigger a rolling update of the StatefulSet using the <code>kubectl rollout restart statefulset STATEFULSET_NAME</code> command.</li> </ol>","title":"Update StatefulSet <code>volumeClaimTemplates</code> And Other Uneditable Sections"},{"location":"kubernetes/cheat-sheet/#quickly-trigger-rolling-update-of-deployment-statefulset-daemonset-etc","text":"","title":"Quickly trigger Rolling Update of Deployment, StatefulSet, DaemonSet, etc"},{"location":"kubernetes/cheat-sheet/#new-way","text":"<p>Example for <code>StatefulSet</code> and <code>Deployment</code> below:</p> <pre><code>kubectl rollout restart statefulset STATEFULSET_NAME\nkubectl rollout restart deployment DEPLOYMENT_NAME\n</code></pre>","title":"New Way"},{"location":"kubernetes/cheat-sheet/#old-way","text":"<pre><code>kubectl patch -n kube-system ds kube-proxy -p \"{\\\"spec\\\":{\\\"template\\\":{\\\"metadata\\\":{\\\"annotations\\\":{\\\"date\\\":\\\"$(date +'%s')\\\"}}}}}\"\n</code></pre>  <p>Running <code>kubectl replace</code>/<code>kubectl apply</code> on an object which the command above was used on, will always trigger a rolling update again. This is due to the change to the annotations.</p>","title":"Old Way"},{"location":"kubernetes/cheat-sheet/#debug-pod-manifest-to-escape-to-the-node","text":"<p>The Pods manifest assumes that you are allowed to run <code>privileged</code> Pods in your cluster. If you are using you may need to set a ServiceAccount which is allowed \"all the things\" (e.g. <code>privileged</code>, <code>hostNetwork</code>, and so on).</p> <pre><code>kind: Pod\napiVersion: v1\nmetadata:\n  name: debug-pod\n  labels:\n    app: debug\nspec:\n  hostNetwork: true\n  tolerations:\n    - key: node-role.kubernetes.io/master\n      effect: NoSchedule\n    - key: \"CriticalAddonsOnly\"\n      operator: \"Exists\"\n  restartPolicy: Never\n  hostIPC: true\n  hostPID: true\n#  nodeName: SPECIFIC_TARGET_NODE\n  priorityClassName: \"system-cluster-critical\"\n  containers:\n  - name: debug-pod\n    image: busybox\n    command: [\"/bin/sleep\", \"36000\"]\n    securityContext:\n      privileged: true\n      allowPrivilegeEscalation: true\n</code></pre>  <p><code>kubectl exec -it POD_NAME -- sh</code> into the Pod and use <code>nsenter</code> to escape the container's namespace:</p> <pre><code>$ nsenter -t 1 -m -u -n -i sh\n</code></pre>","title":"Debug Pod manifest to \"escape\" to the node"},{"location":"kubernetes/cheat-sheet/#role-label-for-node-objects","text":"<p>The <code>node-role.kubernetes.io/</code> can take \"anything\" as a role. Meaning that <code>node-role.kubernetes.io/my-cool-role</code> (any value) will cause the <code>kubectl get nodes</code> output to display <code>my-cool-role</code> (and other such role labels) as the Node role.</p>","title":"Role Label for Node objects"},{"location":"kubernetes/cheat-sheet/#kubectl-config-context-switching","text":"","title":"kubectl: Config/ Context Switching"},{"location":"kubernetes/cheat-sheet/#set-namespace-for-current-context","text":"<pre><code>kubectl config set-context --current --namespace NAMESPACE\n</code></pre>","title":"Set Namespace For Current Context"},{"location":"kubernetes/cheat-sheet/#switch-to-other-context","text":"<pre><code>kubectl config use-context CONTEXT_NAME\n</code></pre>","title":"Switch To Other Context"},{"location":"kubernetes/cheat-sheet/#show-contexts","text":"<pre><code>kubectl config get-contexts\n</code></pre>","title":"Show Contexts"},{"location":"kubernetes/cheat-sheet/#trigger-run-a-cronjob-now-manually","text":"<pre><code>kubectl create job --from=cronjob/CRONJOB_NAME JOB_NAME\nkubectl create job --from=cronjob/curator curator-manual-run\n</code></pre>","title":"Trigger/ Run A CronJob Now (Manually)"},{"location":"kubernetes/cluster-components-upgrade-order/","text":"<p>The following is a recommended Upgrade Order for the Components of a Kubernetes cluster:</p> <ul> <li>Control Plane Components<ol> <li><code>kube-apiserver</code></li> <li><code>kube-controller-manager</code><ol> <li>If used, <code>cloud-controller-manager</code></li> </ol> </li> <li><code>kube-scheduler</code></li> <li><code>etcd</code></li> </ol> </li> <li>Node Components<ol> <li><code>kubelet</code></li> <li><code>kube-proxy</code></li> </ol> </li> </ul> <p>Other components of a Kubernetes cluster can mostly be updated in any order, as long as the documentation of the component doesn't state otherwise:</p> <ul> <li>CNI (e.g., Calico, Cillium)</li> <li>Operators</li> </ul>  <p>Warning</p> <p>Be aware of any changes to the operator configs and CustomResourceDefinitions causing unwanted \"results\".</p> <p>E.g., new versions of CustomResourceDefinitions have new fields and / or change the behavior.</p>","title":"Cluster Components Upgrade Order"},{"location":"kubernetes/kubeadm/","text":"","title":"kubeadm"},{"location":"kubernetes/kubeadm/#kubeadm-cluster-installation","text":"<p>Checkout the official Kubernetes documentation links:</p> <ul> <li>Installation of kubeadm: https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/</li> <li>Kubernetes Cluster Creation using kubeadm:<ul> <li>Single Control Plane Cluster: https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/</li> <li>Highly available Cluster:<ul> <li>Options for highly available clusters: https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/ha-topology/</li> <li>Basic highly available cluster: https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/high-availability/</li> </ul> </li> </ul> </li> </ul>","title":"kubeadm Cluster Installation"},{"location":"kubernetes/kubeadm/#prometheus-kube-prometheus-etcd-metrics-access","text":"<p>You need to create a secret that contains the ETCD healthcheck client cert and key.</p> <pre><code>kubectl \\\n    create \\\n    -n monitoring \\\n    secret generic \\\n    etcd-client-cert \\\n    --from-file=/etc/kubernetes/pki/etcd/ca.crt \\\n    --from-file=/etc/kubernetes/pki/etcd/healthcheck-client.crt \\\n    --from-file=/etc/kubernetes/pki/etcd/healthcheck-client.key\n</code></pre>  <p>For more information checkout this comment: https://github.com/prometheus-community/helm-charts/issues/204#issuecomment-765155883</p>","title":"Prometheus (kube-prometheus) ETCD Metrics Access"},{"location":"kubernetes/name-schema/","text":"","title":"Kubernetes Name Schemas"},{"location":"kubernetes/name-schema/#hostname-schema","text":"","title":"Hostname Schema"},{"location":"kubernetes/name-schema/#domain-tld-usage","text":"<ul> <li>Services: <code>example.services</code></li> <li>Servers (bare metal, VMs, doesn't matter): <code>example.systems</code></li> <li>Network hardware: <code>example.network</code></li> </ul>","title":"Domain TLD Usage"},{"location":"kubernetes/name-schema/#schema-parts","text":"<ul> <li><code>COUNTRY</code> - ISO 3166-1 alpha-3 code, see ISO 3166-1 alpha-3 - Wikipedia and Online Browsing Platform (OBP) - ISO.</li> <li><code>PROVIDER</code> - 3 character long abbreviation of provider name.</li> <li><code>DC</code> - Optional info about datacenter/region (e.g., <code>FSN1-DC1</code>).</li> <li><code>CLUSTER</code> - Cluster designation (in case of Kubernetes, should always have k8s in the beginning) and if there can be multiple a number added (with two digits, e.g., <code>01</code>, <code>12</code>).</li> <li><code>ROLE</code> - In case of Kubernetes, e.g., <code>master</code>, <code>etcd</code>, <code>node</code> (other \"special\" roles could be, e.g., <code>ingress</code>, <code>stora</code> (could have a suffix per storage software, e.g. <code>storaceph</code>)).</li> <li><code>COUNT_OR_ID</code> - A count is a special \"type\". It can for servers that are known to have only a maximum of n machines at maximum, the number of servers padded with zeroes (e.g., max 12 servers results in <code>COUNT</code> for the third machine being <code>03</code>), in case of nodes where there can be an undefined amount of them it should be a shortid.<ul> <li>To generate a \"random\" ID (requires <code>bashids</code> to be installed): <pre><code>bashids \\\n  -e \\\n  -s 'B_r1KMOASvn_5A1hDKCdPXJfrIBcddpwOnT5orXYaQPV4Ixb1zNSpa-nF6HOw8mii3pqovZUtsnGZ5pqbf59wPfeMp9XagGXc8ViJreL_5J1kvSnDCPfqvuV2bmGsx4DrVV_ef3Gr3MgCMrX86TGUjCDeJmM3LONAfKIH_vv0ZR9WWcJJbLCc5xnxWh7Is8qNq95ORIHS6iU4gKZNV-LIxdYxd7WyO2fKeOn8kApv0FFD2ydkJXdz4KjqBEcN5Fu' \\\n  $(date +%s%6N) | \\\n    tr '[:upper:]' '[:lower:]'\n</code></pre> </li> </ul> </li> </ul>","title":"Schema Parts"},{"location":"kubernetes/name-schema/#servers-vms-schema","text":"<pre><code># When country, provider and dc should be omited:\n{CLUSTER}-{ROLE}-{COUNT_OR_ID}.example.systems\n# Examples:\n## Master / Primary Kubernetes node\nk8s02-master-01.example.systems\n## Node / Worker Kubernetes node\nk8s02-node-4ua16bzb6r7.example.systems\n\n# With country, provider and dc in the name:\n{CLUSTER}-{ROLE}-{COUNT_OR_ID}-{PROVIDER}-{COUNTRY}{DC}.example.systems\n# Examples:\n## `deu-fsn1dc1` translates to Germany, FSN1-DC14 (Falkenstein)\nk8s02-master-01-htz-deu-fsn1dc14.example.systems\n## `deu-fsn1dc1` translates to Germany, FSN1-DC1 (Falkenstein)\nk8s02-node-4ua16bzb6r7-htz-deu-fsn1dc1.example.systems\n## `eee-west2` translates to GCP Region `europe-west2`\nk8s02-master-02-gcp-eee-west2.example.systems\n## `usa-west1` translates to AWS Region `us-west1`\nk8s02-node-7ca16bnb2r1-aws-usa-west1.example.systems\n</code></pre>","title":"Servers / VMs Schema"},{"location":"kubernetes/name-schema/#script-gernate-hostname-id","text":"<pre><code>CLUSTER=\"k8s02\"\nROLE=\"node\"\nCOUNT_OR_ID=\"$(sleep 0.00001; \\\nbashids \\\n    -e \\\n    -s 'B_r1KMOASvn_5A1hDKCdPXJfrIBcddpwOnT5orXYaQPV4Ixb1zNSpa-nF6HOw8mii3pqovZUtsnGZ5pqbf59wPfeMp9XagGXc8ViJreL_5J1kvSnDCPfqvuV2bmGsx4DrVV_ef3Gr3MgCMrX86TGUjCDeJmM3LONAfKIH_vv0ZR9WWcJJbLCc5xnxWh7Is8qNq95ORIHS6iU4gKZNV-LIxdYxd7WyO2fKeOn8kApv0FFD2ydkJXdz4KjqBEcN5Fu' \\\n    $(date +%s%6N) | \\\n        tr '[:upper:]' '[:lower:]')\"\necho \"$CLUSTER-$ROLE-$COUNT_OR_ID.example.systems\"\necho \"$CLUSTER-$ROLE-$COUNT_OR_ID-htz-deu-fsn1dc1.example.systems\"\n</code></pre>   <p>Note</p> <p>The <code>sleep 0.00001</code> is used to try to prevent \"duplicates\" when <code>bashids</code> is run in parallel to generate IDs.</p>","title":"Script: Gernate Hostname (+ ID)"},{"location":"kubernetes/name-schema/#services-schema","text":"<pre><code>{CLUSTER}-({OWNER}-){ROLE}.example.services\n\n# Examples\n## Kubernetes cluster \"k8s02\" Loadbalancer owned by the \"system\"\nk8s02-system-lb.example.services\n# or the owner can be omitted in such cases\nk8s02-lb.example.services\n## Kubernetes cluster \"k8s02\" hosted TeamSpeak service owned by customer \"gamer\"\nk8s02-gamer-ts3.example.services\n</code></pre>","title":"Services Schema"},{"location":"kubernetes/system-requirements/","text":"<p>Recommendations for Requirements</p> <p>These are recommendations for requirements of Kubernetes Masters and Nodes.</p>","title":"System Requirements"},{"location":"kubernetes/system-requirements/#general","text":"<p>Network:</p> <ul> <li>Bandwidth: at the very least 1G, recommended for smaller, lower traffic environents is 10G, 25G or more.</li> </ul>  <p>Note</p> <p>To reduce costs you can get away with just having a single interface, instead of, e.g., 2 interfaces being bonded together (it also reduces complexity).</p>","title":"General"},{"location":"kubernetes/system-requirements/#master","text":"","title":"Master"},{"location":"kubernetes/system-requirements/#storage","text":"<p>SSDs or even NVMe based storage is more expensive but your ETCD will love and need it!</p> <p>DO NOT USE HDDs nor any kind of networked storage for ETCD!</p> <p>Even a fast Ceph RBD (e.g., when running in VMs) can look good in the beginning but might \"kill\" the ETCD performance in the end!</p> <p>Too many users of Kubernetes or OpenShift do that and end up with slow performing clusters in many different ways, simply because the ETCD is slow (even though the kube-apiservers are caching a lot)</p>  <p>Use at least SSDs (minimum SATA based, better NVMe based) or any other storage with low latencies! ETCD is pretty much latency bound. It needs \"few\" IOPS but latency is the killer (sequential writing, e.g., WAL and DB).</p>","title":"Storage"},{"location":"kubernetes/ETCD/cheat-sheet/","text":"","title":"Cheat Sheet"},{"location":"kubernetes/ETCD/cheat-sheet/#check-etcd-performance-status-quickly","text":"<pre><code>ETCDCTL_API=3 etcdctl \\\n    [YOUR_FLAGS] \\\n    check perf\n</code></pre>","title":"Check ETCD performance \"status\" quickly"},{"location":"kubernetes/ETCD/cheat-sheet/#example","text":"","title":"Example"},{"location":"kubernetes/ETCD/cheat-sheet/#kubernetes-kubeadm","text":"<pre><code>ETCDCTL_API=3 etcdctl \\\n    --cacert=/etc/kubernetes/pki/etcd/ca.crt \\\n    --cert=/etc/kubernetes/pki/etcd/server.crt \\\n    --key=/etc/kubernetes/pki/etcd/server.key \\\n    check perf\n</code></pre>","title":"Kubernetes (<code>kubeadm</code>)"},{"location":"kubernetes/ETCD/cheat-sheet/#get-metrics-from-etcd-using-curl","text":"<p>Note</p> <p>The <code>/etc/etcd/etcd.conf</code> was actively used in OpenShift 3.x installations and some older Kubernetes deployment \"methods\".</p>  <pre><code># Should you still have a `etcd.conf` source it\nsource /etc/etcd/etcd.conf\n# Otherwise replace each `$ETCD_PEER_*` with the according path\ncurl --cacert=$ETCD_PEER_CA_FILE --cert=$ETCD_PEER_CERT_FILE --key=$ETCD_PEER_KEY_FILE -L https://127.0.0.1:2379/metrics -XGET -v\n</code></pre>","title":"Get Metrics from ETCD using <code>curl</code>"},{"location":"kubernetes/ETCD/cheat-sheet/#show-etcd-cluster-members","text":"<pre><code>ETCDCTL_API=3 etcdctl \\\n    --cacert=/etc/kubernetes/pki/etcd/ca.crt \\\n    --cert=/etc/kubernetes/pki/etcd/server.crt \\\n    --key=/etc/kubernetes/pki/etcd/server.key \\\n    member list\n</code></pre>","title":"Show ETCD Cluster Members"},{"location":"kubernetes/ETCD/editing-kubernetes-objects/","text":"<p>Danger</p> <p>You should be 100% sure what you are doing and should have at least a snapshot of the etcd you want to edit as things can and will possibly go wrong!</p> <p>Do this at your own risk!</p>","title":"Editing Kubernetes Objects"},{"location":"kubernetes/ETCD/editing-kubernetes-objects/#requirements","text":"<ul> <li>ETCD cluster running.<ul> <li><code>etcdctl</code> can reach it (you need to know which flags to provide, e.g., for tls certs and so on).</li> </ul> </li> <li>Golang installed (<code>glide</code> installed in <code>PATH</code>, e.g., <code>go get -u github.com/Masterminds/glide</code>).<ul> <li>PATH includes the <code>$GOPATH/bin</code> (<code>export PATH=\"$GOPATH/bin/:$PATH\"</code>)</li> </ul> </li> </ul>","title":"Requirements"},{"location":"kubernetes/ETCD/editing-kubernetes-objects/#steps","text":"","title":"Steps"},{"location":"kubernetes/ETCD/editing-kubernetes-objects/#step-1-prepare-environment","text":"<pre><code>go get -u github.com/jpbetz/auger\ncd $GOPATH/src/github.com/jpbetz/auger\ngo get -u github.com/Masterminds/glide\nmake vendor\n# Stay in this directory\n</code></pre>   <p>Warning</p> <p>Be sure to stop all API servers, before continuing with the next steps. Be sure to stop all Controller Manager servers, before continuing with the next steps.</p> <p>Depending on the cluster setup, you may just need move out the according <code>manifest</code> from <code>/etc/kubernetes/manifests</code> directory.</p>","title":"Step 1 - Prepare Environment"},{"location":"kubernetes/ETCD/editing-kubernetes-objects/#step-2-locate-object-path","text":"<p>Note</p> <p>The <code>etcdctl</code> probably needs to be run inside the etcd container on one of the Kubernetes masters.</p>  <pre><code>ETCDCTL_API=3 etcdctl \\\n    get /registry/ --keys-only --prefix\n</code></pre>  <p>I recommend you to keep the session on the server for <code>etcdctl</code> open and after finding the correct key to export it using <code>export YOUR_OBJECT_PATH=__PATH__</code> as it will be used like this later on.</p>","title":"Step 2 - Locate object path"},{"location":"kubernetes/ETCD/editing-kubernetes-objects/#step-3-get-object-from-etcd","text":"<p>Note</p> <p>The <code>etcdctl</code> probably needs to be run inside the etcd container on one of the Kubernetes masters.</p>  <p>Replace <code>$YOUR_OBJECT_PATH</code> with the path of the object or set it as a variable.</p> <pre><code>ETCDCTL_API=3 etcdctl \\\n    --endpoints=https://[127.0.0.1]:2379 \\\n    --cacert=/var/lib/minikube/certs//etcd/ca.crt \\\n    --cert=/var/lib/minikube/certs//etcd/healthcheck-client.crt \\\n    --key=/var/lib/minikube/certs//etcd/healthcheck-client.key \\\n    get $YOUR_OBJECT_PATH &gt; etcd-data-old.bin\n</code></pre>  <p>Copy <code>etcd-data-old.bin</code> to the host, e.g.:</p> <pre><code>scp $SSH_USER@$SSH_HOST:etcd-data-old.bin .\n</code></pre>","title":"Step 3 - Get object from ETCD"},{"location":"kubernetes/ETCD/editing-kubernetes-objects/#step-4-decode-and-edit-the-produced-output-as-you-need","text":"<pre><code>cat etcd-data-old.bin | \\\n    go run main.go decode &gt; object.yaml\n</code></pre>  <p>Now edit the <code>object.yaml</code> as you need.</p>","title":"Step 4 - Decode and edit the produced output as you need"},{"location":"kubernetes/ETCD/editing-kubernetes-objects/#step-5-encode-and-save-data-to-etcd","text":"<pre><code>cat object.yaml | \\\n    go run main.go encode &gt; etcd-data-new.bin\n</code></pre>  <p>Copy the <code>etcd-data-new.bin</code> to the host, e.g.:</p> <pre><code>scp etcd-data-new.bin $SSH_USER@$SSH_HOST:\n</code></pre>   <p>Note</p> <p>The <code>etcdctl</code> probably needs to be run inside the etcd container on one of the Kubernetes masters.</p>  <pre><code>cat etcd-data-new.bin | \\\n    ETCDCTL_API=3 etcdctl \\\n    --endpoints=https://[127.0.0.1]:2379 \\\n    --cacert=/var/lib/minikube/certs//etcd/ca.crt \\\n    --cert=/var/lib/minikube/certs//etcd/healthcheck-client.crt \\\n    --key=/var/lib/minikube/certs//etcd/healthcheck-client.key \\\n    put $YOUR_OBJECT_PATH\n</code></pre>","title":"Step 5 - Encode and save data to ETCD"},{"location":"kubernetes/ETCD/editing-kubernetes-objects/#step-6-verify-the-object-is-valid-for-kubernetes","text":"<p>Just run <code>kubectl get OBJECT_KIND OBJECT_NAME -o yaml</code> on the object you just edited to ensure it is still in working order.</p> <p>If it returns the objects YAML, you are fine. In case of errors, such as <code>illegal bytes</code> or so, you should restore a backup ASAP!</p>","title":"Step 6 - Verify the object is valid for Kubernetes"},{"location":"kubernetes/ETCD/snapshots-save-restore/","text":"<p>Warning</p> <p>This page is only for ETCD version <code>3.x</code> and higher!</p>   <p>For the original commands and more information on ETCD, see https://coreos.com/etcd/docs/latest/op-guide/recovery.html.</p>","title":"Snapshots: Save & Restore"},{"location":"kubernetes/ETCD/snapshots-save-restore/#take-a-snapshot","text":"<pre><code>ETCDCTL_API=3 etcdctl \\\n    --endpoints $ETCD_ENDPOINT \\\n    snapshot save snapshot.db\n</code></pre>  <p>(where <code>snapshot.db</code> is the name of the snapshot file to be created)</p>","title":"Take a snapshot"},{"location":"kubernetes/ETCD/snapshots-save-restore/#restore-a-snapshot","text":"<p>Danger</p> <p>Before restoring a snapshot, all ETCDs in the cluster must be stopped!</p>  <p>You must rename/remove the current data dir (probably <code>/var/lib/etcd</code>).</p> <p>Be sure to provide all flags that are specified in, e.g., systemd unit file, Kubespray: <code>/etc/etcd.env</code> and others otherwise you may create issues for the ETCD cluster!</p> <p>The command is looking about like that depending on what flags are used for your ETCD node:</p> <pre><code># Run the command as `root` user after that use `chown` to correct ownership of files\nETCDCTL_API=3 etcdctl \\\n    snapshot restore snapshot.db \\\n    --name m1 \\\n    --initial-cluster m1=http://host1:2380,m2=http://host2:2380,m3=http://host3:2380 \\\n    --initial-cluster-token etcd-cluster-1 \\\n    --initial-advertise-peer-urls http://host1:2380\n    --data-dir=/var/lib/etcd\n\n# chown etcd:etcd -R /var/lib/etcd\n</code></pre>  <p>This has to be done on all ETCD servers one by one with each having their own name given by flag as they were when the snapshot was taken.</p>","title":"Restore a snapshot"},{"location":"kubernetes/logging/regex/","text":"<p>Info</p> <p>In general make sure to forward the audit logs to your log store of choice.</p> <p>Kubernetes docs: https://kubernetes.io/docs/tasks/debug-application-cluster/audit/</p>   <p>Some neat regex to parse certain messages from Kubernetes logs.</p>","title":"Regex"},{"location":"kubernetes/logging/regex/#rbac-deny-messages","text":"<p>Good to setup some log alerting on those messages to make sure the applications are not hammering the API servers with \"bad\" RBAC.</p> <p>Match message (should be enough for matching): <pre><code>\\] RBAC DENY:\n</code></pre> </p> <p>Rewriting into a comma separated list + showing occurence counts: <pre><code>perl -n -e'/\\] RBAC DENY: user \"(.+)\" groups \\[(\".+\")\\] cannot \"([a-zA-Z]+)\" resource \"([a-zA-Z._-]+)\" in namespace \"([a-zA-Z-_]+)\"/ &amp;&amp; print \"ns=$5,verb=$3,resource=$4,user=$1,groups=$2\\n\"' | sort | uniq -c\n</code></pre> </p>","title":"<code>RBAC DENY</code> Messages"},{"location":"kubernetes/monitoring/","text":"<p>Use Prometheus for in-cluster monitoring!</p> <p>If you need a centralized view of multiple Kubernetes clusters, checkout Thanos.</p>","title":"Monitoring"},{"location":"kubernetes/monitoring/components/","text":"<p>Infrastructure / Cluster components should be monitored separately from your applications. This allows you to \"kill\" the application Prometheus in case you have screwed up in some way (e.g., messed up application metrics causing to have a billion labeled metrics).</p>","title":"Components"},{"location":"kubernetes/monitoring/components/#master-components","text":"","title":"Master Components"},{"location":"kubernetes/monitoring/components/#etcd","text":"<ul> <li>Summary: \"Why do my kubectl commands take so long?\"</li> <li>Port: <code>2379/TCP</code></li> <li>Path: <code>/metrics</code></li> <li>Auth: Client Certificate<ul> <li>Notes:<ul> <li>Thanks to etcd having a role concept, can be a \"separate\" user with just metrics access.</li> <li>Additionally one might want to run a Kubernetes authenticated OAuth proxy in front of th etcd so that the (one or more) Prometheus can be granted access to it by RoleBinding a Role to the ServiceAccount.</li> <li>E.g., you can create a Secret with the ETCD certificate and key using the <code>kubectl create secret generic --from-file=.../ca.crt --from-file=.../monitoring.crt  --from-file=.../monitoring.key</code><ul> <li>Be sure to mount that Secret inside your Prometheus instance and adjust the path(s) according to the <code>mountPath</code>.</li> </ul> </li> <li>Prometheus Config Scrape Job Reference, see References Prometheus Kubernetes ETCD Scrape Job Config.</li> </ul> </li> </ul> </li> <li>What can the Metrics tell us:<ul> <li>ETCD disk write latencies (they should be low, very low).</li> <li>ETCD Quorum status as well (+ some other metrics, e.g., how many requests / streams).</li> <li>Golang Process information (e.g., CPU, memory, GC status).</li> </ul> </li> </ul>","title":"etcd"},{"location":"kubernetes/monitoring/components/#kube-apiserver","text":"<ul> <li>Summary: \"Why do my kubectl commands take so long?\"</li> <li>Port: <code>6443/TCP</code> (depends on your installation)</li> <li>Path: <code>/metrics</code></li> <li>Auth: ServiceAccount token.<ul> <li>Note(s):<ul> <li>RBAC ClusterRole needed, see References Prometheus ClusterRole and References Prometheus Per-Namespace Role.</li> </ul> </li> </ul> </li> <li>What can the Metrics tell us:<ul> <li>API Request latencies (kubectl, operators, scheduler, controller, etc).<ul> <li>Best used with <code>histogram_quantile()</code> Prometheus func over some time.</li> </ul> </li> <li>Admission step and webhook controller durations and latencies.</li> <li>ETCD request metrics.</li> <li>API server cache metrics.</li> <li>Kubernetes API Rest Client latency, requests and duration metrics (own requests made).</li> <li>Golang Process information (e.g., CPU, memory, GC status).</li> </ul> </li> </ul>","title":"kube-apiserver"},{"location":"kubernetes/monitoring/components/#kube-scheduler","text":"<ul> <li>Summary: \"How long do my Pods take to be scheudled?\" (Hints at \"you need to change your scheduler config\")</li> <li>Port: <code>443/TCP</code> (depends on your installation)</li> <li>Path: <code>/metrics</code></li> <li>Auth: ServiceAccount token.<ul> <li>Note(s):<ul> <li>RBAC ClusterRole needed, see References Prometheus ClusterRole and References Prometheus Per-Namespace Role.</li> <li>The kube-scheduler needs to either listen on <code>::</code> (<code>0.0.0.0</code>) or have a proxy which is available to Prometheus for scraping running.</li> </ul> </li> </ul> </li> <li>What can the Metrics tell us:<ul> <li>\"How long do my Pods take to be scheudled?\" (<code>scheduler_binding_*</code>)</li> <li>Pod Preemption Algorithm latencies and more.</li> <li>Volume scheduling duration.</li> <li>Kubernetes API Rest Client latency, requests and duration metrics (own requests made to the API).</li> <li>Golang Process information (e.g., CPU, memory, GC status).</li> </ul> </li> </ul>","title":"kube-scheduler"},{"location":"kubernetes/monitoring/components/#kube-controller-manager","text":"<p>Info</p> <p>If the cloud controller is used, it should also be monitored / scraped for metrics.</p>  <ul> <li>Summary: \"How long do my Pods take to be scheudled?\" (Hints at \"you need to change your scheduler config\")</li> <li>Port: <code>443/TCP</code> (depends on your installation)</li> <li>Path: <code>/metrics</code></li> <li>Auth: ServiceAccount token.<ul> <li>Note(s):<ul> <li>RBAC ClusterRole needed, see References Prometheus ClusterRole and References Prometheus Per-Namespace Role.</li> <li>The kube-controller-manager needs to either listen on <code>::</code> (<code>0.0.0.0</code>) or have a proxy which is available to Prometheus for scraping running.</li> </ul> </li> </ul> </li> <li>What can the Metrics tell us:<ul> <li>Work queue item count and duration, and lease holder status.</li> <li>Kubernetes API Rest Client latency, requests and duration metrics (own requests made to the API).</li> <li>Golang Process information (e.g., CPU, memory, GC status).</li> </ul> </li> </ul>","title":"kube-controller-manager"},{"location":"kubernetes/monitoring/components/#kubelet-cadvisor","text":"<p>See Node Components - kubelet.</p>","title":"kubelet (+ cadvisor)"},{"location":"kubernetes/monitoring/components/#kube-proxy","text":"<p>See Node Components - kube-proxy.</p>","title":"kube-proxy"},{"location":"kubernetes/monitoring/components/#sdn-eg-calico-cilium","text":"<p>See Node Components - kube-proxy.</p>","title":"SDN (e.g., Calico, Cilium)"},{"location":"kubernetes/monitoring/components/#node-components","text":"","title":"Node Components"},{"location":"kubernetes/monitoring/components/#kubelet-cadvisor_1","text":"<ul> <li>Summary: \"How long do my Pods take to be scheudled?\" (Hints at \"you need to change your scheduler config\")</li> <li>Port: <code>443/TCP</code> (depends on your installation)</li> <li>Path: <code>/metrics</code></li> <li>Auth: ServiceAccount token.<ul> <li>Note(s):<ul> <li>RBAC ClusterRole needed, see References Prometheus ClusterRole and References Prometheus Per-Namespace Role.</li> <li>kubelet needs to have the following flags active: <code>--authorization-mode=Webhook</code> and <code>--authentication-token-webhook=true</code>.</li> </ul> </li> </ul> </li> <li>What can the Metrics tell us:<ul> <li><code>kubelet_node_config_error</code> good to know if the latest config works.</li> <li>Kubelet PLEG (\"container runtime status\") metrics.</li> <li>\"Pod Start times\" (use <code>histogram_quantile()</code>).</li> <li>CGroup metrics of containers (cpu, memory, network) with Namespace and Pod labels.</li> <li>Kubernetes API Rest Client latency, requests and duration metrics (own requests made to the API).</li> <li>Golang Process information (e.g., CPU, memory, GC status).</li> </ul> </li> </ul>","title":"kubelet (+ \"cadvisor\")"},{"location":"kubernetes/monitoring/components/#kube-proxy_1","text":"<ul> <li>Summary: \"How long do my Pods take to be scheudled?\" (Hints at \"you need to change your scheduler config\")</li> <li>Port: <code>10250/TCP</code> (depends on your installation)</li> <li>Path: <code>/metrics</code></li> <li>Auth: ServiceAccount token.<ul> <li>Note(s):<ul> <li>RBAC ClusterRole needed, see References Prometheus ClusterRole and References Prometheus Per-Namespace Role.</li> </ul> </li> </ul> </li> <li>What can the Metrics tell us:<ul> <li><code>iptables</code> and / or <code>ipvs</code> sync information (~= how long does it take for Service changes to be reflected in the \"routing\" rules).</li> <li>Kubernetes API Rest Client latency, requests and duration metrics (own requests made to the API).</li> <li>Golang Process information (e.g., CPU, memory, GC status).</li> </ul> </li> </ul>","title":"kube-proxy"},{"location":"kubernetes/monitoring/components/#sdn-cni-eg-calico-cilium","text":"<p>Depends on the SDN / CNI used, if there are metrics available. Calico for example can expose metrics, but that must be enabled through a environemnt variable on the Calico Node DaemonSet.</p> <p>For other SDNs, e.g., OpenVSwitch you may need to use an \"external\" exporter when available:</p> <ul> <li>https://github.com/digitalocean/openvswitch_exporter</li> <li> <p>https://github.com/ovnworks/ovn_exporter</p> </li> <li> <p>What can the Metrics tell us:</p> <ul> <li>Depending on the exporter, at least how much traffic is flowing and / or if there are issues with the daemon.</li> </ul> </li> </ul>","title":"SDN / CNI (e.g., Calico, Cilium)"},{"location":"kubernetes/monitoring/components/#the-nodes-themself","text":"","title":"The Nodes themself"},{"location":"kubernetes/monitoring/components/#node_exporter","text":"<p>See Monitoring/Prometheus/Exporters - node_exporter.</p>","title":"node_exporter"},{"location":"kubernetes/monitoring/components/#ethtool_exporter","text":"<p>See Monitoring/Prometheus/Exporters - ethtool_exporter.</p>","title":"ethtool_exporter"},{"location":"kubernetes/monitoring/components/#additional-in-cluster-components","text":"<p>Other components that are in and / or around a Kubernetes cluster.</p>","title":"Additional In-Cluster Components"},{"location":"kubernetes/monitoring/components/#metrics-server-previously-named-heapster","text":"<p>(More information https://kubernetes.io/docs/tasks/debug-application-cluster/resource-metrics-pipeline/)</p> <ul> <li>Summary: \"How long do my Pods take to be scheudled?\" (Hints at \"you need to change your scheduler config\")</li> <li>Port: <code>443/TCP</code></li> <li>Path: <code>/metrics</code></li> <li>Auth: None. (Recommendation: Add Kubernetes OAuth Proxy in front)</li> <li>What can the Metrics tell us:<ul> <li>Possibly metrics on how</li> <li>Golang Process information (e.g., CPU, memory, GC status).</li> </ul> </li> </ul>","title":"metrics-server (previously named heapster)"},{"location":"kubernetes/monitoring/components/#kube-state-metrics","text":"<ul> <li>Summary: \"How long do my Pods take to be scheudled?\" (Hints at \"you need to change your scheduler config\")</li> <li>Port: <code>8080/TCP</code> for \"cluster\" metrics and <code>8081/TCP</code> for kube-state-metrics metrics. (Recommended to scrape both)</li> <li>Path: <code>/metrics</code></li> <li>Auth: ServiceAccount token.<ul> <li>Note(s):<ul> <li>RBAC ClusterRole needed, see References Prometheus ClusterRole and References Prometheus Per-Namespace Role.</li> </ul> </li> </ul> </li> <li>What can the Metrics tell us:<ul> <li>Metrics about Deployments, StatefulSets, Jobs, CronJobs, and basically any other objects <code>Status</code> (that is in the official Kubernetes APIs).</li> <li>Golang Process information (e.g., CPU, memory, GC status).</li> </ul> </li> </ul>","title":"kube-state-metrics"},{"location":"kubernetes/monitoring/components/#other-components","text":"","title":"Other Components"},{"location":"kubernetes/monitoring/components/#elasticsearch","text":"<p>Elasticsearch is not providing Prometheus metrics itself, but there is a well written exporter GitHub justwatchcom/elasticsearch_exporter. (There are some other exporters also available, though I have used mainly used this one for the amount of metrics I'm able to get from Elasticsearch with it)</p> <ul> <li>Summary: \"Is my Elasticsearch able to ingest the amount of logs? Do I need to add more data nodes and / or resources?\"</li> <li>Port: <code>9114/TCP</code> (depends on your installation)</li> <li>Path: <code>/metrics</code></li> <li>Auth: ServiceAccount token<ul> <li>Note(s):<ul> <li>RBAC ClusterRole needed, see References Prometheus ClusterRole and References Prometheus Per-Namespace Role.</li> </ul> </li> </ul> </li> <li>What can the Metrics tell us:<ul> <li>Elasticsearch cluster status.<ul> <li>Resources (CPU and Memory) and also Storage usage.</li> </ul> </li> <li>Elasticsearch Indices status (when enabled (can be filtered in different ways)).</li> <li>Elasticsearch JVM info (GC, memory, etc).</li> <li>Checkout the metrics list for a list of all Metrics available: https://github.com/justwatchcom/elasticsearch_exporter#metrics</li> <li>Golang Process information (e.g., CPU, memory, GC status).</li> </ul> </li> </ul>","title":"Elasticsearch"},{"location":"kubernetes/monitoring/components/#prometheus","text":"<ul> <li>Summary: \"Does my Prometheus have enough resources? Can I take in another X-thousand / million metrics?\"</li> <li>Port: <code>9090/TCP</code> (depends on your installation)</li> <li>Path: <code>/metrics</code></li> <li>Auth: None. (Recommendation: Add Kubernetes OAuth Proxy in front)</li> <li>What can the Metrics tell us:<ul> <li>Is my Prometheus doing \"Okay\", e.g., have enough resources<ul> <li>Can be used to see if the Prometheus is able to take in another X-thousand / million metrics.</li> </ul> </li> <li>Golang Process information (e.g., CPU, memory, GC status).</li> </ul> </li> </ul>","title":"Prometheus"},{"location":"kubernetes/monitoring/components/#references","text":"","title":"References"},{"location":"kubernetes/monitoring/components/#prometheus-clusterrole","text":"<pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: prometheus-infra\nrules:\n- apiGroups:\n  - \"\"\n  resources:\n  - nodes/metrics\n  verbs:\n  - get\n- nonResourceURLs:\n  - /metrics\n  - /metrics/cadvisor\n  verbs:\n  - get\n</code></pre>","title":"Prometheus ClusterRole"},{"location":"kubernetes/monitoring/components/#prometheus-per-namespace-role","text":"<pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: prometheus\n  namespace: YOUR_NAMESPACE\nrules:\n- apiGroups:\n  - \"\"\n  resources:\n  - services\n  - endpoints\n  - pods\nverbs:\n- get\n- list\n- watch\n</code></pre>","title":"Prometheus Per-Namespace Role"},{"location":"kubernetes/monitoring/components/#prometheus-kubernetes-etcd-scrape-job-config","text":"<p>Info</p> <p>This selects the master Nodes based on the <code>node-role.kubernetes.io/control-plane</code> (old label <code>node-role.kubernetes.io/master</code>) label. So be sure to have it set on the control plane nodes (e.g., kubeadm automatically sets it on control plane nodes).</p> <p>For more information see Kubernetes Cheat Sheet - Role Label for Node objects.</p>  <p>I can only recommend you to use the Prometheus Community Helm Charts for this and checking out their documentation regarding the ETCD certificates / credentials required for accessing the metrics.</p> <ul> <li>https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-prometheus-stack</li> <li>https://github.com/prometheus-operator/kube-prometheus</li> <li>https://github.com/prometheus-community/helm-charts/issues/204#issuecomment-765155883</li> </ul>","title":"Prometheus Kubernetes ETCD Scrape Job Config"},{"location":"kubernetes/networking/benchmarking/","text":"","title":"Benchmarking"},{"location":"kubernetes/networking/benchmarking/#network","text":"<ul> <li>https://github.com/kubernetes/perf-tests/tree/master/network/benchmarks/netperf</li> </ul>","title":"Network"},{"location":"kubernetes/networking/benchmarking/#dns","text":"<ul> <li>https://github.com/DNS-OARC/flamethrower</li> </ul>","title":"DNS"},{"location":"kubernetes/networking/explained/","text":"","title":"Explained"},{"location":"kubernetes/networking/explained/#assumptions","text":"<ul> <li>Basic network knowledge (e.g. CIDR, Source and Destination NAT)</li> <li>Basic <code>iptables</code> knowledge</li> <li>Pod/Cluster CIDR: <code>100.64.0.0/13</code><ul> <li>Every node gets a <code>/24</code> <code>podCIDR</code>.</li> </ul> </li> <li>Service IP CIDR: <code>100.72.0.0/16</code></li> <li>Node IP CIDR: <code>10.10.10.0/24</code></li> </ul> <p>More info on IP Classless Inter-Domain Routing (short CIDR):</p> <ul> <li>IPv4 Classless Inter-Domain Routing</li> <li>IPv6 Classless Inter-Domain Routing</li> </ul> <p>The source for the diagrams, can be found as <code>.graphml</code> at the same path as the images.</p> <p>Example: <code>kubernetes-networking-explained-network_stack.svg</code> -&gt; <code>kubernetes-networking-explained-network_stack.graphml</code></p>","title":"Assumptions"},{"location":"kubernetes/networking/explained/#network-stack","text":"Network Overview caption","title":"Network Stack"},{"location":"kubernetes/networking/explained/#traffic-flow","text":"","title":"Traffic Flow"},{"location":"kubernetes/networking/explained/#pod-to-pod","text":"Pod to Pod Traffic","title":"Pod to Pod"},{"location":"kubernetes/networking/explained/#pod-to-service-ip","text":"Pod to Service IP","title":"Pod to Service IP"},{"location":"kubernetes/networking/explained/#service-ip-iptables","text":"Service IP iptables","title":"Service IP iptables"},{"location":"kubernetes/networking/explained/#nodeport-to-service-ip-to-pod","text":"NodePort to Service IP to Pod","title":"NodePort to Service IP to Pod"},{"location":"kubernetes/networking/troubleshooting/","text":"","title":"Troubleshooting"},{"location":"kubernetes/networking/troubleshooting/#general-connectivity-test","text":"<p>Kuberang is a simple but efficient way to quickly check cluster network connectivity. To quote from the project's README:</p>  <p>Quote from GitHub apprenda/kuberang README.md:</p> <ul> <li>Has kubectl installed correctly with access controls</li> <li>Has active kubernetes namespace (if specified)</li> <li>Has available workers</li> <li>Has working pod &amp; service networks</li> <li>Has working pod &lt;-&gt; pod DNS</li> <li>Has working master(s)</li> <li>Has the ability to access pods and services from the node you run it on.</li> </ul>  <p>You can run it:</p> <p><pre><code>kuberang\n</code></pre>  This will start the test Deployments and Services.</p>","title":"General connectivity test"},{"location":"kubernetes/networking/troubleshooting/#dns-server","text":"<ul> <li>https://github.com/DNS-OARC/flamethrower</li> </ul>","title":"DNS Server"},{"location":"linux/git/","text":"","title":"git"},{"location":"linux/git/#removes-any-files-and-directories-not-commited","text":"<pre><code># `-i` will run the cleaning process interactively\n$ git clean -i -d\n# Setting `-f` instead of `-i` will remove everything \"not commited\" automatically\n$ git clean -f -d\n</code></pre>","title":"Removes Any Files and Directories Not Commited"},{"location":"linux/mdam/","text":"<p>Note</p> <p>Don't forget to keep your <code>mdadm.conf</code> uptodate when creating, modifiying, deleting mdadm arrays.</p>","title":"mdadm"},{"location":"linux/mdam/#generate-mdadmconf","text":"<pre><code>mdadm --detail --scan &gt;&gt; /etc/mdadm.conf\n</code></pre>","title":"Generate <code>mdadm.conf</code>"},{"location":"linux/mdam/#grow-raid-5-to-raid-6","text":"<p>Danger</p> <p>DON'T FORGET TO SPECIFY THE <code>--backup-file=FILE</code> for <code>mdadm --grow</code> operations! Otherwise if the host is (forced) shutdowned (e.g., power failure), data can / will be lost.</p>  <p>(This \"backup file\" should be on different disk / storage, not on the mdadm array you are growing!)</p>","title":"Grow RAID 5 to RAID 6"},{"location":"linux/mdam/#speed-up-raid-rebuild","text":"<p>Note</p> <p>This may or may not improve your mdadm RAID rebuild performance, as it the speed depends on many factors such as disk speeds, etc.</p>  <p>This assumes your disks are <code>sda</code>, <code>sdb</code> and <code>sdc</code>, and the RAID array is <code>md0</code> (<code>/dev/md0</code>).</p> <pre><code>for disk in sd{a..c}; do\n    blockdev --setra 16384 \"/dev/${disk}\"\n    echo 1024 &gt; \"/sys/block/${disk}/queue/read_ahead_kb\"\n    echo 256 &gt; \"/sys/block/${disk}/queue/nr_requests\"\n    # Disable NCQ on all disks.\n    echo 1 &gt; \"/sys/block/${disk}/device/queue_depth\"\ndone\n\n# Set read-ahead to 64 MiB for /dev/md0\nblockdev --setra 65536 /dev/md0\n\n# Set stripe_cache_size to 16 MiB for /dev/md0\necho 16384 &gt; /sys/block/md0/md/stripe_cache_size\n</code></pre>","title":"Speed-up RAID rebuild"},{"location":"linux/mdam/#references","text":"<ul> <li>mdadm man page</li> </ul>","title":"References"},{"location":"linux/quick-commands/","text":"","title":"Quick Commands"},{"location":"linux/quick-commands/#archives","text":"","title":"Archives"},{"location":"linux/quick-commands/#extract-all-zip-files-into-a-directory-named-after-the-zips-filename","text":"<pre><code>find -name '*.zip' -exec sh -c 'unzip -d \"${1%.*}\" \"$1\"' _ {} \\;\n</code></pre>","title":"Extract all '*.zip' files into a directory named after the zip's filename"},{"location":"linux/quick-commands/#extract-all-rar-files-into-a-directry-named-after-the-rars-filename","text":"<pre><code>find -name '*.rar' -exec sh -c 'mkdir \"${1%.*}\"; unrar e \"$1\" \"${1%.*}\"' _ {} \\;\n</code></pre>","title":"Extract all '*.rar' files into a directry named after the rar's filename"},{"location":"linux/quick-commands/#extract-all-7z-files-into-a-directry-named-after-the-rars-filename","text":"<pre><code>find -name '*.7z' -exec sh -c 'mkdir \"${1%.*}\"; 7z x \"$1\" -o\"${1%.*}\"' _ {} \\;\n</code></pre>","title":"Extract all '*.7z' files into a directry named after the rar's filename"},{"location":"linux/quick-commands/#extrat-all-tar-files-into-a-directory-named-after-the-tars-filename","text":"<pre><code>find -name '*.tar' -exec sh -c 'mkdir -p \"${1%.*}\"; tar -C \"${1%.*}\" -xvf \"$1\"' _ {} \\;\n</code></pre>","title":"Extrat all '*.tar' files into a directory named after the tar's filename"},{"location":"linux/quick-commands/#extrat-all-targz-files-into-a-directory-named-after-the-tars-filename","text":"<pre><code>find -name '*.tar.gz' -or -name '*.tgz' -exec sh -c 'mkdir -p \"${1%.*}\"; tar -C \"${1%.*}\" -xvzf \"$1\"' _ {} \\;\n</code></pre>","title":"Extrat all '*.tar.gz' files into a directory named after the tar's filename"},{"location":"linux/quick-commands/#music","text":"","title":"Music"},{"location":"linux/quick-commands/#convert-all-flac-to-mp3-same-directory","text":"<pre><code>find -name \"*.flac\" -print | parallel -j 14 ffmpeg -i {} -acodec libmp3lame -ab 192k {.}.mp3 \\;\n</code></pre>","title":"Convert all FLAC to MP3 (same directory)"},{"location":"linux/quick-commands/#convert-all-ogg-to-flac-same-directory","text":"<pre><code>find -name \"*.ogg\" -print | parallel -j 14 ffmpeg -i {} -c:a flac {.}.flac \\;\n</code></pre>","title":"Convert all OGG to FLAC (same directory)"},{"location":"linux/quick-commands/#documents","text":"","title":"Documents"},{"location":"linux/quick-commands/#convert-pdfs-to-pngs-each-page-is-its-own-image","text":"<pre><code>for file in *.pdf; do\n    echo \"Processing file: $file ...\"\n    mkdir -p \"$(basename \"$file\" .pdf)\"\n    pdftoppm -png \"$file\" \"$(basename \"$file\" .pdf)/page\"\ndone\n</code></pre>","title":"Convert PDFs to PNGs (each page is its own image)"},{"location":"linux/quick-commands/#run-tesseract-ocr-on-all-converted-pages","text":"<pre><code>for file in */*.png; do\n    echo \"Processing file: $file ...\"\n    tesseract -l deu+eng \"$file\" \"$(echo \"$file\" | sed 's/\\.png$//g')\"\ndone\n</code></pre>   <p>Info</p> <p>The <code>-l deu+eng</code> are the languages to use.  In this case <code>deu+eng</code> means \"Deutsch\" (German) and \"English\".</p>","title":"Run <code>tesseract</code> OCR on all converted Pages"},{"location":"linux/quick-commands/#convert-all-docx-files-into-pdfs-using-libreoffices-lowriter","text":"<pre><code>find . -name '*.docx' -print0 |\n    while IFS= read -r -d $'\\0' line; do\n        echo \"Processing file: $line ...\"\n        lowriter --convert-to pdf \"$line\" --outdir \"$(dirname \"$line\")\"\n    done\n</code></pre>","title":"Convert all '*.docx' files into PDFs (using LibreOffice's <code>lowriter</code>)"},{"location":"linux/quick-commands/#disks","text":"","title":"Disks"},{"location":"linux/quick-commands/#get-uuid-for-partition","text":"<pre><code>blkid /dev/sdXY -s UUID -o value\n</code></pre>  <p>Where <code>/dev/sdXY</code> could be, <code>/dev/sda2</code>, <code>/dev/nvme0n1p1</code>, and so on.</p>","title":"Get UUID for partition"},{"location":"linux/quick-commands/#images","text":"","title":"Images"},{"location":"linux/quick-commands/#optimize-jpeg-images","text":"<p>Warning</p> <p>The <code>-m LEVEL</code> flag reduces the JPEG image quality to that level, in the example below to <code>95</code>.</p>  <pre><code>$ jpegoptim -p --strip-com --strip-iptc -m 95 IMAGE.jpeg\n# Find and optimize PNGs in parallel\n$ find -iname '*.jpg' -iname '*.jpeg' -print0 | xargs -n1 -P6 -0 jpegoptim -p --strip-com --strip-iptc -m 95\n</code></pre>","title":"Optimize JPEG Images"},{"location":"linux/quick-commands/#optimize-png-images","text":"<pre><code>$ jpegoptim\n# Find and optimize PNGs in parallel\n$ find -iname '*.png' -print0 | xargs -n1 -P6 -0 optipng -strip all -clobber -fix -o9\n</code></pre>","title":"Optimize PNG Images"},{"location":"linux/quick-commands/#remove-exif-data-from-images","text":"<pre><code>$ exiftool -overwrite_original -all= IMAGE1.jpeg IMAGE2.png ...\n# Remove EXIF data from all `*.jpeg` files\n$ find -iname '*.jpeg' -exec exiftool -overwrite_original -all= {} \\;\n</code></pre>","title":"Remove EXIF data from Image(s)"},{"location":"linux/sysctl/","text":"<p>See GitHub Gist <code>90-edenmal-custom.conf</code> for more information on the used <code>sysctl</code> settings / values.</p> <p>The sysctl can be easily using the following command:</p> <pre><code>curl -L https://gist.githubusercontent.com/galexrt/8faa48a05bab303ec922bd89e8f7adc5/raw/90-edenmal-custom.conf -o /etc/sysctl.d/90-edenmal-custom.conf\nsysctl --system\n</code></pre>    <p>Info</p> <p>The below list might be outdated, please check the GitHub Gist linked above for the latest version.</p>  <pre><code>fs.aio_max_nr = 1048576\nfs.file-max = 2097152\nfs.inotify.max_user_instances = 5120\nfs.inotify.max_user_watches = 1572864\nfs.nr_open = 3145728\nfs.suid_dumpable = 0\nkernel.core_uses_pid = 1\nkernel.dmesg_restrict = 1\nkernel.exec-shield = 2\nkernel.panic_on_oops = 1\nkernel.panic = 10\nkernel.pid_max = 4194303\nkernel.randomize_va_space = 2\nkernel.sched_autogroup_enabled = 0\nkernel.sched_migration_cost = 5000000\nkernel.sysrq = 0\nnet.core.default_qdisc = fq\nnet.core.netdev_budget = 600\nnet.core.netdev_max_backlog = 65536\nnet.core.optmem_max = 4048000\nnet.core.rmem_default = 266240\nnet.core.rmem_max = 4048000\nnet.core.somaxconn = 65536\nnet.core.wmem_default = 266240\nnet.core.wmem_max = 4048000\nnet.ipv4.conf.all.accept_redirects = 0\nnet.ipv4.conf.all.accept_source_route = 0\nnet.ipv4.conf.all.bootp_relay = 0\nnet.ipv4.conf.all.forwarding = 1\nnet.ipv4.conf.all.igmpv2_unsolicited_report_interval = 10000\nnet.ipv4.conf.all.igmpv3_unsolicited_report_interval = 1000\nnet.ipv4.conf.all.ignore_routes_with_linkdown = 0\nnet.ipv4.conf.all.log_martians = 1\nnet.ipv4.conf.all.proxy_arp = 0\nnet.ipv4.conf.all.rp_filter = 2\nnet.ipv4.conf.all.secure_redirects = 1\nnet.ipv4.conf.all.send_redirects = 0\nnet.ipv4.conf.default.accept_redirects = 0\nnet.ipv4.conf.default.accept_source_route = 0\nnet.ipv4.conf.default.forwarding = 1\nnet.ipv4.conf.default.log_martians = 1\nnet.ipv4.conf.default.rp_filter = 2\nnet.ipv4.conf.default.secure_redirects = 1\nnet.ipv4.conf.default.send_redirects = 0\nnet.ipv4.conf.lo.accept_source_route = 1\nnet.ipv4.fwmark_reflect = 0\nnet.ipv4.icmp_echo_ignore_all = 0\nnet.ipv4.icmp_echo_ignore_broadcasts = 1\nnet.ipv4.icmp_ignore_bogus_error_responses = 1\nnet.ipv4.icmp_msgs_burst = 50\nnet.ipv4.icmp_msgs_per_sec = 1000\nnet.ipv4.ip_forward = 1\nnet.ipv4.ip_local_port_range = 1024 65535\nnet.ipv4.ipfrag_secret_interval = 600\nnet.ipv4.neigh.default.gc_thresh1 = 4048\nnet.ipv4.neigh.default.gc_thresh2 = 6144\nnet.ipv4.neigh.default.gc_thresh3 = 8192\nnet.ipv4.netfilter.ip_conntrack_tcp_timeout_syn_recv = 45\nnet.ipv4.netfilter.nf_conntrack_generic_timeout = 300\nnet.ipv4.netfilter.nf_conntrack_tcp_timeout_time_wait = 60\nnet.ipv4.tcp_abort_on_overflow = 1\nnet.ipv4.tcp_congestion_control = bbr\nnet.ipv4.tcp_fin_timeout = 10\nnet.ipv4.tcp_keepalive_intvl = 25\nnet.ipv4.tcp_keepalive_probes = 5\nnet.ipv4.tcp_keepalive_time = 420\nnet.ipv4.tcp_max_syn_backlog = 4096\nnet.ipv4.tcp_max_tw_buckets = 160000\nnet.ipv4.tcp_moderate_rcvbuf = 1\nnet.ipv4.tcp_no_metrics_save = 1\nnet.ipv4.tcp_notsent_lowat = 16384\nnet.ipv4.tcp_rfc1337 = 1\nnet.ipv4.tcp_rmem = 4096 87380 8388608\nnet.ipv4.tcp_sack = 1\nnet.ipv4.tcp_slow_start_after_idle = 0\nnet.ipv4.tcp_syn_retries = 2\nnet.ipv4.tcp_synack_retries = 3\nnet.ipv4.tcp_syncookies = 1\nnet.ipv4.tcp_timestamps = 1\nnet.ipv4.tcp_tw_recycle = 0\nnet.ipv4.tcp_tw_reuse = 1\nnet.ipv4.tcp_window_scaling = 1\nnet.ipv4.tcp_wmem = 4096 87380 8388608\nnet.ipv4.udp_rmem_min = 16384\nnet.ipv4.udp_wmem_min = 16384\nnet.ipv4.vs.conn_reuse_mode = 1\nnet.ipv4.vs.conntrack = 1\nnet.ipv4.vs.expire_nodest_conn = 1\nnet.ipv4.vs.sloppy_tcp = 1\nnet.ipv6.conf.all.accept_ra_defrtr = 0\nnet.ipv6.conf.all.accept_ra_pinfo = 0\nnet.ipv6.conf.all.accept_ra = 0\nnet.ipv6.conf.all.accept_redirects = 0\nnet.ipv6.conf.all.accept_source_route = 0\nnet.ipv6.conf.all.forwarding = 1\nnet.ipv6.conf.default.accept_ra_defrtr = 0\nnet.ipv6.conf.default.accept_ra_pinfo = 0\nnet.ipv6.conf.default.accept_ra_rtr_pref = 0\nnet.ipv6.conf.default.accept_redirects = 0\nnet.ipv6.conf.default.accept_source_route = 0\nnet.ipv6.conf.default.autoconf = 0\nnet.ipv6.conf.default.dad_transmits = 0\nnet.ipv6.conf.default.forwarding = 1\nnet.ipv6.conf.default.max_addresses = 16\nnet.ipv6.conf.default.router_solicitations = 0\nnet.ipv6.ip6frag_secret_interval = 600\nnet.ipv6.route.max_size = 16384\nnet.ipv6.xfrm6_gc_thresh = 32768\nnet.netfilter.nf_conntrack_expect_max = 2048\nnet.netfilter.nf_conntrack_max = 1024000\nnet.netfilter.nf_conntrack_tcp_timeout_established = 600\nnet.nf_conntrack_max = 1024000\nvm.mmap_rnd_bits=32\nvm.mmap_rnd_compat_bits=16\nvm.overcommit_memory = 1\nvm.overcommit_ratio = 20\nvm.panic_on_oom = 0\n</code></pre>","title":"sysctl"},{"location":"linux/GRUB/preparations-for-boot-xyz-file/","text":"<p>Danger</p> <p>Only run the following commands if you know what they are doing!</p> <p>If you already have GRUB installed and working, you probably just need to edit your <code>grub.cfg</code> file (for most OSes in the <code>/boot</code> directory).</p>  <p>The disk to be used should be:</p> <ul> <li>Boot Target in BIOS / UEFI</li> <li>First in the BIOS / UEFI boot order</li> </ul> <p>The disk used in this example is <code>/dev/sda</code>.</p> <ol> <li>Prepare Disk partition layout<ul> <li>Create two partitions on the disk (use, e.g., <code>fdisk</code>, <code>parted</code>, etc)<ul> <li>1MB  (flags: <code>boot</code>)</li> <li>10G or more as you want / need (flags: <code>boot</code>), <code>ext4</code> formatted</li> </ul> </li> <li>Run <code>blkid /dev/sda2 -s UUID -o value</code> to get the UUID of the \"first\" disk's second partition.<ul> <li>Save the UUID of the \"first\" disk down. The UUID of the \"first\" disk will be used in form of the <code>__BOOT_PART_UUID__</code> later on.</li> </ul> </li> </ul> </li> <li>Mount \"boot\" Partition<ul> <li>Mount the second created partition (<code>/dev/sda2</code>): <code>mount /dev/sda2 /boot</code>.</li> </ul> </li> <li>Download Fedora <code>vmlinuz</code>, <code>initramfs</code> and <code>install.img</code> to <code>/boot</code> directory.<ul> <li>Prefix the downloaded files with <code>fedora-</code> (or whatever you want as long as you change it in the upcoming steps as well)</li> </ul> </li> <li> <p>Grub Installation</p> <ul> <li> <p>For GRUB2:</p> <pre><code>grub2-install --no-floppy /dev/sda2\n</code></pre>  </li> <li> <p>For GRUB:</p> </li> </ul> <pre><code>grub-install --no-floppy /dev/sda2\n</code></pre>  </li> <li> <p>Create GRUB boot config file</p> <ul> <li>For GRUB2 the path is <code>/boot/grub2/grub.cfg</code>.</li> <li>For GRUB the path is <code>/boot/grub/grub.cfg</code>.</li> </ul> </li> </ol> <p>Optional steps:</p> <ol> <li>Copy your Kickstart and / or config file to the <code>/boot</code> directory<ul> <li>This assumes the \"system\" you are using is able to open the <code>/boot</code> mounted partition and read the file from there, e.g., Kickstart can do it like that <code>inst.ks=\"hd:UUID=__BOOT_PART_UUID__:/ks.cfg\"</code> (where the <code>__BOOT_PART_UUID__</code> is the partition UUID).</li> </ul> </li> <li>Reboot and enjoy!</li> </ol>","title":"Preparations for 'Boot XYZ FIle'"},{"location":"linux/GRUB/Boot-XYZ-File/img-File/","text":"<p>Info</p> <p>You must have a working GRUB installation already, if not checkout the Preparations for Boot .</p>  <p>Example <code>grub.cfg</code> (for Fedora CoreOS installation):</p> <pre><code>set default=0\nset timeout=5\n# Fedora Kickstart Install\nmenuentry \"Fedora Kickstart Install\" {\n    search --no-floppy --fs-uuid __BOOT_PART_UUID__ --set root\n    # Add aditional kernel command line arguments at the end of the next line\n    linux /fedora-vmlinuz selinux=0 inst.resolution=800x600 inst.ks=\"hd:UUID=__BOOT_PART_UUID__:/ks.cfg\" inst.stage2=\"hd:UUID=__BOOT_PART_UUID__:/fedora-install.img\"\n    initrd /fedora-initrd.img\n}\n</code></pre>","title":".img File"},{"location":"linux/GRUB/Boot-XYZ-File/iso-File/","text":"<p>Info</p> <p>You must have a working GRUB installation already, if not checkout the Preparations for Boot .</p>  <p>Why would you need this? When you don't have a KVM or just don't want to use ILO, iDRAC, iPMI or other management tools to mount an ISO.</p> <p>This snippet is from a try to install Fedora CoreOS from an ISO file named <code>install.io</code> from the <code>/boot</code> partition.</p> <p>Example <code>grub.cfg</code> contents (<code>coreos.inst.install_dev=sda</code> would use the <code>sda</code> device for installation in case of *CoreOS):</p> <pre><code>set default=0\nset timeout=5\n# Fedora Kickstart Install\n menuentry \"Fedora Kickstart Install\" {\n    search --no-floppy --fs-uuid __BOOT_PART_UUID__ --set root\n    # Booting an ISO\n    loopback loop /install.iso\n    linux (loop)/images/vmlinuz coreos.inst=yes coreos.inst.install_dev=sda coreos.inst.ignition_url=http://example.com/example.ign\n    initrd (loop)/images/initramfs.img\n}\n</code></pre>  <p>It worked fine though the original case was to load the Ignition file from the boot disk which didn't work, a Webserver / Matchbox server was required to load the Ignition file from.</p>","title":".iso File"},{"location":"logging/loki/","text":"<p>Coming Soon</p>  <p>If you can use Loki instead of Elasticsearch, do it!</p> <p>From an operational perspective Loki can be run with a lot less resources than a whole EFK (Elasticsearch Fluentd Kibana) stack.</p>  <p>It just feels better not having to \"spend\" 3 servers to just run an Elasticsearch cluster for \"some logs\" which maybe \"once a month\" someone needs to take a look to investigate an issue with a Pod.</p>","title":"Loki"},{"location":"monitoring/thanos/","text":"<p>More info coming Soon</p>","title":"Thanos"},{"location":"monitoring/thanos/#important-hints","text":"<ul> <li>Thanos Compactor</li> <li>As of writing this, the compactor unless \"configured\" in a certain way must only be run once at the same time (NO PARALLEL)!</li> </ul>","title":"Important Hints"},{"location":"monitoring/prometheus/tips/","text":"","title":"Tips"},{"location":"monitoring/prometheus/tips/#multiple-exporter-on-one-server-but-only-one-port","text":"<p>(Let's not talk about the reasons, why one only has \"one\" port for such a case..)</p> <p>The following projects can be quite useful to accomplish this:</p> <ul> <li>https://github.com/rebuy-de/exporter-merger</li> </ul>","title":"Multiple Exporter on one Server but only one port?"},{"location":"monitoring/prometheus/exporters/dellhw_exporter/","text":"<ul> <li>Website / Source Code: https://github.com/galexrt/dellhw_exporter</li> <li>Port: <code>9137/TCP</code></li> <li>Path: <code>/metrics</code></li> <li>Auth: None. (Recommendation: Add (Kubernetes) OAuth Proxy in front)</li> <li>What can the Metrics tell us:<ul> <li>Dell HW status through the use of Dell's OMSA <code>omreport</code> tool.</li> </ul> </li> </ul>  <p>The dellhw_exporter uses Dell Open Manage Server Administrator <code>omreport</code> to get metrics from Dell hardware.</p> <p>For more info, visit the GitHub repository: https://github.com/galexrt/dellhw_exporter.</p>  <p>Info</p> <p>The exporter is written by the project contributors, initially created by Alexander Trost.</p>","title":"dellhw_exporter by galexrt"},{"location":"monitoring/prometheus/exporters/ethtool_exporter/","text":"","title":"ethtool_exporter by Showmax"},{"location":"monitoring/prometheus/exporters/ethtool_exporter/#info","text":"<ul> <li>Website / Source Code: https://github.com/Showmax/prometheus-ethtool-exporter</li> <li>Port: <code>9417/TCP</code> (can also export to prom textilfe format)</li> <li>Path: <code>/metrics</code></li> <li>Auth: None. (Recommendation: Add (Kubernetes) OAuth Proxy in front)</li> <li>What can the Metrics tell us:<ul> <li>Interface information like SFP status / information and other interesting network interface details.</li> </ul> </li> </ul>  <p>Exports interface metrics using the <code>ethtool</code> tool.</p> <p>E.g., reports metrics such as temperature, dampening, etc., of SFP interfaces (the SFPs need to have / support Digital Diagnostic Monitoring (DDM)).</p>","title":"Info"},{"location":"monitoring/prometheus/exporters/node_exporter/","text":"<ul> <li>Website / Source Code: https://github.com/prometheus/node_exporter</li> <li>Port: <code>9100/TCP</code></li> <li>Path: <code>/metrics</code></li> <li>Auth: None. (Recommendation: Add (Kubernetes) OAuth Proxy in front)</li> <li>What can the Metrics tell us:<ul> <li>OS metrics (e.g., <code>cpu</code>, <code>loadavg</code>, <code>meminfo</code> and many more).<ul> <li>The exporter can export a ton of metrics for Linux based systems.</li> </ul> </li> </ul> </li> </ul> <p>Can export metrics for the following OSes: Darwin, Dragonfly, FreeBSD, Linux, NetBSD, OpenBSD, Solaris.</p> <p>Not all metrics are available for each OS (e.g., only Linux has <code>mdadm</code> metrics). For a list of metrics per OS, see https://github.com/prometheus/node_exporter#enabled-by-default and https://github.com/prometheus/node_exporter#disabled-by-default.</p> <p>Special point about the node_exporter is that it can export metrics from textiles that are in a certain format, see https://github.com/prometheus/node_exporter#textfile-collector. SMART metrics are normally exported like this (e.g., https://github.com/galexrt/docker-node_exporter-smartmon for a container image that runs the smartctl script every X time).</p>","title":"node_exporter by Prometheus Project"},{"location":"monitoring/prometheus/exporters/others/","text":"<p>Checkout the official Prometheus exporters list for many more exporters.</p>","title":"Other exporters"},{"location":"networking/cloudflare/","text":"","title":"Cloudflare"},{"location":"networking/cloudflare/#create-ipv4-and-ipv6-ipsets","text":"<pre><code># Create ipsets for IPv4 and IPv6\nipset create cf4 hash:net family inet\nipset create cf6 hash:net family inet6\n# Create ipset for both lists, so both IP versions can use the same list name `cf`\nipset create cf list:set cf4 cf6\n# Get the current Cloudflare IP lists\nfor ip in $(curl https://www.cloudflare.com/ips-v4); do\n    ipset add cf4 \"$ip\";\ndone\nfor ip in $(curl https://www.cloudflare.com/ips-v6); do\n    ipset add cf6 \"$ip\";\ndone\n</code></pre>","title":"Create IPv4 and IPv6 IPSets"},{"location":"networking/cloudflare/#allow-80tcp-http-and-443tcp-https-access-to-cloudflare-ips-only","text":"<p>Note</p> <p>These iptables rules are for a stateful firewall!</p>  <pre><code>iptables -A INPUT -m set --match-set cf4 src -p tcp -m multiport --dports http,https -m state --state NEW -j ACCEPT\niptables -A INPUT -p tcp -m multiport --dports http,https  -m state --state NEW -j DROP\nip6tables -A INPUT -m set --match-set cf6 src -p tcp -m multiport --dports http,https -m state --state NEW -j ACCEPT\nip6tables -A INPUT -p tcp -m multiport --dports http,https  -m state --state NEW -j DROP\n</code></pre>","title":"Allow <code>80/tcp</code> (http) and <code>443/tcp</code> (https) Access to Cloudflare IPs only"},{"location":"networking/cisco/acls/","text":"","title":"ACLs"},{"location":"networking/cisco/acls/#acls","text":"<pre><code># faculty auf faculty\naccess-list 100 permit tcp 172.17.10.0 0.0.0.255 172.17.0.0 0.0.255.255\naccess-list 100 permit udp 172.17.10.0 0.0.0.255 172.17.0.0 0.0.255.255\n# Students auf http and https faculty\naccess-list 101 permit tcp 172.17.20.0 0.0.0.255 host 172.17.10.2 eq 80\naccess-list 101 permit tcp 172.17.20.0 0.0.0.255 host 172.17.10.2 eq 443\n# Guests block all access\naccess-list 102 deny tcp 172.17.30.0 0.0.0.255 any\n</code></pre>","title":"ACLs"},{"location":"networking/cisco/acls/#enter-a-sub-interface","text":"<pre><code>ip access-group NUMBER in\n</code></pre>","title":"Enter a \"sub\" interface"},{"location":"networking/cisco/cheat-sheet/","text":"","title":"Cheat Sheet"},{"location":"networking/cisco/cheat-sheet/#allow-unsupported-transceivers-to-be-used","text":"<pre><code>enable\nconfigure terminal\nno errdisable detect cause gbic-invalid\nservice unsupported-transceiver\nexit\n</code></pre>","title":"Allow unsupported Transceivers to be used"},{"location":"networking/cisco/cheat-sheet/#quick-setup-cheap-network","text":"<p>More to come here to get a Cisco switch running with \"cheap\" network equipment.</p> <pre><code>enable\nconfigure terminal\nno errdisable detect cause gbic-invalid\nservice unsupported-transceiver\nexit\n</code></pre>","title":"Quick setup \"cheap\" network"},{"location":"networking/cisco/switch-configuration/","text":"","title":"Switch Configuration"},{"location":"networking/cisco/switch-configuration/#basic-commands","text":"<ul> <li><code>enable</code> - Privileged mode.</li> <li><code>configure terminal</code> - Enter global config mode</li> <li><code>hostname NAME</code> - Set a hostname</li> <li><code>configure terminal</code> - Enable config mode.</li> <li><code>no ip domain lookup</code> - Disable accidental DNS lookup (in priv and non priv mode).</li> <li><code>exit</code> - Go one mode back.</li> </ul>","title":"Basic Commands"},{"location":"networking/cisco/switch-configuration/#config-mode","text":"<p>Enter config mode using <code>configure terminal</code>.</p> <ul> <li><code>line console 0</code> - Enter line <code>console 0</code> \"interface\".</li> <li><code>interface Gi 0/48</code> - Enter Interface Gigabit 0/48 interface.</li> <li><code>ip address IP_ADDR SUBNET_MASK</code> - Set IP_ADDR and the SUBNET_MASK for an interface.</li> </ul>","title":"Config Mode"},{"location":"networking/cisco/switch-configuration/#make-interface-dedicated-for-mgmt","text":"<pre><code>configure terminal\ninterface Gi 0/48\nno interface port\nint vlan1\nshutdown\nexit\nip default-gateway DEFAULT_GATEWAY\n</code></pre>","title":"Make interface dedicated for mgmt"},{"location":"networking/cisco/switch-configuration/#set-ip-address-on-interface","text":"<pre><code>configure terminal\ninterface Gi 0/48\nip address IP_ADDR SUBNET_MASK\n</code></pre>","title":"Set IP address on interface"},{"location":"networking/cisco/switch-configuration/#activate-ssh-rsa-key","text":"<pre><code>crypto key generate rsa general-keys modulus 4096 label sw-azubi-1\n</code></pre>","title":"\"Activate\" SSH RSA Key"},{"location":"networking/cisco/switch-configuration/#show-interface-status","text":"<pre><code>do show interface status\n</code></pre>","title":"Show interface status"},{"location":"networking/cisco/switch-configuration/#add-passwords-to-console-login-thingy","text":"<pre><code>configure terminal\nline console 0\npassword YOUR_PASSWORD\nline vty 0 4\nlogin\npassword YOUR_PASSWORD\nexit\n</code></pre>  <p>In the global config mode:</p> <pre><code>enable secret YOUR_PASSWORD\n</code></pre>","title":"Add passwords to console login thingy"},{"location":"networking/fiber/cheat-sheet/","text":"","title":"Cheat Sheet"},{"location":"networking/fiber/cheat-sheet/#rj45-copper-t-base-sfp-sfp","text":"<p>As long as they support one of the T-Base standards (e.g., 1000T-Base), they should be able to be used like a \"normal RJ45 ethernet network\" port.</p>  <p>T-Base standard</p> <p>For the SFP / SFP+ to work with, e.g, 10 / 100 / 1000, each of the nT-Base standards must be \"implemented\" in them.</p>","title":"RJ45 (Copper) T-Base SFP / SFP+"},{"location":"networking/fiber/cheat-sheet/#simplex-and-multimode-fiber-cables-and-transceivers","text":"<p>You should not mix simplex with multimode fibers and transceivers, and the other way around.</p>","title":"Simplex and Multimode fiber cables and transceivers"},{"location":"networking/fiber/cheat-sheet/#youll-never-have-a-second-go-for-apc-connectors","text":"<p>Due to the polishing (angle 8\u00b0) of APC fiber \"ends\", if you plugin the fiber connector the wrong way, you'll \"break\" them (slight crunch noise). E.g., plugging SC APC connectors in the wrong way.</p> <p>Make sure to check the orientation markers and with that always plug APC connectors in the correct way!</p> <p>UPC connectors are having no polish at all so no need to worry about the way to plug them in.</p>","title":"\"You'll never have a second go for APC connectors\""},{"location":"networking/fiber/glossar/","text":"","title":"Glossar"},{"location":"networking/fiber/glossar/#os1-and-os2","text":"<p>Mode: Singlemode.</p> <p>Bi-Directional, only one line / cable core needed.</p> <p>\"Only one side can sent at once\", used for longer distances.</p> <p>TODO</p>","title":"OS1 and OS2"},{"location":"networking/fiber/glossar/#om1-om2-om3-om4-om5","text":"<p>Mode: Multimode.</p> <p>\"Two lines / cable cores each for 'one way' of data\", used for \"short\" distances.</p> <p>TODO</p>","title":"OM1 / OM2 / OM3 / OM4 / OM5"},{"location":"networking/fiber/glossar/#simplex-vs-duplex-fiber","text":"<ul> <li>Simplex cable are consisting of a single cable strand.</li> <li>Duplex cable are consisting of two cable strands (\"glued\" together).</li> </ul>","title":"Simplex vs Duplex fiber"},{"location":"networking/fiber/glossar/#singlemode-vs-multimode","text":"<ul> <li>Singlemode only allows one light mode to pass through at a time.<ul> <li>Wikipedia</li> <li>Advantages:<ul> <li>Much further distances (e.g., OS2 up to 10 kilometers for 10G).</li> <li>Good for WAN network \"applications\".</li> </ul> </li> <li>Disadvantages:<ul> <li>Higher cost (depending on where you buy, up to 6-times more expensive).</li> </ul> </li> </ul> </li> <li>Multimode have shorter reach though are because of their price better suited, for, e.g., in datacenter usage.<ul> <li>Wikipedia</li> <li>Advantages:<ul> <li>Lower cost (in comparison with Singlemode).</li> <li>Compatible with many different data protocols, e.g., ethernet.</li> </ul> </li> <li>Disadvantages:<ul> <li>Nowhere near the distance of Singlemode (e.g., OM4 10G distance is \"only\" up to 550 meters).</li> </ul> </li> </ul> </li> </ul>","title":"Singlemode vs Multimode"},{"location":"networking/mikrotik/cheat-sheet/","text":"","title":"Cheat Sheet"},{"location":"networking/mikrotik/cheat-sheet/#quick-run-snippets","text":"","title":"Quick Run Snippets"},{"location":"networking/mikrotik/cheat-sheet/#automatic-os-and-firmware-update","text":"<pre><code>/system routerboard settings set auto-upgrade=yes\n/system package update\ncheck-for-updates once\n:delay 3s;\n:if ( [get status] = \"New version is available\") do={ install }\n:delay 1s:\n/system routerboard upgrade\n/system reboot\n</code></pre>","title":"Automatic OS and Firmware Update"},{"location":"networking/mikrotik/cheat-sheet/#config","text":"","title":"Config"},{"location":"networking/mikrotik/cheat-sheet/#set-hostname-identity","text":"<pre><code>/system identity\nset name=HOSTNAME\n</code></pre>","title":"Set Hostname / Identity"},{"location":"networking/mikrotik/cheat-sheet/#set-timezone","text":"<p>To <code>Europe/Berlin</code>.</p> <pre><code>/system clock\nset time-zone-name=Europe/Berlin\n</code></pre>","title":"Set Timezone"},{"location":"networking/mikrotik/cheat-sheet/#disable-ports-beginning-with-ether","text":"<pre><code>:foreach i in=[/interface find name~\"ether\"] do={ /interface ethernet set $i disabled=yes }\n</code></pre>","title":"Disable Ports beginning with <code>ether</code>"},{"location":"networking/mikrotik/cheat-sheet/#advertise-10g-on-sfp-ports","text":"<pre><code>:foreach i in=[/interface find name~\"sfp-sfpplus\"] do={ /interface ethernet set $i advertise=10000M-full; }\n</code></pre>","title":"\"Advertise\" 10G on SFP+ Ports"},{"location":"networking/mikrotik/cheat-sheet/#enable-graphs-graphing","text":"<pre><code>/tool graphing\nset page-refresh=240\n/tool graphing interface\nadd\n/tool graphing resource\n# Put your admin / configuration network here\nadd allow-address=172.16.0.0/24\n</code></pre>","title":"Enable Graphs / Graphing"},{"location":"networking/mikrotik/cheat-sheet/#set-boot-target-of-device","text":"<pre><code>/system routerboard settings\nset boot-os=router-os\n</code></pre>","title":"Set Boot target of Device"},{"location":"networking/mikrotik/example-configs/","text":"","title":"Example Configs"},{"location":"networking/mikrotik/example-configs/#vlans-vlan-ingress-filtering","text":"<pre><code>/interface bridge\n# DO NOT SET `vlan-filtering=yes` here already! Otherwise you would lock yourself out.\nadd dhcp-snooping=yes frame-types=admit-only-vlan-tagged igmp-snooping=yes ingress-filtering=no name=bridge1 pvid=4094 vlan-filtering=no\n/interface vlan\nadd interface=bridge1 name=vlan_10_misc vlan-id=10\nadd interface=bridge1 name=vlan_20_seccam vlan-id=20\nadd interface=bridge1 name=vlan_30_iot vlan-id=30\nadd interface=bridge1 name=vlan_100_guest vlan-id=100\nadd interface=bridge1 name=vlan_4093_admin vlan-id=4093\nadd interface=bridge1 name=vlan_4094_netmgmt vlan-id=4094\n/interface bridge port\nadd bridge=bridge1 frame-types=admit-only-vlan-tagged hw=yes ingress-filtering=yes interface=bonding1 pvid=4094 trusted=yes\nadd bridge=bridge1 frame-types=admit-only-vlan-tagged hw=yes ingress-filtering=yes interface=sfp-sfpplus1 pvid=4094 trusted=yes\nadd bridge=bridge1 frame-types=admit-only-vlan-tagged hw=yes ingress-filtering=yes interface=sfp-sfpplus2 pvid=4094 trusted=yes\nadd bridge=bridge1 frame-types=admit-only-vlan-tagged hw=yes ingress-filtering=yes interface=sfp-sfpplus3 pvid=4094 trusted=yes\nadd bridge=bridge1 frame-types=admit-only-untagged-and-priority-tagged hw=yes ingress-filtering=yes interface=sfp-sfpplus4 pvid=10\nadd bridge=bridge1 frame-types=admit-only-untagged-and-priority-tagged hw=yes ingress-filtering=yes interface=sfp-sfpplus5 pvid=4093\nadd bridge=bridge1 frame-types=admit-only-vlan-tagged hw=yes ingress-filtering=yes interface=sfp-sfpplus6 pvid=4094 trusted=yes\n/interface bridge vlan\nadd bridge=bridge1 tagged=bonding1,sfp-sfpplus1,sfp-sfpplus6 untagged=sfp-sfpplus4 vlan-ids=10\nadd bridge=bridge1 tagged=bonding1,sfp-sfpplus1,sfp-sfpplus6 vlan-ids=20\nadd bridge=bridge1 tagged=bonding1,sfp-sfpplus1,sfp-sfpplus6 vlan-ids=30\nadd bridge=bridge1 tagged=bonding1,sfp-sfpplus1,sfp-sfpplus6 vlan-ids=100\nadd bridge=bridge1 tagged=bonding1,sfp-sfpplus1,sfp-sfpplus2,sfp-sfpplus3,sfp-sfpplus6 untagged=sfp-sfpplus5 vlan-ids=4093\nadd bridge=bridge1 tagged=bonding1,bridge1,sfp-sfpplus1,sfp-sfpplus2,sfp-sfpplus3,sfp-sfpplus6 vlan-ids=4094\n# Get an IP for the network management interface\n/ip dhcp-client\nadd dhcp-options=hostname,clientid disabled=no interface=vlan_4094_netmgmt\n# Wait for everything to \"settle down\"\n:delay 3\n# Enable VLAN Filtering\n/interface bridge\nset bridge1 ingress-filtering=yes vlan-filtering=yes\n</code></pre>","title":"VLANs + VLAN Ingress Filtering"},{"location":"networking/mikrotik/example-configs/#hardware-offloading-hwyes","text":"<p>It depends on the Router OS version and if the Switch Chip in your MikroTik device supports hardware offloading.</p> <p>See MikroTik Wiki - Manual:Switch Chip Features.</p>","title":"Hardware Offloading (<code>hw=yes</code>)"},{"location":"networking/mikrotik/example-configs/#bonding","text":"<p>This bonds interfaces <code>sfp-sfpplus7</code> and <code>sfp-sfpplus8</code> together as <code>bonding1</code> interface:</p> <pre><code>/interface bonding\nadd lacp-rate=1sec name=bonding1 slaves=sfp-sfpplus7,sfp-sfpplus8 transmit-hash-policy=layer-2-and-3\n</code></pre>","title":"Bonding"},{"location":"software/crio/","text":"","title":"CRI-O"},{"location":"software/crio/#default-cni-configs-causes-cluster-network-issues","text":"<p>Hint</p> <p>If you use CRI-O with Kubernetes, you should always remove these default CNI config files!</p>  <p>On most operating systems where CRI-O can be installed through a packge, CRI-O comes with some default CNI configs located at <code>/etc/cni/net.d/</code> (default directory). That means if you run Kubernetes or anything else that \"brings its own CNI\" (e.g., Calico, Cilium, Flannel, etc.) you need to remove those files.</p> <p>Currently it should be those two files that need to be removed:</p> <ul> <li><code>/etc/cni/net.d/100-crio-bridge.conf</code></li> <li><code>/etc/cni/net.d/200-loopback.conf</code></li> </ul>  <p>Tip</p> <p>If CRI-O was already running/started at the time of removing these CNI config files, you need to restart the <code>crio</code> service on the server(s).</p> <p>Using <code>sudo</code> or as the root user run the following to restart the <code>crio</code> service on servers that use systemd as the \"service manager\":</p> <pre><code>systemctl restart crio\n</code></pre>","title":"Default CNI configs causes Cluster Network Issues"},{"location":"software/docker-registry/","text":"","title":"Docker Registry"},{"location":"software/docker-registry/#garbage-collection-doesnt-work-with-non-aws-s3-stores","text":"<p>(Non AWS S3 stores, e.g., Ceph RGW, Minio, Linode Object store and other similar stores)</p> <p>This is due to GitHub docker/distribution - Issue <code>failed to garbage collect</code> #3200.</p>  <p>Note</p> <p>This workaround / \"fix\" is based on @thomasf (Thomas Fr\u00f6ssman)'s comment in the issue.</p>  <p>There is an issue in the <code>s3aws.Walk()</code> function which fails for (most) non AWS S3 storages.</p> <p>To make the garbage collection work, an empty file needs to be created in the bucket at the following path (default S3 bucket settings used in the docker-registry itself) <code>BUCKET_NAME/docker/registry/v2/repositories/</code>.</p>  <p>Info</p> <p>Replace the placeholders (<code>MC_HOST_CONFIG_NAME</code>, <code>BUCKET_NAME</code>) according to your docker-registry S3 storage configuration.</p>  <p>Using the Minio client <code>mc</code> (compatible with most / all S3 based storages) the following command should \"fix\" the issue:</p> <pre><code># Create empty file\ntouch workaround_s3aws_walk_issue_github_docker_distribtuion_issue_3200\n# Upload the empty file\nmc cp workaround_s3aws_walk_issue_github_docker_distribtuion_issue_3200 MC_HOST_CONFIG_NAME/BUCKET_NAME/docker/registry/v2/repositories/\n# Verify that the file has been uploaded\nmc ls MC_HOST_CONFIG_NAME/BUCKET_NAME/docker/registry/v2/repositories/\n</code></pre>","title":"Garbage Collection doesn't work with non AWS S3 stores"},{"location":"software/docker-registry/#configuration-through-environment-variables-fails","text":"<p>Sometimes when \"deeply nesting\" environment variables to configure a certain aspect of the docker-registry configuration, e.g., <code>REGISTRY_STORAGE_MAINTENANCE_READONLY_ENABLED</code>, need to be specified as YAML or JSON starting from \"a few levels\" further below of the config structure:</p> <pre><code>REGISTRY_STORAGE_MAINTENANCE: |-\n  uploadpurging:\n    enabled: false\n</code></pre>  <p>This has been posted for a similar config situation by @0rax in GitHub docker/distribution - Registry - Upload purging environment overrides crash registry at startup Issue #1736.</p>","title":"Configuration through Environment Variables Fails"},{"location":"software/gitlab-ci/","text":"","title":"GitLab CI"},{"location":"software/gitlab-ci/#cicd-variables-with-dollar-signs","text":"<p>Bug</p> <p>Dollar Signs (<code>$</code>) are resolved as a variable in GitLab CI.</p>  <p>If you have a variable value (e.g., Harbor robot account username) with a <code>$</code> in it, you need to \"escape\" it like this:</p> <ul> <li>From: <pre><code>secret$example123\n</code></pre> </li> <li>To: <pre><code>secret$$example123\n</code></pre> </li> </ul>","title":"CI/CD Variables with Dollar Signs (<code>$</code>)"},{"location":"software/harbor-registry/","text":"","title":"Harbor Registry"},{"location":"software/harbor-registry/#garbage-collection-gc-not-working-with-non-aws-s3-storage","text":"<p>(Non AWS S3 stores, e.g., Ceph RGW, Minio, Linode Object store and other similar stores)</p> <p>See Docker Registry - Garbage Collection doesn't work with non AWS S3 stores.</p>","title":"Garbage Collection (GC) not working with non-AWS S3 storage?"},{"location":"software/harbor-registry/#notes","text":"<ul> <li>[Security]: Images are not scanned on push by default. This option must be enabled per project / group as of today, 28.08.2020.</li> <li>[Kubernetes]: The Job Service <code>Deployment</code>'s <code>PersistentVolumeClaim</code> must be of type <code>ReadWriteMany</code>. Otherwise having more than <code>replicas: 1</code> will not work!</li> </ul>","title":"Notes"},{"location":"storage/NFS/common-issues/","text":"","title":"Common Issues"},{"location":"storage/NFS/common-issues/#php-applications-hanging-timing-out-eg-nextcloud","text":"<p>In case of Nextcloud on NFS, the application was stuck / hanging at a <code>flock</code> syscall.</p> <p>To \"workaround\" the issue, if applicable for the application (should be for most PHP applications), the NFS must be mounted with the <code>nolock</code> mount option added.</p>","title":"PHP Applications hanging / timing out (e.g., Nextcloud)"},{"location":"storage/ceph/","text":"<p>Ceph offers block storage (RBD), network filesystem (CephFS), object storage (RGW, S3, SWIFT) and key value storage (librados).</p>","title":"Ceph"},{"location":"storage/ceph/#glossary","text":"<ul> <li>Daemons<ul> <li>MON: Monitor Daemon</li> <li>MGR: Manager Daemon</li> <li>OSD: Object Storage Daemon</li> <li>RGW: Rados Gateway</li> </ul> </li> <li>\"Things\"<ul> <li>Pool: Group of PGs, separation .</li> <li>PG: Placement Group, group of objects in a pool.</li> <li>Object: A single object \"somewhere in the cluster\". Part of a PG.</li> </ul> </li> </ul>","title":"Glossary"},{"location":"storage/ceph/#pool-pg-object","text":"<pre><code>graph LR\nsubgraph Pool\n    subgraph PG-1[PG 1]\n        Object-1[Object 1]\n        Object-2[Object 2]\n    end\n    subgraph PG-2[PG 2]\n        Object-3[Object 3]\n        Object-4[Object 4]\n    end\nend</code></pre>","title":"Pool -&gt; PG -&gt; Object"},{"location":"storage/ceph/#ceph-cluster-components","text":"","title":"Ceph Cluster Components"},{"location":"storage/ceph/#mon-monitor-daemon","text":"<p>The MONs are keeping a map of the current MONs, OSDs, PGs, Pools and so on.</p> <p>MONs require a quorum to function, meaning that you should always run at least 3 mons for production 5 can be a good idea as well.</p> <pre><code>graph LR\n    MON-A\n    MON-A -.- MON-B\n    MON-A -.- MON-C\n\n    MON-B\n    MON-B -.- MON-A\n    MON-B -.- MON-C\n\n    MON-C\n    MON-C -.- MON-B\n    MON-C -.- MON-C</code></pre> <p>Man Page: https://docs.ceph.com/en/latest/man/8/ceph-mon/</p>","title":"MON - Monitor Daemon"},{"location":"storage/ceph/#quorum","text":"<p>Simply put, if you run with 3 MONs you can lose 1 MON before the cluster will come to a halt as the quorum would be lost.</p>    MONs How many can be lost?     1 0   2 0   3 1   4 1   5 2   6 2   7 3    <p>For more information regarding quorum, checkout the following link from the ETCD documentation: ETCD v3.3. - FAQ Why an odd number of cluster members?. ETCD is a good example of an infrastructure critical application (for Kubernetes) which requires a Quorum for (most; depending on your settings) operations.</p>","title":"Quorum"},{"location":"storage/ceph/#mgr-manager-daemon","text":"<p>The MGR daemon(s) \"provide additional monitoring and interfaces to external monitoring and management systems\". You need to have at least one running as otherwise certain status information in, e.g., <code>ceph status</code> (<code>ceph -s</code>), will not be shown. It is recommended to run at least 1.</p> <p>The MGR daemon(s) talk with the MON, OSD MDS, and even RGW.</p> <pre><code>graph LR\n    MGR --&gt; MON\n    MGR --&gt; OSD\n    MGR --&gt; MDS\n    MGR --&gt; RGW</code></pre> <p>The MGR daemon(s) have many modules to for example provide metrics for Prometheus, Zabbix and others. In addition to that a Ceph dashboard can be activated which contains some basic information and even an integration with Prometheus and Grafana.</p> <p>Man Page: https://docs.ceph.com/en/latest/mgr/index.html</p>","title":"MGR - Manager Daemon"},{"location":"storage/ceph/#osd-object-storage-daemon","text":"<p>All OSDs normally talk with each other for hearbeat  checking and data replication (actual data and also for data \"scrubbing\" operations).</p> <pre><code>graph LR\n    OSD-0\n    OSD-0 -.- OSD-1\n    OSD-0 -.- OSD-2\n    OSD-0 -.- OSD-n\n\n    OSD-1\n    OSD-1 -.- OSD-0\n    OSD-1 -.- OSD-2\n    OSD-1 -.- OSD-n\n\n    OSD-2\n    OSD-2 -.- OSD-0\n    OSD-2 -.- OSD-1\n    OSD-2 -.- OSD-n\n\n    OSD-n\n    OSD-n -.- OSD-0\n    OSD-n -.- OSD-1\n    OSD-n -.- OSD-2</code></pre> <ul> <li>If a client writes data, the data is replicated by the OSD and not the client.</li> <li>If a client reads data, the data can be read from multiple OSDs at the same time (as long as they have a replica of the data).</li> <li>Side note: That is the reason why read speeds are so fast and writes can be so slow. One slow OSD can \"ruin your day\" because of that.</li> </ul> <p>Man Page: https://docs.ceph.com/en/latest/man/8/ceph-osd/</p>","title":"OSD - Object Storage Daemon"},{"location":"storage/ceph/#mds-metadata-server-daemon","text":"<p>The MDS is the metadata server for the CephFilesystem (CephFS).</p> <p>It talks with the OSDs and coordinates the filesystem access. Clients still need to access the OSDs, but the MDS is the \"gateway\" to know where to go so to say.</p> <pre><code>graph LR\n    Client-A --&gt; MDS-1\n    Client-B --&gt; MDS-1\n    Client-C ---&gt; MDS-2\n\n    MDS-1 -.- MDS-2\n    MDS-1 --&gt; OSDs\n    MDS-2 --&gt; OSDs\n\n    Client-A ---&gt; OSDs\n    Client-B ---&gt; OSDs\n    Client-C ---&gt; OSDs</code></pre> <p>Man Page: https://docs.ceph.com/en/latest/man/8/ceph-mds/</p>","title":"MDS - Metadata Server Daemon"},{"location":"storage/ceph/#rgw-rados-rest-gateway","text":"<p>RGW can offer S3 and / or SWIFT compatible storage, allowing to use it as a \"replacement\" for AWS S3 object storage in some cases. An advantage to, e.g., the block storage (RBD) and CephFS is that the client itself does not need direct access to the MONs, OSDs, etc., though it depends on the use case and the performance required per client / application.</p> <pre><code>graph LR\n    Client-A --&gt; RGW-1\n    Client-B --&gt; RGW-1\n    Client-C --&gt; RGW-2\n    RGW-1 --&gt; OSDs\n    RGW-2 --&gt; OSDs</code></pre> <p>Common scenario is that you have a load balancer in front of your RGWs, in itself the structure stays the same:</p> <pre><code>graph LR\n    Client-A --&gt; Loadbalancer\n    Client-B --&gt; Loadbalancer\n    Client-C --&gt; Loadbalancer\n    Loadbalancer --&gt; RGW-1\n    Loadbalancer --&gt; RGW-2\n    RGW-1 --&gt; OSDs\n    RGW-2 --&gt; OSDs</code></pre> <p>Man Page: https://docs.ceph.com/en/latest/man/8/radosgw/</p>  <p>This wonderful Ceph cheat sheet from @TheJJ on GitHub has much more insight into the processes, tricks and setup of a Ceph cluster and the moving parts.</p>","title":"RGW - RADOS REST Gateway"},{"location":"storage/ceph/#old-diagrams","text":"<p>The source for the diagrams, can be found as <code>.graphml</code> at the same path as the images.</p>","title":"Old Diagrams"},{"location":"storage/ceph/#basic-cluster-with-hdds-and-ssds","text":"<p></p>","title":"Basic Cluster with HDDs and SSDs"},{"location":"storage/ceph/#cluster-with-rgw-for-s3-compatible-object-storage","text":"<p>No direct OSD access network is required by the consumers of the object storage. This can be seen as a \"advantage\" over RBD and CephFS, though it completely depends on your use case.</p> <p></p>","title":"Cluster with RGW for S3-compatible Object Storage"},{"location":"storage/ceph/#cluster-with-nvme-osds-multi-datacenter-scenario","text":"<p></p>","title":"Cluster with NVMe OSDs (+ Multi Datacenter Scenario)"},{"location":"storage/ceph/common-issues/","text":"","title":"Common Issues"},{"location":"storage/ceph/common-issues/#benchmarking-ceph-storage","text":"<p>You want to benchmark the storage of your Ceph cluster(s)? This is a short list of tools to benchmark storage.</p> <p>Recommended tools:</p> <ul> <li>General Benchmarking Testing of Storage (e.g., plain disks, and other storage software)<ul> <li><code>fio</code><ul> <li>References<ul> <li>https://github.com/axboe/fio/tree/master/examples</li> <li>https://docs.oracle.com/en-us/iaas/Content/Block/References/samplefiocommandslinux.htm</li> </ul> </li> </ul> </li> </ul> </li> <li>Ceph specific Benchmarking:<ul> <li><code>rbd bench</code> command<ul> <li>References:<ul> <li>https://tracker.ceph.com/projects/ceph/wiki/Benchmark_Ceph_Cluster_Performance</li> <li>https://edenmal.moe/post/2017/Ceph-rbd-bench-Commands/</li> </ul> </li> </ul> </li> </ul> </li> </ul>","title":"Benchmarking Ceph Storage"},{"location":"storage/ceph/common-issues/#cephfs-mount-issues-on-hosts","text":"<p>Make sure you have a (active) Linux kernel of version <code>4.17</code> or higher.</p>  <p>Tip</p> <p>In general it is recommended to have a very up-to-date version of the Linux kernel, as many improvements have been made to the Ceph kernel drivers in newer kernel versions (<code>5.x</code> or higher).</p>","title":"CephFS mount issues on Hosts"},{"location":"storage/ceph/common-issues/#health_warn-1-large-omap-objects","text":"","title":"<code>HEALTH_WARN 1 large omap objects</code>"},{"location":"storage/ceph/common-issues/#issue","text":"<pre><code>HEALTH_WARN 1 large omap objects\n# and/or\nLARGE_OMAP_OBJECTS 1 large omap objects\n</code></pre>","title":"Issue"},{"location":"storage/ceph/common-issues/#solution","text":"<p>The following command should fix the issue:</p> <pre><code>radosgw-admin reshard stale-instances rm\n</code></pre>","title":"Solution"},{"location":"storage/ceph/common-issues/#mdss-report-oversized-cache","text":"","title":"<code>MDSs report oversized cache</code>"},{"location":"storage/ceph/common-issues/#issue_1","text":"<p>Ceph health status reports, e.g., <code>1 MDSs report oversized cache</code>.</p> <pre><code>[root@rook-ceph-tools-86d54cbd8d-6ktjh /]# ceph -s\n  cluster:\n    id:     67e1ce27-0405-441e-ad73-724c93b7aac4\n    health: HEALTH_WARN\n            1 MDSs report oversized cache\n[...]\n</code></pre>","title":"Issue"},{"location":"storage/ceph/common-issues/#solution_1","text":"<p>You can try to increase the <code>mds cache memory limit</code> setting1.</p>  <p>Tip</p> <p>For Rook Ceph users, you set/increase the memory requests on the CephFilesystem object for the MDS daemons2.</p>","title":"Solution"},{"location":"storage/ceph/common-issues/#find-device-osd-is-using","text":"","title":"Find Device OSD is using"},{"location":"storage/ceph/common-issues/#issue_2","text":"<p>You need to find out which disk/device is used by an OSD daemon.</p> <p>Scenarios: <code>smartctl</code> is showing that the disk should be replaced, disk has already failed, etc.</p>","title":"Issue"},{"location":"storage/ceph/common-issues/#solution_2","text":"<p>Use the various <code>ls*</code> subcommands of <code>ceph device</code>.</p> <pre><code>$ ceph device --help\ndevice check-health                                                         Check life expectancy of devices\ndevice get-health-metrics &lt;devid&gt; [&lt;sample&gt;]                                Show stored device metrics for the device\ndevice info &lt;devid&gt;                                                         Show information about a device\ndevice light on|off &lt;devid&gt; [ident|fault] [--force]                         Enable or disable the device light. Default type is `ident`\n'Usage: device\n                                                                             light (on|off) &lt;devid&gt; [ident|fault] [--force]'\ndevice ls                                                                   Show devices\ndevice ls-by-daemon &lt;who&gt;                                                   Show devices associated with a daemon\ndevice ls-by-host &lt;host&gt;                                                    Show devices on a host\ndevice ls-lights                                                            List currently active device indicator lights\ndevice monitoring off                                                       Disable device health monitoring\ndevice monitoring on                                                        Enable device health monitoring\ndevice predict-life-expectancy &lt;devid&gt;                                      Predict life expectancy with local predictor\ndevice query-daemon-health-metrics &lt;who&gt;                                    Get device health metrics for a given daemon\ndevice rm-life-expectancy &lt;devid&gt;                                           Clear predicted device life expectancy\ndevice scrape-daemon-health-metrics &lt;who&gt;                                   Scrape and store device health metrics for a given daemon\ndevice scrape-health-metrics [&lt;devid&gt;]                                      Scrape and store device health metrics\ndevice set-life-expectancy &lt;devid&gt; &lt;from&gt; [&lt;to&gt;]                            Set predicted device life expectancy\n</code></pre>  <p>The <code>ceph device</code> subcommands allow you to do even more things, e.g., turn on the disk light in server chassis. Enabling the light for the disk can help the datacenter workers to easily locate the disk and not replacing the wrong disk.</p>","title":"Solution"},{"location":"storage/ceph/common-issues/#locate-disk-of-osd-by-osd-daemon-id-eg-osd-13","text":"<pre><code>$ ceph device ls-by-daemon osd.13\nDEVICE                                     HOST:DEV                                           EXPECTED FAILURE\nSAMSUNG_MZVL2512HCJQ-00B00_S1234567890123  HOSTNAME:nvme1n1\n</code></pre>","title":"Locate Disk of OSD by OSD daemon ID (e.g., OSD 13):"},{"location":"storage/ceph/common-issues/#show-all-disks-by-host-hostname","text":"<pre><code>$ ceph device ls-by-host HOSTNAME\nDEVICE                                     HOST:DEV                                           EXPECTED FAILURE\nDEVICE                                     DEV      DAEMONS  EXPECTED FAILURE\nSAMSUNG_MZVL2512HCJQ-00B00_S1234567890123  nvme1n1  osd.5\nSAMSUNG_MZVL2512HCJQ-00B00_S1234567890123  nvme0n1  osd.2\nSAMSUNG_MZVL2512HCJQ-00B00_S1234567890123  nvme2n1  osd.8\nSAMSUNG_MZVL2512HCJQ-00B00_S1234567890123  nvme3n1  osd.13\n</code></pre>","title":"Show all disks by host (hostname):"},{"location":"storage/ceph/common-issues/#cephosdslowops-alerts","text":"","title":"CephOSDSlowOps Alerts"},{"location":"storage/ceph/common-issues/#issue_3","text":"<p>TODO</p>","title":"Issue"},{"location":"storage/ceph/common-issues/#things-to-try","text":"<ul> <li>Ensure the disks you are using are healthy<ul> <li>Check the SMART values. A bad disks can lock up an application (such as a Ceph OSD) or worse the whole server.</li> </ul> </li> <li></li> </ul>  <p>Should this page not have yielded you a solution, checkout the Rook Ceph Common Issues doc as well.</p>   <ol> <li> <p>Report / Source for information regarding this issue has been taken from http://lists.ceph.com/pipermail/ceph-users-ceph.com/2019-December/037633.html \u21a9</p> </li> <li> <p>Rook Ceph Docs v1.7 - Ceph Filesystem CRD - MDS Resources Configuration Settings \u21a9</p> </li> </ol>","title":"Things to Try"},{"location":"storage/ceph/osds/","text":"","title":"OSDs"},{"location":"storage/ceph/osds/#osd-maintenance","text":"","title":"OSD Maintenance"},{"location":"storage/ceph/osds/#gracefully-remove-osd","text":"<p>Tip</p> <p>If you are using Rook Ceph Operator to run a Ceph Cluster in Kubernetes, please follow the official documentation here: Rook Docs - Ceph OSD Management.</p>  <p>First thing is to set the crush weight to zero, either instantly to <code>0.0</code> or a bit gracefully. (gracefully should always be used when the cluster is in use, though any OSD weight change will cause data redistribution)</p> <pre><code>ceph osd crush reweight osd.&lt;ID&gt; 0.0\n</code></pre>  <p>or graceful:</p> <pre><code>for i in {9 1}; do\n    ceph osd crush reweight osd.&lt;ID&gt; 0.$i\n    # Wait five minutes each step or longer depending on your Ceph cluster recovery speed\n    sleep 300\ndone\n</code></pre>  <p>After the reweight, set the OSD out and remove it (+ its credentials):</p> <pre><code>ceph osd out &lt;ID&gt;\n</code></pre>  <pre><code>ceph osd crush remove osd.&lt;ID&gt;\nceph auth del osd.&lt;ID&gt;\nceph osd rm &lt;ID&gt;\n</code></pre>","title":"Gracefully remove OSD"},{"location":"storage/ceph/rbd/","text":"","title":"RBD (Block Storage)"},{"location":"storage/ceph/rbd/#rbd-performance-stats","text":"<p>Source: Ceph Block Performance Monitoring: Putting noisy neighbors in their place with RBD top and QoS - Red Hat Blog</p> <pre><code>$ ceph ceph mgr module enable rbd_support\n</code></pre>  <pre><code>$ rbd perf image iotop\n$ rbd perf image iostat\n</code></pre>","title":"RBD Performance Stats"},{"location":"storage/gluster/common-issues/","text":"","title":"Common Issues"},{"location":"storage/gluster/common-issues/#no-file-locking","text":"<p>Note</p> <p>The described case here is a define \"no-go\" to run on a GlusterFS, but it is worth to mention as some other applications (possibly git and others) might have issues in the long run as well.</p>  <p>GlusterFS doesn't seem to have file locking, meaning that, e.g., a SQLite database will corrupt if multiple hosts try to access it.</p>","title":"No File Locking"},{"location":"storage/rook/","text":"<p>The source for the diagrams, can be found as <code>.graphml</code> at the same path as the images.</p>","title":"Rook"},{"location":"storage/rook/#rook-ceph-components","text":"<p>Where the \"basic\" components are the rook-ceph-agent and <code>rook-discover</code> DaemonSet.</p> <p></p>  <p>Note</p> <ul> <li>Rook Ceph Discovery DaemonSet is only started after at least one CephCluster has been created!</li> <li>The Rook Ceph Agent was used in earlier Rook Ceph versions for the \"Flex driver\" before the swaitch to use, e.g., Ceph's CSI driver.</li> </ul>","title":"Rook Ceph Components"},{"location":"storage/rook/architecture/","text":"<p>The source for the diagrams, can be found as <code>.graphml</code> at the same path as the images.</p>","title":"Architecture"},{"location":"storage/rook/architecture/#rook-ceph-components","text":"<p>Where the \"basic\" components are the rook-ceph-agent and <code>rook-discover</code> DaemonSet.</p> <p></p>  <p>Note</p> <ul> <li>Rook Ceph Discovery DaemonSet is only started after at least one CephCluster has been created!</li> <li>Rook Ceph Agent (\"Flex driver\") was used in earlier Rook Ceph versions for mounting storage, before it a switch to use Ceph CSI driver was made.</li> </ul>","title":"Rook Ceph Components"},{"location":"storage/rook/cheat-sheet/","text":"","title":"Cheat Sheet"},{"location":"storage/rook/cheat-sheet/#get-persistentvolume-and-persistentvolumeclaim-from-csi-vol-name-in-kubernetes","text":"<p>Replace <code>csi-vol-............</code> with the volume name:</p> <pre><code>$ kubectl get pv -o jsonpath='{range .items[?(@.spec.csi.volumeAttributes.imageName==\"csi-vol-............\")]}{.metadata.name}{\"\\t\"}{.spec.claimRef.namespace}{\"/\"}{.spec.claimRef.name}{\"\\n\"}{end}'\n</code></pre>","title":"Get PersistentVolume and PersistentVolumeClaim from <code>csi-vol-...</code> Name in Kubernetes"},{"location":"storage/rook/common-issues/","text":"<p>Be sure to checkout the Rook Ceph Common Issues page and that all prerequisites for the storage backend of your choice are met!</p> <ul> <li>General Rook prerequisites</li> <li>Ceph prerequisites</li> </ul>","title":"Common Issues"},{"location":"storage/rook/common-issues/#where-did-the-rook-discover-pods-go-after-a-recent-rook-ceph-update","text":"<p>A recent change in Rook Ceph has disabled the <code>rook-discover</code> DaemonSet by default. This behavior is controlled by the <code>ROOK_ENABLE_DISCOVERY_DAEMON</code> located in the <code>operator.yaml</code> or for Helm users <code>enableDiscoveryDaemon: (false|true</code> in your values file. It is a boolean, so <code>false</code> or <code>true</code>.</p>","title":"Where did the <code>rook-discover-*</code> Pods go after a recent Rook Ceph update?"},{"location":"storage/rook/common-issues/#when-do-you-want-to-have-rook-discover-pods-rook_enable_discovery_daemon-true","text":"<ul> <li>You are on (plain) bare metal and / or simply have \"some disks\" installed /attached to your server(s), that you want to use for the Rook Ceph cluster.</li> <li>If your cloud environment / provider does not provide PVCs with <code>volumeMode: Block</code>. Ceph requires block devices (Ceph's <code>filestore</code> is not available, through Rook, since a bunch of versions as <code>bluestore</code> is superior in certain ways).</li> </ul>","title":"When do you want to have <code>rook-discover-*</code> Pods / <code>ROOK_ENABLE_DISCOVERY_DAEMON: true</code>?"},{"location":"storage/rook/common-issues/#crash-collector-pods-are-pending-containercreating","text":"<ul> <li>Check the events of the Crash Collector Pod(s) using <code>kubectl describe pod POD_NAME</code>.</li> <li>If the Pod(s) is waiting for a Secret from the Ceph MONs (keyring for each crash collector), you need to wait a bit longer as the Ceph Cluster is probably still being bootsraped / started up.</li> <li>If they are stuck for more than 15-30 minutes, check the Rook Ceph Operator logs if it is stuck in the Ceph Cluster bootstrap / start up procedure.</li> </ul>","title":"Crash Collector Pods are <code>Pending</code> / <code>ContainerCreating</code>"},{"location":"storage/rook/common-issues/#no-rook-ceph-mon-pods-are-running","text":"<ol> <li>First of all make sure your Kubernetes CNI is working fine! In what feels like 90% of the cases it is network related, e.g., some weird thing with the Kubernetes cluster CNI or other network environment issue.<ul> <li>Can you talk to Cluster Service IPs from every node?</li> <li>Can you talk to Pod IPs from every node? Even to Pods not on the same node you are testing from?</li> <li>Check the docs of your CNI, most have a troubleshooting section, e.g., Cilium had some issues from systemd version 245 onwards with <code>rp_filter</code>, see here: rp_filter (default) strict mode breaks certain load balancing cases in kube-proxy-free mode \u00b7 Issue #13130 \u00b7 cilium/cilium</li> </ul> </li> <li>Does your environment fit all the prerequisites? Check top of page for the links to some of the prerequisites and / or consult the Rook.io docs.</li> <li>Check the <code>rook-ceph-operator</code> Logs for any warnings, errors, etc.</li> </ol>","title":"No <code>rook-ceph-mon-*</code> Pods are running"},{"location":"storage/rook/common-issues/#disks-partitions-not-used-for-ceph","text":"<ul> <li>Does section When do you want to have <code>rook-discover-*</code> Pods / <code>ROOK_ENABLE_DISCOVERY_DAEMON: true</code>? apply to you? If so, make sure the operator has the discovery daemon enabled in its (Pod) config!</li> <li>Is the disk empty? No leftover partitions on it? Make sure it is either \"empty\", e.g., nulled by <code>shred</code>, <code>dd</code> or similar,<ul> <li>To make sure the disk is blank as the Rook docs and I recommend the following commands followed by a reboot of the server: <pre><code>DISK=\"/dev/sdXYZ\"\nsgdisk --zap-all \"$DISK\"\ndd if=/dev/zero of=\"$DISK\" bs=1M count=100 oflag=direct,dsync\nblkdiscard \"$DISK\"\n</code></pre>      Source: https://rook.io/docs/rook/v1.8/ceph-teardown.html#delete-the-data-on-hosts</li> </ul> </li> <li>Was the disk previously used as a Ceph OSD?<ul> <li>Make sure to follow the teardown steps, but make sure to only remove the LVM stuff from that one disk and not from all, see https://rook.io/docs/rook/v1.8/ceph-teardown.html#delete-the-data-on-hosts.</li> </ul> </li> </ul>","title":"Disk(s) / Partition(s) not used for Ceph"},{"location":"storage/rook/common-issues/#a-pod-cant-mount-its-persistentvolume-after-an-unclean-undrained-node-shutdown","text":"<ol> <li>Check the events of the Pod using <code>kubectl describe pod POD_NAME</code>.</li> <li>Check the Node's <code>dmesg</code> logs.</li> <li>Check the kubelet logs for errors related to CSI connectivity and / or make sure the node can reach every other Kubernetes cluster node (at least the Rook Ceph cluster nodes (Ceph Mons, OSDs, MGRs, etc.)).</li> <li>Checkout the CSI Common Issues - Rook Docs.</li> </ol>","title":"A Pod can't mount its PersistentVolume after an \"unclean\" / \"undrained\" Node shutdown"},{"location":"storage/rook/common-issues/#ceph-csi-provisioning-mounting-deletion-or-something-doesnt-work","text":"<p>Make sure you have checked out the CSI Common Issues - Rook Docs.</p> <p>If you have some weird kernel and / or kubelet configuration, make sure Ceph CSI's config options in the Rook Ceph Operator config is correctly setup (e.g., <code>LIB_MODULES_DIR_PATH</code>, <code>ROOK_CSI_KUBELET_DIR_PATH</code>, <code>AGENT_MOUNTS</code>).</p>","title":"Ceph CSI: Provisioning, Mounting, Deletion or something doesn't work"},{"location":"storage/rook/common-issues/#cant-run-any-ceph-commands-in-the-toolbox-ceph-commands-timeout","text":"<ul> <li>Are your <code>rook-ceph-mon-*</code> Pods all in <code>Running</code> state?</li> <li>Does a basic <code>ceph -s</code> work?</li> <li>Is your <code>rook-ceph-mgr-*</code> Pod(s) running as well?</li> <li>Check the <code>rook-ceph-mon-*</code> and <code>rook-ceph-mgr-*</code> logs for errors</li> <li>Try deleteing the toolbox Pod, \"maybe it is just a fluke in your Kubernetes cluster network / CNI.<ul> <li>Also make sure you are using the latest Rook Ceph Toolbox YAML for the Rook Ceph version you are running on, see Rook Ceph Toolbox Pod not Creating / Stuck section.</li> </ul> </li> <li>In case all these seem to indicate a loss of quorum, e.g., the <code>rook-ceph-mon-*</code> talk about <code>probing</code> for other mons only, you might need to follow the disaster recovery guide for your Rook Ceph version here: Rook v1.8 Docs - Ceph Disaster Recovery - Restoring Mon Quorum.</li> </ul>","title":"Can't run any Ceph Commands in the Toolbox / Ceph Commands timeout"},{"location":"storage/rook/common-issues/#a-mon-pod-is-running-on-a-node-which-is-down","text":"<ul> <li>DO NOT EDIT THE MON DEPLOYMENT! A MON Deployment can't just be moved to another node without being failovered by the operator and / or if the MON is running using a PVC for its data.</li> <li>As long as the operator is running the operator should see the mon being down and fail it over after a configurable timeout.<ul> <li>Env var <code>ROOK_MON_OUT_TIMEOUT</code>, by default <code>600s</code> (10 minutes)</li> </ul> </li> </ul>","title":"A MON Pod is running on a Node which is down"},{"location":"storage/rook/common-issues/#remove-replace-a-failed-disk","text":"<p>Checkout the official Ceph OSD Management guide from Rook here: Rook v1.8 Docs - Ceph OSD Management.</p>","title":"Remove / Replace a failed disk"},{"location":"storage/rook/common-issues/#rook-ceph-toolbox-pod-not-creating-stuck","text":"<ul> <li>Make sure that you are not using an old version of the Rook Ceph Toolbox, grab the latest manifest here (make sure to switch to the <code>release-</code> branch of your Rook release): <code>https://github.com/rook/rook/blob/master/cluster/examples/kubernetes/ceph/toolbox.yaml</code></li> <li>The Rook Ceph Toolbox can only fully startup after a Ceph Cluster has at least passed the initial setup by the Rook Ceph operator.<ul> <li>Monitor the Rook Ceph Operator logs for errors.</li> </ul> </li> <li>Check the events of the Toolbox Pod using <code>kubectl describe pod POD_NAME</code>.</li> </ul>","title":"Rook Ceph Toolbox Pod not Creating / Stuck"},{"location":"storage/rook/common-issues/#ceph-osd-tree-wrong-device-class","text":"<ol> <li>Check device class, second column in <code>ceph osd tree</code> output.</li> <li>If you need to change the device class, you first must remove the current one (if it has one set): <code>ceph osd crush rm-device-class osd.ID</code>.</li> <li>Now you can set the device class for the OSD: <code>ceph osd crush set-device-class CLASS osd.ID</code></li> <li>Default device classes (at the time of writing): <code>hdd</code>, <code>ssd</code>, <code>nvme</code></li> <li>Source: Ceph Docs Latest - CRUSH Maps - Device Classes</li> </ol>","title":"Ceph OSD Tree: Wrong Device Class"},{"location":"storage/rook/common-issues/#health_warn-clients-are-using-insecure-global_id-reclaim-health_warn-mons-are-allowing-insecure-global_id-reclaim","text":"<p>Source: https://github.com/rook/rook/issues/7746</p>  <p>I can confirm this is happening in all clusters, whether a clean install or upgraded cluster, running at least versions: <code>v14.2.20</code>, <code>v15.2.11</code> or <code>v16.2.1</code>.</p> <p>According to the CVE also previously mentioned, there is a security issue where clients need to be upgraded to the releases mentioned. Once all the clients are updated (e.g. the rook daemons and csi driver), a new setting needs to be applied to the cluster that will disable allowing the insecure mode.</p> <p>If you see both these health warnings, then either one of the rook or csi daemons has not been upgraded yet, or some other client is detected on the older version:</p> <pre><code>health: HEALTH_WARN\n        client is using insecure global_id reclaim\n        mon is allowing insecure global_id reclaim\n</code></pre>  <p>If you only see this one warning, then the insecure mode should be disabled:</p> <pre><code>health: HEALTH_WARN\n        mon is allowing insecure global_id reclaim\n</code></pre>  <p>To disable the insecure mode from the toolbox after all the clients are upgraded: Make sure all clients have been upgraded, or else those clients will be blocked after this is set:</p> <pre><code>ceph config set mon auth_allow_insecure_global_id_reclaim false\n</code></pre>  <p>Rook could set this flag automatically after the clients have all been updated.</p>","title":"<code>HEALTH_WARN: clients are using insecure global_id reclaim</code> / <code>HEALTH_WARN: mons are allowing insecure global_id reclaim</code>"},{"location":"storage/rook/common-issues/#check-which-object-store-is-used-by-an-osd","text":"<pre><code>$ ceph osd metadata 0 | grep osd_objectstore\n\"osd_objectstore\": \"bluestore\",\n</code></pre>  <p>To get a quick overview of the \"object stores\" (<code>bluestore</code>, (don't use it) <code>filestore</code>): <pre><code>$ ceph osd count-metadata osd_objectstore\n{\n    \"bluestore\": 6\n}\n</code></pre> </p>","title":"Check which \"Object Store\" is used by an OSD"},{"location":"storage/rook/common-issues/#persistentvolumeclaims-persistentvolumes-are-not-resized","text":"<ul> <li>Make sure the Ceph CSI driver for the storage (block or filesystem) is running (check the logs if you are unsure as well).</li> <li>Check if you use a StorageClass that has <code>allowVolumeExpansion: false</code>: <pre><code>$ kubectl get storageclasses.storage.k8s.io\nNAME              PROVISIONER                     RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE\nrook-ceph-block   rook-ceph.rbd.csi.ceph.com      Retain          Immediate           false                  3d21h\nrook-ceph-fs      rook-ceph.cephfs.csi.ceph.com   Retain          Immediate           true                   3d21h\n</code></pre> </li> <li>To fix this simply set <code>allowVolumeExpansion: true</code> in the <code>StorageClass</code>. Below is a <code>StorageClass</code> with this option set, it is at the top level of the object (not in <code>.spec</code> or similar): <pre><code>allowVolumeExpansion: true\napiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: rook-ceph-block\nparameters:\n  clusterID: rook-ceph\n  csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner\n  [...]\n  imageFeatures: layering\n  imageFormat: \"2\"\n  pool: replicapool\nprovisioner: rook-ceph.rbd.csi.ceph.com\nreclaimPolicy: Retain\nvolumeBindingMode: Immediate\n</code></pre> </li> </ul>","title":"PersistentVolumeClaims / PersistentVolumes are not Resized"},{"location":"storage/rook/common-issues/#failed-to-retrieve-servicemonitor-servicemonitorsmonitoringcoreoscom-rook-ceph-mgr-is-forbidden","text":"<p>You have the Prometheus Operator installed in your Kubernetes cluster, but have not applied the RBAC necessary for the Rook Ceph Operator to be able to create the monitoring objects.</p> <p>To rectify this, you can run the following command and / or add the file to your deployment system:</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/rook/rook/master/cluster/examples/kubernetes/ceph/monitoring/rbac.yaml\n</code></pre>  <p>(Original file located at: https://github.com/rook/rook/blob/master/cluster/examples/kubernetes/ceph/monitoring/rbac.yaml)</p>","title":"<code>[...] failed to retrieve servicemonitor. servicemonitors.monitoring.coreos.com \"rook-ceph-mgr\" is forbidden: [...]</code>"},{"location":"storage/rook/common-issues/#failed-to-reconcile-cluster-rook-ceph-failed-to-create-servicemonitor-the-server-could-not-find-the-requested-resource-post-servicemonitorsmonitoringcoreoscom","text":"<p>This normally means that you don't have the Prometheus Operator installed in your Kubernetes cluster. It is required for <code>.spec.monitoring.enabled: true</code> in the CephCluster object to work (the operator to be able to create the <code>ServiceMonitor</code> object to enable monitoring).</p> <p>For the Rook Ceph - Prometheus Monitoring Setup Steps check the link.</p>","title":"<code>[...] failed to reconcile cluster \"rook-ceph\": [...] failed to create servicemonitor. the server could not find the requested resource (post servicemonitors.monitoring.coreos.com)</code>"},{"location":"storage/rook/common-issues/#solution-a-disable-monitoring-in-cephcluster","text":"<p>Set <code>.spec.monitoring.enabled</code> to <code>false</code> in your CephCluster object / yaml (and apply it).</p>","title":"Solution A: Disable Monitoring in CephCluster"},{"location":"storage/rook/common-issues/#solution-b-install-prometheus-operator","text":"<p>If you want to use Prometheus for monitoring your applications and in this case also Rook Ceph Cluster easily in Kubernetes, make sure to install the Prometheus Operator.</p> <p>Checkout the Prometheus Operator - Getting Started Guide.</p>","title":"Solution B: Install Prometheus Operator"},{"location":"storage/rook/common-issues/#unable-to-get-monitor-info-from-dns-srv-with-service-name-ceph-mon-cant-run-ceph-and-rbd-commands-in-the-rook-ceph-xyz-pod","text":"<p>You are only supposed to run <code>ceph</code>, <code>rbd</code>, <code>radosgw-admin</code>, etc., commands in the Rook Ceph Toolbox / Tools Pod.</p> <p>Regarding the Rook Ceph Toolbox Pod checkout the Rook documentation here: Rook Ceph Docs - Ceph Toolbox.</p>","title":"<code>unable to get monitor info from DNS SRV with service name: ceph-mon</code> / Can't run <code>ceph</code> and <code>rbd</code> commands in the Rook Ceph XYZ Pod"},{"location":"storage/rook/common-issues/#quick-command-to-rook-ceph-toolbox-pod","text":"<p>This requires you to have the Rook Ceph Toolbox deployed, see Rook Ceph Docs - Ceph Toolbox for more information.</p> <pre><code>kubectl -n rook-ceph exec -it $(kubectl -n rook-ceph get pod -l \"app=rook-ceph-tools\" -o jsonpath='{.items[0].metadata.name}') -- bash\n</code></pre>","title":"Quick Command to Rook Ceph Toolbox Pod"},{"location":"storage/rook/common-issues/#osd-id-x-my-id-y-osd-crash","text":"<ol> <li>Exec into a working Ceph OSD on that host <code>kubectl exec -n rook-ceph -it OSD_POD_NAME -- bash</code> (<code>ceph-bluestore-tool</code> command is needed), run the following commands:<ol> <li>Run <code>lsblk</code> to see all disks of the host.</li> <li>For every disks, run:<ol> <li>Run <code>ceph-bluestore-tool show-label --dev=/dev/sdX</code> (note down the OSD ID (<code>whoami</code> field in the JSON output) and which disk the OSD is on (example: <code>OSD 11 /dev/sda</code>).</li> </ol> </li> </ol> </li> <li>The <code>rook-ceph-osd-...</code> deployment needs to be updated with the new/ correct device path. The <code>ROOK_BLOCK_PATH</code> environment variable must have the correct device path (there are two occurrences, in the <code>containers:</code> and in <code>initContainers:</code> list).</li> <li>After a few seconds / minutes the OSD should show up as <code>up</code> in the <code>ceph osd tree</code> output (the command can be run in the <code>rook-ceph-tools</code> Pod). If you have scaled down the OSD Deployment, make sure to scale it up to <code>1</code> again (<code>kubectl scale -n rook-ceph deployment --replicas=1 rook-ceph-osd...</code>)</li> </ol>","title":"<code>OSD id X != my id Y</code> - OSD Crash"},{"location":"storage/rook/common-issues/#_read_bdev_label-failed-to-open-varlibcephosdceph-1block-13-permission-denied","text":"","title":"<code>_read_bdev_label failed to open /var/lib/ceph/osd/ceph-1/block: (13) Permission denied</code>"},{"location":"storage/rook/common-issues/#issue","text":"<ul> <li>OSD Pod is not starting with logs about the \"ceph osd block device\" and \"permission denied\"</li> </ul>","title":"Issue"},{"location":"storage/rook/common-issues/#solution-do-you-have-the-ceph-packages-installed-on-the-host-and-or-a-usergroup-named-ceph","text":"<p>This can potentially mess with the owner/group of the ceph osd block device, as described in GitHub rook/rook Issue 7519 \"OSD pod permissions broken, unable to open OSD superblock after node restart\".</p> <p>You can either change the user and group ID of the <code>ceph</code> user on the host to the one inside the <code>ceph/ceph</code> image that your Rook Ceph cluster is running right now (CephCluster object <code>.spec.cephVersion.image</code>).</p> <pre><code>$ kubectl get -n rook-ceph cephclusters.ceph.rook.io rook-ceph -o yaml\napiVersion: ceph.rook.io/v1\nkind: CephCluster\nmetadata:\n[...]\n  name: rook-ceph\n  namespace: rook-ceph\n[...]\nspec:\n  cephVersion:\n    image: quay.io/ceph/ceph:v16.2.6-20210927\n[...]\n</code></pre>  <p>Depending your hosts, you might not need to even have the <code>ceph</code> packages installed. If you are using Rook Ceph, you normally don't need any ceph related packages on the hosts.</p> <p>Should this have not fixed your issue, you might be running into some other permission issue. If your hosts are using a Linux distribution that uses SELinux, you might need to follow these steps to re-configure the Rook Ceph operator: Rook Ceph Docs - OpenShift Special Configuration Guide.</p>  <p>Should this page not have yielded you a solution, checkout the Ceph Common Issues doc as well.</p>","title":"Solution: Do you have the <code>ceph</code> package(s) installed on the host and / or a user/group named <code>ceph</code>?"}]}