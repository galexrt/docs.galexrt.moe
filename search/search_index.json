{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Home \u00b6 Welcome to galexrt's docs! This is a documentation of tips, tricks, \"hacks\", pitfalls and more in multiple different areas. This work is licensed under a Creative Commons Attribution 4.0 International License . Use the information on this page however you want, just remember no guarantees given.","title":"Home"},{"location":"#home","text":"Welcome to galexrt's docs! This is a documentation of tips, tricks, \"hacks\", pitfalls and more in multiple different areas. This work is licensed under a Creative Commons Attribution 4.0 International License . Use the information on this page however you want, just remember no guarantees given.","title":"Home"},{"location":"privacy-policy/","text":"The Site Notice / Impressum can be found here: Site Notice / Impressum page . German version of the Privacy Policy / Datenschutzerkl\u00e4rung below. Privacy Policy \u00b6 1. An overview of data protection \u00b6 General information \u00b6 The following information will provide you with an easy to navigate overview of what will happen with your personal data when you visit this website. The term \"personal data\" comprises all data that can be used to personally identify you. For detailed information about the subject matter of data protection, please consult our Data Protection Declaration, which we have included beneath this copy. Data recording on this website \u00b6 Who is the responsible party for the recording of data on this website (i.e. the \"controller\")? \u00b6 The data on this website is processed by the operator of the website, whose contact information is available under section \"Information Required by Law\" on this website. How do we record your data? \u00b6 We collect your data as a result of your sharing of your data with us. This may, for instance be information you enter into our contact form. Other data shall be recorded by our IT systems automatically or after you consent to its recording during your website visit. This data comprises primarily technical information (e.g. web browser, operating system or time the site was accessed). This information is recorded automatically when you access this website. What are the purposes we use your data for? \u00b6 A portion of the information is generated to guarantee the error free provision of the website. Other data may be used to analyze your user patterns. What rights do you have as far as your information is concerned? \u00b6 You have the right to receive information about the source, recipients and purposes of your archived personal data at any time without having to pay a fee for such disclosures. You also have the right to demand that your data are rectified or eradicated. If you have consented to data processing, you have the option to revoke this consent at any time, which shall affect all future data processing. Moreover, you have the right to demand that the processing of your data be restricted under certain circumstances. Furthermore, you have the right to log a complaint with the competent supervising agency. Please do not hesitate to contact us at any time under the address disclosed in section \"Information Required by Law\" on this website if you have questions about this or any other data protection related issues. Analysis tools and tools provided by third parties \u00b6 There is a possibility that your browsing patterns will be statistically analyzed when your visit this website. Such analyses are performed primarily with what we refer to as analysis programs. For detailed information about these analysis programs please consult our Data Protection Declaration below. 2. Hosting and Content Delivery Networks (CDN) \u00b6 External Hosting \u00b6 This website is hosted by an external service provider (host). Personal data collected on this website are stored on the servers of the host. These may include, but are not limited to, IP addresses, contact requests, metadata and communications, contract information, contact information, names, web page access, and other data generated through a web site. The host is used for the purpose of fulfilling the contract with our potential and existing customers (Art. 6 para. 1 lit. b GDPR) and in the interest of secure, fast and efficient provision of our online services by a professional provider (Art. 6 para. 1 lit. f GDPR). Our host will only process your data to the extent necessary to fulfil its performance obligations and to follow our instructions with respect to such data. We are using the following host: Cloudflare Inc. 101 Townsend St, San Francisco, CA 94107 USA GitHub Inc. 88 Colin P Kelly Jr St, San Francisco, CA 94107 United States Hetzner Online GmbH Industriestr. 25 91710 Gunzenhausen Germany Execution of a contract data processing agreement \u00b6 In order to guarantee processing in compliance with data protection regulations, we have concluded an order processing contract with our host. Cloudflare \u00b6 We use the \"Cloudflare\" service provided by Cloudflare Inc., 101 Townsend St., San Francisco, CA 94107, USA. (hereinafter referred to as \"Cloudflare\"). Cloudflare offers a content delivery network with DNS that is available worldwide. As a result, the information transfer that occurs between your browser and our website is technically routed via Cloudflare\u2019s network. This enables Cloudflare to analyze data transactions between your browser and our website and to work as a filter between our servers and potentially malicious data traffic from the Internet. In this context, Cloudflare may also use cookies or other technologies deployed to recognize Internet users, which shall, however, only be used for the herein described purpose. The use of Cloudflare is based on our legitimate interest in a provision of our website offerings that is as error free and secure as possible (Art. 6 Sect. 1 lit. f GDPR). For more information on Cloudflare\u2019s security precautions and data privacy policies, please follow this link: https://www.cloudflare.com/privacypolicy/ . 3. General information and mandatory information \u00b6 Data protection \u00b6 The operators of this website and its pages take the protection of your personal data very seriously. Hence, we handle your personal data as confidential information and in compliance with the statutory data protection regulations and this Data Protection Declaration. Whenever you use this website, a variety of personal information will be collected. Personal data comprises data that can be used to personally identify you. This Data Protection Declaration explains which data we collect as well as the purposes we use this data for. It also explains how, and for which purpose the information is collected. We herewith advise you that the transmission of data via the Internet (i.e. through e-mail communications) may be prone to security gaps. It is not possible to completely protect data against third-party access. Information about the responsible party (referred to as the \"controller\" in the GDPR) \u00b6 The data processing controller on this website is: Alexander Trost Postfach 11 75196 K\u00e4mpfelbach Germany Phone: +497232370228 E-mail: galexrt@googlemail.com The controller is the natural person or legal entity that single-handedly or jointly with others makes decisions as to the purposes of and resources for the processing of personal data (e.g. names, e-mail addresses, etc.). Storage duration \u00b6 Unless a more specific storage period has been specified in this privacy policy, your personal data will remain with us until the purpose for which it was collected no longer applies. If you assert a justified request for deletion or revoke your consent to data processing, your data will be deleted, unless we have other legally permissible reasons for storing your personal data (e.g. tax or commercial law retention periods); in the latter case, the deletion will take place after these reasons cease to apply. Information on data transfer to the USA \u00b6 Our website uses, in particular, tools from companies based in the USA. When these tools are active, your personal information may be transferred to the US servers of these companies. We must point out that the USA is not a safe third country within the meaning of EU data protection law. US companies are required to release personal data to security authorities without you as the data subject being able to take legal action against this. The possibility cannot therefore be excluded that US authorities (e.g. secret services) may process, evaluate and permanently store your data on US servers for monitoring purposes. We have no influence over these processing activities. Revocation of your consent to the processing of data \u00b6 A wide range of data processing transactions are possible only subject to your express consent. You can also revoke at any time any consent you have already given us. This shall be without prejudice to the lawfulness of any data collection that occurred prior to your revocation. Right to object to the collection of data in special cases; right to object to direct advertising (Art. 21 GDPR) \u00b6 IN THE EVENT THAT DATA ARE PROCESSED ON THE BASIS OF ART. 6 SECT. 1 LIT. E OR F GDPR, YOU HAVE THE RIGHT TO AT ANY TIME OBJECT TO THE PROCESSING OF YOUR PERSONAL DATA BASED ON GROUNDS ARISING FROM YOUR UNIQUE SITUATION. THIS ALSO APPLIES TO ANY PROFILING BASED ON THESE PROVISIONS. TO DETERMINE THE LEGAL BASIS, ON WHICH ANY PROCESSING OF DATA IS BASED, PLEASE CONSULT THIS DATA PROTECTION DECLARATION. IF YOU LOG AN OBJECTION, WE WILL NO LONGER PROCESS YOUR AFFECTED PERSONAL DATA, UNLESS WE ARE IN A POSITION TO PRESENT COMPELLING PROTECTION WORTHY GROUNDS FOR THE PROCESSING OF YOUR DATA, THAT OUTWEIGH YOUR INTERESTS, RIGHTS AND FREEDOMS OR IF THE PURPOSE OF THE PROCESSING IS THE CLAIMING, EXERCISING OR DEFENCE OF LEGAL ENTITLEMENTS (OBJECTION PURSUANT TO ART. 21 SECT. 1 GDPR). IF YOUR PERSONAL DATA IS BEING PROCESSED IN ORDER TO ENGAGE IN DIRECT ADVERTISING, YOU HAVE THE RIGHT TO AT ANY TIME OBJECT TO THE PROCESSING OF YOUR AFFECTED PERSONAL DATA FOR THE PURPOSES OF SUCH ADVERTISING. THIS ALSO APPLIES TO PROFILING TO THE EXTENT THAT IT IS AFFILIATED WITH SUCH DIRECT ADVERTISING. IF YOU OBJECT, YOUR PERSONAL DATA WILL SUBSEQUENTLY NO LONGER BE USED FOR DIRECT ADVERTISING PURPOSES (OBJECTION PURSUANT TO ART. 21 SECT. 2 GDPR). Right to log a complaint with the competent supervisory agency \u00b6 In the event of violations of the GDPR, data subjects are entitled to log a complaint with a supervisory agency, in particular in the member state where they usually maintain their domicile, place of work or at the place where the alleged violation occurred. The right to log a complaint is in effect regardless of any other administrative or court proceedings available as legal recourses. Right to data portability \u00b6 You have the right to demand that we hand over any data we automatically process on the basis of your consent or in order to fulfil a contract be handed over to you or a third party in a commonly used, machine readable format. If you should demand the direct transfer of the data to another controller, this will be done only if it is technically feasible. SSL and/or TLS encryption \u00b6 For security reasons and to protect the transmission of confidential content, such as purchase orders or inquiries you submit to us as the website operator, this website uses either an SSL or a TLS encryption program. You can recognize an encrypted connection by checking whether the address line of the browser switches from \"http://\" to \"https://\" and also by the appearance of the lock icon in the browser line. If the SSL or TLS encryption is activated, data you transmit to us cannot be read by third parties. Information about, rectification and eradication of data \u00b6 Within the scope of the applicable statutory provisions, you have the right to at any time demand information about your archived personal data, their source and recipients as well as the purpose of the processing of your data. You may also have a right to have your data rectified or eradicated. If you have questions about this subject matter or any other questions about personal data, please do not hesitate to contact us at any time at the address provided in section \"Information Required by Law.\" Right to demand processing restrictions \u00b6 You have the right to demand the imposition of restrictions as far as the processing of your personal data is concerned. To do so, you may contact us at any time at the address provided in section \"Information Required by Law.\" The right to demand restriction of processing applies in the following cases: In the event that you should dispute the correctness of your data archived by us, we will usually need some time to verify this claim. During the time that this investigation is ongoing, you have the right to demand that we restrict the processing of your personal data. If the processing of your personal data was/is conducted in an unlawful manner, you have the option to demand the restriction of the processing of your data in lieu of demanding the eradication of this data. If we do not need your personal data any longer and you need it to exercise, defend or claim legal entitlements, you have the right to demand the restriction of the processing of your personal data instead of its eradication. If you have raised an objection pursuant to Art. 21 Sect. 1 GDPR, your rights and our rights will have to be weighed against each other. As long as it has not been determined whose interests prevail, you have the right to demand a restriction of the processing of your personal data. If you have restricted the processing of your personal data, these data \u2013 with the exception of their archiving \u2013 may be processed only subject to your consent or to claim, exercise or defend legal entitlements or to protect the rights of other natural persons or legal entities or for important public interest reasons cited by the European Union or a member state of the EU. 4. Recording of data on this website \u00b6 Cookies \u00b6 Our websites and pages use what the industry refers to as \"cookies.\" Cookies are small text files that do not cause any damage to your device. They are either stored temporarily for the duration of a session (session cookies) or they are permanently archived on your device (permanent cookies). Session cookies are automatically deleted once you terminate your visit. Permanent cookies remain archived on your device until you actively delete them or they are automatically eradicated by your web browser. In some cases, it is possible that third-party cookies are stored on your device once you enter our site (third-party cookies). These cookies enable you or us to take advantage of certain services offered by the third party (e.g. cookies for the processing of payment services). Cookies have a variety of functions. Many cookies are technically essential since certain website functions would not work in the absence of the cookies (e.g. the shopping cart function or the display of videos). The purpose of other cookies may be the analysis of user patterns or the display of promotional messages. Cookies, which are required for the performance of electronic communication transactions (required cookies) or for the provision of certain functions you want to use (functional cookies, e.g. for the shopping cart function) or those that are necessary for the optimization of the website (e.g. cookies that provide measurable insights into the web audience), shall be stored on the basis of Art. 6 Sect. 1 lit. f GDPR, unless a different legal basis is cited. The operator of the website has a legitimate interest in the storage of cookies to ensure the technically error free and optimized provision of the operator\u2019s services. If your consent to the storage of the cookies has been requested, the respective cookies are stored exclusively on the basis of the consent obtained (Art. 6 Sect. 1 lit. a GDPR); this consent may be revoked at any time. You have the option to set up your browser in such a manner that you will be notified any time cookies are placed and to permit the acceptance of cookies only in specific cases. You may also exclude the acceptance of cookies in certain cases or in general or activate the delete function for the automatic eradication of cookies when the browser closes. If cookies are deactivated, the functions of this website may be limited. In the event that third-party cookies are used or if cookies are used for analytical purposes, we will separately notify you in conjunction with this Data Protection Policy and, if applicable, ask for your consent. Server log files \u00b6 The provider of this website and its pages automatically collects and stores information in so-called server log files, which your browser communicates to us automatically. The information comprises: The type and version of browser used The used operating system Referrer URL The hostname of the accessing computer The time of the server inquiry The IP address This data is not merged with other data sources. This data is recorded on the basis of Art. 6 Sect. 1 lit. f GDPR. The operator of the website has a legitimate interest in the technically error free depiction and the optimization of the operator\u2019s website. In order to achieve this, server log files must be recorded. Contact form \u00b6 If you submit inquiries to us via our contact form, the information provided in the contact form as well as any contact information provided therein will be stored by us in order to handle your inquiry and in the event that we have further questions. We will not share this information without your consent. The processing of these data is based on Art. 6 para. 1 lit. b GDPR, if your request is related to the execution of a contract or if it is necessary to carry out pre-contractual measures. In all other cases the processing is based on our legitimate interest in the effective processing of the requests addressed to us (Art. 6 Para. 1 lit. f GDPR) or on your agreement (Art. 6 Para. 1 lit. a GDPR) if this has been requested. The information you have entered into the contact form shall remain with us until you ask us to eradicate the data, revoke your consent to the archiving of data or if the purpose for which the information is being archived no longer exists (e.g. after we have concluded our response to your inquiry). This shall be without prejudice to any mandatory legal provisions \u2013 in particular retention periods. Request by e-mail, telephone or fax \u00b6 If you contact us by e-mail, telephone or fax, your request, including all resulting personal data (name, request) will be stored and processed by us for the purpose of processing your request. We do not pass these data on without your consent. These data are processed on the basis of Art. 6 Sect. 1 lit. b GDPR if your inquiry is related to the fulfillment of a contract or is required for the performance of pre-contractual measures. In all other cases, the data are processed on the basis of our legitimate interest in the effective handling of inquiries submitted to us (Art. 6 Sect. 1 lit. f GDPR) or on the basis of your consent (Art. 6 Sect. 1 lit. a GDPR) if it has been obtained. The data sent by you to us via contact requests remain with us until you request us to delete, revoke your consent to the storage or the purpose for the data storage lapses (e.g. after completion of your request). Mandatory statutory provisions - in particular statutory retention periods - remain unaffected. Datenschutz\u00aderkl\u00e4rung \u00b6 1. Datenschutz auf einen Blick \u00b6 Allgemeine Hinweise \u00b6 Die folgenden Hinweise geben einen einfachen \u00dcberblick dar\u00fcber, was mit Ihren personenbezogenen Daten passiert, wenn Sie diese Website besuchen. Personenbezogene Daten sind alle Daten, mit denen Sie pers\u00f6nlich identifiziert werden k\u00f6nnen. Ausf\u00fchrliche Informationen zum Thema Datenschutz entnehmen Sie unserer unter diesem Text aufgef\u00fchrten Datenschutzerkl\u00e4rung. Datenerfassung auf dieser Website \u00b6 Wer ist verantwortlich f\u00fcr die Datenerfassung auf dieser Website? \u00b6 Die Datenverarbeitung auf dieser Website erfolgt durch den Websitebetreiber. Dessen Kontaktdaten k\u00f6nnen Sie dem Impressum dieser Website entnehmen. Wie erfassen wir Ihre Daten? \u00b6 Ihre Daten werden zum einen dadurch erhoben, dass Sie uns diese mitteilen. Hierbei kann es sich z. B. um Daten handeln, die Sie in ein Kontaktformular eingeben. Andere Daten werden automatisch oder nach Ihrer Einwilligung beim Besuch der Website durch unsere IT-Systeme erfasst. Das sind vor allem technische Daten (z. B. Internetbrowser, Betriebssystem oder Uhrzeit des Seitenaufrufs). Die Erfassung dieser Daten erfolgt automatisch, sobald Sie diese Website betreten. Wof\u00fcr nutzen wir Ihre Daten? \u00b6 Ein Teil der Daten wird erhoben, um eine fehlerfreie Bereitstellung der Website zu gew\u00e4hrleisten. Andere Daten k\u00f6nnen zur Analyse Ihres Nutzerverhaltens verwendet werden. Welche Rechte haben Sie bez\u00fcglich Ihrer Daten? \u00b6 Sie haben jederzeit das Recht, unentgeltlich Auskunft \u00fcber Herkunft, Empf\u00e4nger und Zweck Ihrer gespeicherten personenbezogenen Daten zu erhalten. Sie haben au\u00dferdem ein Recht, die Berichtigung oder L\u00f6schung dieser Daten zu verlangen. Wenn Sie eine Einwilligung zur Datenverarbeitung erteilt haben, k\u00f6nnen Sie diese Einwilligung jederzeit f\u00fcr die Zukunft widerrufen. Au\u00dferdem haben Sie das Recht, unter bestimmten Umst\u00e4nden die Einschr\u00e4nkung der Verarbeitung Ihrer personenbezogenen Daten zu verlangen. Des Weiteren steht Ihnen ein Beschwerderecht bei der zust\u00e4ndigen Aufsichtsbeh\u00f6rde zu. Hierzu sowie zu weiteren Fragen zum Thema Datenschutz k\u00f6nnen Sie sich jederzeit unter der im Impressum angegebenen Adresse an uns wenden. Analyse-Tools und Tools von Dritt\u00adanbietern \u00b6 Beim Besuch dieser Website kann Ihr Surf-Verhalten statistisch ausgewertet werden. Das geschieht vor allem mit sogenannten Analyseprogrammen. Detaillierte Informationen zu diesen Analyseprogrammen finden Sie in der folgenden Datenschutzerkl\u00e4rung. 2. Hosting und Content Delivery Networks (CDN) \u00b6 Externes Hosting \u00b6 Diese Website wird bei einem externen Dienstleister gehostet (Hoster). Die personenbezogenen Daten, die auf dieser Website erfasst werden, werden auf den Servern des Hosters gespeichert. Hierbei kann es sich v. a. um IP-Adressen, Kontaktanfragen, Meta- und Kommunikationsdaten, Vertragsdaten, Kontaktdaten, Namen, Websitezugriffe und sonstige Daten, die \u00fcber eine Website generiert werden, handeln. Der Einsatz des Hosters erfolgt zum Zwecke der Vertragserf\u00fcllung gegen\u00fcber unseren potenziellen und bestehenden Kunden (Art. 6 Abs. 1 lit. b DSGVO) und im Interesse einer sicheren, schnellen und effizienten Bereitstellung unseres Online-Angebots durch einen professionellen Anbieter (Art. 6 Abs. 1 lit. f DSGVO). Unser Hoster wird Ihre Daten nur insoweit verarbeiten, wie dies zur Erf\u00fcllung seiner Leistungspflichten erforderlich ist und unsere Weisungen in Bezug auf diese Daten befolgen. Wir setzen folgenden Hoster ein: Cloudflare Inc. 101 Townsend St, San Francisco, CA 94107 USA GitHub Inc. 88 Colin P Kelly Jr St, San Francisco, CA 94107 United States Hetzner Online GmbH Industriestr. 25 91710 Gunzenhausen Germany Abschluss eines Vertrages \u00fcber Auftragsverarbeitung \u00b6 Um die datenschutzkonforme Verarbeitung zu gew\u00e4hrleisten, haben wir einen Vertrag \u00fcber Auftragsverarbeitung mit unserem Hoster geschlossen. Cloudflare \u00b6 Wir nutzen den Service \u201eCloudflare\". Anbieter ist die Cloudflare Inc., 101 Townsend St., San Francisco, CA 94107, USA (im Folgenden \u201eCloudflare\"). Cloudflare bietet ein weltweit verteiltes Content Delivery Network mit DNS an. Dabei wird technisch der Informationstransfer zwischen Ihrem Browser und unserer Website \u00fcber das Netzwerk von Cloudflare geleitet. Das versetzt Cloudflare in die Lage, den Datenverkehr zwischen Ihrem Browser und unserer Website zu analysieren und als Filter zwischen unseren Servern und potenziell b\u00f6sartigem Datenverkehr aus dem Internet zu dienen. Hierbei kann Cloudflare auch Cookies oder sonstige Technologien zur Wiedererkennung von Internetnutzern einsetzen, die jedoch allein zum hier beschriebenen Zweck verwendet werden. Der Einsatz von Cloudflare beruht auf unserem berechtigten Interesse an einer m\u00f6glichst fehlerfreien und sicheren Bereitstellung unseres Webangebotes (Art. 6 Abs. 1 lit. f DSGVO). Weitere Informationen zum Thema Sicherheit und Datenschutz bei Cloudflare finden Sie hier: https://www.cloudflare.com/privacypolicy/ . 3. Allgemeine Hinweise und Pflicht\u00adinformationen \u00b6 Datenschutz \u00b6 Die Betreiber dieser Seiten nehmen den Schutz Ihrer pers\u00f6nlichen Daten sehr ernst. Wir behandeln Ihre personenbezogenen Daten vertraulich und entsprechend der gesetzlichen Datenschutzvorschriften sowie dieser Datenschutzerkl\u00e4rung. Wenn Sie diese Website benutzen, werden verschiedene personenbezogene Daten erhoben. Personenbezogene Daten sind Daten, mit denen Sie pers\u00f6nlich identifiziert werden k\u00f6nnen. Die vorliegende Datenschutzerkl\u00e4rung erl\u00e4utert, welche Daten wir erheben und wof\u00fcr wir sie nutzen. Sie erl\u00e4utert auch, wie und zu welchem Zweck das geschieht. Wir weisen darauf hin, dass die Daten\u00fcbertragung im Internet (z. B. bei der Kommunikation per E-Mail) Sicherheitsl\u00fccken aufweisen kann. Ein l\u00fcckenloser Schutz der Daten vor dem Zugriff durch Dritte ist nicht m\u00f6glich. Hinweis zur verantwortlichen Stelle \u00b6 Die verantwortliche Stelle f\u00fcr die Datenverarbeitung auf dieser Website ist: Alexander Trost Kelterstr. 4/1 75196 Remchingen Germany Telefon: +497232370228 E-Mail: galexrt@googlemail.com Verantwortliche Stelle ist die nat\u00fcrliche oder juristische Person, die allein oder gemeinsam mit anderen \u00fcber die Zwecke und Mittel der Verarbeitung von personenbezogenen Daten (z. B. Namen, E-Mail-Adressen o. \u00c4.) entscheidet. Speicherdauer \u00b6 Soweit innerhalb dieser Datenschutzerkl\u00e4rung keine speziellere Speicherdauer genannt wurde, verbleiben Ihre personenbezogenen Daten bei uns, bis der Zweck f\u00fcr die Datenverarbeitung entf\u00e4llt. Wenn Sie ein berechtigtes L\u00f6schersuchen geltend machen oder eine Einwilligung zur Datenverarbeitung widerrufen, werden Ihre Daten gel\u00f6scht, sofern wir keinen anderen rechtlich zul\u00e4ssigen Gr\u00fcnde f\u00fcr die Speicherung Ihrer personenbezogenen Daten haben (z.B. steuer- oder handelsrechtliche Aufbewahrungsfristen); im letztgenannten Fall erfolgt die L\u00f6schung nach Fortfall dieser Gr\u00fcnde. Hinweis zur Datenweitergabe in die USA \u00b6 Auf unserer Website sind unter anderem Tools von Unternehmen mit Sitz in den USA eingebunden. Wenn diese Tools aktiv sind, k\u00f6nnen Ihre personenbezogenen Daten an die US-Server der jeweiligen Unternehmen weitergegeben werden. Wir weisen darauf hin, dass die USA kein sicherer Drittstaat im Sinne des EU-Datenschutzrechts sind. US-Unternehmen sind dazu verpflichtet, personenbezogene Daten an Sicherheitsbeh\u00f6rden herauszugeben, ohne dass Sie als Betroffener hiergegen gerichtlich vorgehen k\u00f6nnten. Es kann daher nicht ausgeschlossen werden, dass US-Beh\u00f6rden (z.B. Geheimdienste) Ihre auf US-Servern befindlichen Daten zu \u00dcberwachungszwecken verarbeiten, auswerten und dauerhaft speichern. Wir haben auf diese Verarbeitungst\u00e4tigkeiten keinen Einfluss. Widerruf Ihrer Einwilligung zur Datenverarbeitung \u00b6 Viele Datenverarbeitungsvorg\u00e4nge sind nur mit Ihrer ausdr\u00fccklichen Einwilligung m\u00f6glich. Sie k\u00f6nnen eine bereits erteilte Einwilligung jederzeit widerrufen. Die Rechtm\u00e4\u00dfigkeit der bis zum Widerruf erfolgten Datenverarbeitung bleibt vom Widerruf unber\u00fchrt. Widerspruchsrecht gegen die Datenerhebung in besonderen F\u00e4llen sowie gegen Direktwerbung (Art. 21 DSGVO) \u00b6 WENN DIE DATENVERARBEITUNG AUF GRUNDLAGE VON ART. 6 ABS. 1 LIT. E ODER F DSGVO ERFOLGT, HABEN SIE JEDERZEIT DAS RECHT, AUS GR\u00dcNDEN, DIE SICH AUS IHRER BESONDEREN SITUATION ERGEBEN, GEGEN DIE VERARBEITUNG IHRER PERSONENBEZOGENEN DATEN WIDERSPRUCH EINZULEGEN; DIES GILT AUCH F\u00dcR EIN AUF DIESE BESTIMMUNGEN GEST\u00dcTZTES PROFILING. DIE JEWEILIGE RECHTSGRUNDLAGE, AUF DENEN EINE VERARBEITUNG BERUHT, ENTNEHMEN SIE DIESER DATENSCHUTZERKL\u00c4RUNG. WENN SIE WIDERSPRUCH EINLEGEN, WERDEN WIR IHRE BETROFFENEN PERSONENBEZOGENEN DATEN NICHT MEHR VERARBEITEN, ES SEI DENN, WIR K\u00d6NNEN ZWINGENDE SCHUTZW\u00dcRDIGE GR\u00dcNDE F\u00dcR DIE VERARBEITUNG NACHWEISEN, DIE IHRE INTERESSEN, RECHTE UND FREIHEITEN \u00dcBERWIEGEN ODER DIE VERARBEITUNG DIENT DER GELTENDMACHUNG, AUS\u00dcBUNG ODER VERTEIDIGUNG VON RECHTSANSPR\u00dcCHEN (WIDERSPRUCH NACH ART. 21 ABS. 1 DSGVO). WERDEN IHRE PERSONENBEZOGENEN DATEN VERARBEITET, UM DIREKTWERBUNG ZU BETREIBEN, SO HABEN SIE DAS RECHT, JEDERZEIT WIDERSPRUCH GEGEN DIE VERARBEITUNG SIE BETREFFENDER PERSONENBEZOGENER DATEN ZUM ZWECKE DERARTIGER WERBUNG EINZULEGEN; DIES GILT AUCH F\u00dcR DAS PROFILING, SOWEIT ES MIT SOLCHER DIREKTWERBUNG IN VERBINDUNG STEHT. WENN SIE WIDERSPRECHEN, WERDEN IHRE PERSONENBEZOGENEN DATEN ANSCHLIESSEND NICHT MEHR ZUM ZWECKE DER DIREKTWERBUNG VERWENDET (WIDERSPRUCH NACH ART. 21 ABS. 2 DSGVO). Beschwerde\u00adrecht bei der zust\u00e4ndigen Aufsichts\u00adbeh\u00f6rde \u00b6 Im Falle von Verst\u00f6\u00dfen gegen die DSGVO steht den Betroffenen ein Beschwerderecht bei einer Aufsichtsbeh\u00f6rde, insbesondere in dem Mitgliedstaat ihres gew\u00f6hnlichen Aufenthalts, ihres Arbeitsplatzes oder des Orts des mutma\u00dflichen Versto\u00dfes zu. Das Beschwerderecht besteht unbeschadet anderweitiger verwaltungsrechtlicher oder gerichtlicher Rechtsbehelfe. Recht auf Daten\u00ad\u00fcbertrag\u00adbarkeit \u00b6 Sie haben das Recht, Daten, die wir auf Grundlage Ihrer Einwilligung oder in Erf\u00fcllung eines Vertrags automatisiert verarbeiten, an sich oder an einen Dritten in einem g\u00e4ngigen, maschinenlesbaren Format aush\u00e4ndigen zu lassen. Sofern Sie die direkte \u00dcbertragung der Daten an einen anderen Verantwortlichen verlangen, erfolgt dies nur, soweit es technisch machbar ist. SSL- bzw. TLS-Verschl\u00fcsselung \u00b6 Diese Seite nutzt aus Sicherheitsgr\u00fcnden und zum Schutz der \u00dcbertragung vertraulicher Inhalte, wie zum Beispiel Bestellungen oder Anfragen, die Sie an uns als Seitenbetreiber senden, eine SSL- bzw. TLS-Verschl\u00fcsselung. Eine verschl\u00fcsselte Verbindung erkennen Sie daran, dass die Adresszeile des Browsers von \u201ehttp://\" auf \u201ehttps://\" wechselt und an dem Schloss-Symbol in Ihrer Browserzeile. Wenn die SSL- bzw. TLS-Verschl\u00fcsselung aktiviert ist, k\u00f6nnen die Daten, die Sie an uns \u00fcbermitteln, nicht von Dritten mitgelesen werden. Auskunft, L\u00f6schung und Berichtigung \u00b6 Sie haben im Rahmen der geltenden gesetzlichen Bestimmungen jederzeit das Recht auf unentgeltliche Auskunft \u00fcber Ihre gespeicherten personenbezogenen Daten, deren Herkunft und Empf\u00e4nger und den Zweck der Datenverarbeitung und ggf. ein Recht auf Berichtigung oder L\u00f6schung dieser Daten. Hierzu sowie zu weiteren Fragen zum Thema personenbezogene Daten k\u00f6nnen Sie sich jederzeit unter der im Impressum angegebenen Adresse an uns wenden. Recht auf Einschr\u00e4nkung der Verarbeitung \u00b6 Sie haben das Recht, die Einschr\u00e4nkung der Verarbeitung Ihrer personenbezogenen Daten zu verlangen. Hierzu k\u00f6nnen Sie sich jederzeit unter der im Impressum angegebenen Adresse an uns wenden. Das Recht auf Einschr\u00e4nkung der Verarbeitung besteht in folgenden F\u00e4llen: Wenn Sie die Richtigkeit Ihrer bei uns gespeicherten personenbezogenen Daten bestreiten, ben\u00f6tigen wir in der Regel Zeit, um dies zu \u00fcberpr\u00fcfen. F\u00fcr die Dauer der Pr\u00fcfung haben Sie das Recht, die Einschr\u00e4nkung der Verarbeitung Ihrer personenbezogenen Daten zu verlangen. Wenn die Verarbeitung Ihrer personenbezogenen Daten unrechtm\u00e4\u00dfig geschah/geschieht, k\u00f6nnen Sie statt der L\u00f6schung die Einschr\u00e4nkung der Datenverarbeitung verlangen. Wenn wir Ihre personenbezogenen Daten nicht mehr ben\u00f6tigen, Sie sie jedoch zur Aus\u00fcbung, Verteidigung oder Geltendmachung von Rechtsanspr\u00fcchen ben\u00f6tigen, haben Sie das Recht, statt der L\u00f6schung die Einschr\u00e4nkung der Verarbeitung Ihrer personenbezogenen Daten zu verlangen. Wenn Sie einen Widerspruch nach Art. 21 Abs. 1 DSGVO eingelegt haben, muss eine Abw\u00e4gung zwischen Ihren und unseren Interessen vorgenommen werden. Solange noch nicht feststeht, wessen Interessen \u00fcberwiegen, haben Sie das Recht, die Einschr\u00e4nkung der Verarbeitung Ihrer personenbezogenen Daten zu verlangen. Wenn Sie die Verarbeitung Ihrer personenbezogenen Daten eingeschr\u00e4nkt haben, d\u00fcrfen diese Daten \u2013 von ihrer Speicherung abgesehen \u2013 nur mit Ihrer Einwilligung oder zur Geltendmachung, Aus\u00fcbung oder Verteidigung von Rechtsanspr\u00fcchen oder zum Schutz der Rechte einer anderen nat\u00fcrlichen oder juristischen Person oder aus Gr\u00fcnden eines wichtigen \u00f6ffentlichen Interesses der Europ\u00e4ischen Union oder eines Mitgliedstaats verarbeitet werden. 4. Datenerfassung auf dieser Website \u00b6 Cookies \u00b6 Unsere Internetseiten verwenden so genannte \u201eCookies\". Cookies sind kleine Textdateien und richten auf Ihrem Endger\u00e4t keinen Schaden an. Sie werden entweder vor\u00fcbergehend f\u00fcr die Dauer einer Sitzung (Session-Cookies) oder dauerhaft (permanente Cookies) auf Ihrem Endger\u00e4t gespeichert. Session-Cookies werden nach Ende Ihres Besuchs automatisch gel\u00f6scht. Permanente Cookies bleiben auf Ihrem Endger\u00e4t gespeichert, bis Sie diese selbst l\u00f6schen oder eine automatische L\u00f6schung durch Ihren Webbrowser erfolgt. Teilweise k\u00f6nnen auch Cookies von Drittunternehmen auf Ihrem Endger\u00e4t gespeichert werden, wenn Sie unsere Seite betreten (Third-Party-Cookies). Diese erm\u00f6glichen uns oder Ihnen die Nutzung bestimmter Dienstleistungen des Drittunternehmens (z.B. Cookies zur Abwicklung von Zahlungsdienstleistungen). Cookies haben verschiedene Funktionen. Zahlreiche Cookies sind technisch notwendig, da bestimmte Websitefunktionen ohne diese nicht funktionieren w\u00fcrden (z.B. die Warenkorbfunktion oder die Anzeige von Videos). Andere Cookies dienen dazu, das Nutzerverhalten auszuwerten oder Werbung anzuzeigen. Cookies, die zur Durchf\u00fchrung des elektronischen Kommunikationsvorgangs (notwendige Cookies) oder zur Bereitstellung bestimmter, von Ihnen erw\u00fcnschter Funktionen (funktionale Cookies, z. B. f\u00fcr die Warenkorbfunktion) oder zur Optimierung der Website (z.B. Cookies zur Messung des Webpublikums) erforderlich sind, werden auf Grundlage von Art. 6 Abs. 1 lit. f DSGVO gespeichert, sofern keine andere Rechtsgrundlage angegeben wird. Der Websitebetreiber hat ein berechtigtes Interesse an der Speicherung von Cookies zur technisch fehlerfreien und optimierten Bereitstellung seiner Dienste. Sofern eine Einwilligung zur Speicherung von Cookies abgefragt wurde, erfolgt die Speicherung der betreffenden Cookies ausschlie\u00dflich auf Grundlage dieser Einwilligung (Art. 6 Abs. 1 lit. a DSGVO); die Einwilligung ist jederzeit widerrufbar. Sie k\u00f6nnen Ihren Browser so einstellen, dass Sie \u00fcber das Setzen von Cookies informiert werden und Cookies nur im Einzelfall erlauben, die Annahme von Cookies f\u00fcr bestimmte F\u00e4lle oder generell ausschlie\u00dfen sowie das automatische L\u00f6schen der Cookies beim Schlie\u00dfen des Browsers aktivieren. Bei der Deaktivierung von Cookies kann die Funktionalit\u00e4t dieser Website eingeschr\u00e4nkt sein. Soweit Cookies von Drittunternehmen oder zu Analysezwecken eingesetzt werden, werden wir Sie hier\u00fcber im Rahmen dieser Datenschutzerkl\u00e4rung gesondert informieren und ggf. eine Einwilligung abfragen. Server-Log-Dateien \u00b6 Der Provider der Seiten erhebt und speichert automatisch Informationen in so genannten Server-Log-Dateien, die Ihr Browser automatisch an uns \u00fcbermittelt. Dies sind: Browsertyp und Browserversion verwendetes Betriebssystem Referrer URL Hostname des zugreifenden Rechners Uhrzeit der Serveranfrage IP-Adresse Eine Zusammenf\u00fchrung dieser Daten mit anderen Datenquellen wird nicht vorgenommen. Die Erfassung dieser Daten erfolgt auf Grundlage von Art. 6 Abs. 1 lit. f DSGVO. Der Websitebetreiber hat ein berechtigtes Interesse an der technisch fehlerfreien Darstellung und der Optimierung seiner Website \u2013 hierzu m\u00fcssen die Server-Log-Files erfasst werden. Kontaktformular \u00b6 Wenn Sie uns per Kontaktformular Anfragen zukommen lassen, werden Ihre Angaben aus dem Anfrageformular inklusive der von Ihnen dort angegebenen Kontaktdaten zwecks Bearbeitung der Anfrage und f\u00fcr den Fall von Anschlussfragen bei uns gespeichert. Diese Daten geben wir nicht ohne Ihre Einwilligung weiter. Die Verarbeitung dieser Daten erfolgt auf Grundlage von Art. 6 Abs. 1 lit. b DSGVO, sofern Ihre Anfrage mit der Erf\u00fcllung eines Vertrags zusammenh\u00e4ngt oder zur Durchf\u00fchrung vorvertraglicher Ma\u00dfnahmen erforderlich ist. In allen \u00fcbrigen F\u00e4llen beruht die Verarbeitung auf unserem berechtigten Interesse an der effektiven Bearbeitung der an uns gerichteten Anfragen (Art. 6 Abs. 1 lit. f DSGVO) oder auf Ihrer Einwilligung (Art. 6 Abs. 1 lit. a DSGVO) sofern diese abgefragt wurde. Die von Ihnen im Kontaktformular eingegebenen Daten verbleiben bei uns, bis Sie uns zur L\u00f6schung auffordern, Ihre Einwilligung zur Speicherung widerrufen oder der Zweck f\u00fcr die Datenspeicherung entf\u00e4llt (z. B. nach abgeschlossener Bearbeitung Ihrer Anfrage). Zwingende gesetzliche Bestimmungen \u2013 insbesondere Aufbewahrungsfristen \u2013 bleiben unber\u00fchrt. Anfrage per E-Mail, Telefon oder Telefax \u00b6 Wenn Sie uns per E-Mail, Telefon oder Telefax kontaktieren, wird Ihre Anfrage inklusive aller daraus hervorgehenden personenbezogenen Daten (Name, Anfrage) zum Zwecke der Bearbeitung Ihres Anliegens bei uns gespeichert und verarbeitet. Diese Daten geben wir nicht ohne Ihre Einwilligung weiter. Die Verarbeitung dieser Daten erfolgt auf Grundlage von Art. 6 Abs. 1 lit. b DSGVO, sofern Ihre Anfrage mit der Erf\u00fcllung eines Vertrags zusammenh\u00e4ngt oder zur Durchf\u00fchrung vorvertraglicher Ma\u00dfnahmen erforderlich ist. In allen \u00fcbrigen F\u00e4llen beruht die Verarbeitung auf unserem berechtigten Interesse an der effektiven Bearbeitung der an uns gerichteten Anfragen (Art. 6 Abs. 1 lit. f DSGVO) oder auf Ihrer Einwilligung (Art. 6 Abs. 1 lit. a DSGVO) sofern diese abgefragt wurde. Die von Ihnen an uns per Kontaktanfragen \u00fcbersandten Daten verbleiben bei uns, bis Sie uns zur L\u00f6schung auffordern, Ihre Einwilligung zur Speicherung widerrufen oder der Zweck f\u00fcr die Datenspeicherung entf\u00e4llt (z. B. nach abgeschlossener Bearbeitung Ihres Anliegens). Zwingende gesetzliche Bestimmungen \u2013 insbesondere gesetzliche Aufbewahrungsfristen \u2013 bleiben unber\u00fchrt.","title":"Privacy Policy"},{"location":"privacy-policy/#privacy-policy","text":"","title":"Privacy Policy"},{"location":"privacy-policy/#1-an-overview-of-data-protection","text":"","title":"1. An overview of data protection"},{"location":"privacy-policy/#general-information","text":"The following information will provide you with an easy to navigate overview of what will happen with your personal data when you visit this website. The term \"personal data\" comprises all data that can be used to personally identify you. For detailed information about the subject matter of data protection, please consult our Data Protection Declaration, which we have included beneath this copy.","title":"General information"},{"location":"privacy-policy/#data-recording-on-this-website","text":"","title":"Data recording on this website"},{"location":"privacy-policy/#who-is-the-responsible-party-for-the-recording-of-data-on-this-website-ie-the-controller","text":"The data on this website is processed by the operator of the website, whose contact information is available under section \"Information Required by Law\" on this website.","title":"Who is the responsible party for the recording of data on this website (i.e. the \"controller\")?"},{"location":"privacy-policy/#how-do-we-record-your-data","text":"We collect your data as a result of your sharing of your data with us. This may, for instance be information you enter into our contact form. Other data shall be recorded by our IT systems automatically or after you consent to its recording during your website visit. This data comprises primarily technical information (e.g. web browser, operating system or time the site was accessed). This information is recorded automatically when you access this website.","title":"How do we record your data?"},{"location":"privacy-policy/#what-are-the-purposes-we-use-your-data-for","text":"A portion of the information is generated to guarantee the error free provision of the website. Other data may be used to analyze your user patterns.","title":"What are the purposes we use your data for?"},{"location":"privacy-policy/#what-rights-do-you-have-as-far-as-your-information-is-concerned","text":"You have the right to receive information about the source, recipients and purposes of your archived personal data at any time without having to pay a fee for such disclosures. You also have the right to demand that your data are rectified or eradicated. If you have consented to data processing, you have the option to revoke this consent at any time, which shall affect all future data processing. Moreover, you have the right to demand that the processing of your data be restricted under certain circumstances. Furthermore, you have the right to log a complaint with the competent supervising agency. Please do not hesitate to contact us at any time under the address disclosed in section \"Information Required by Law\" on this website if you have questions about this or any other data protection related issues.","title":"What rights do you have as far as your information is concerned?"},{"location":"privacy-policy/#analysis-tools-and-tools-provided-by-third-parties","text":"There is a possibility that your browsing patterns will be statistically analyzed when your visit this website. Such analyses are performed primarily with what we refer to as analysis programs. For detailed information about these analysis programs please consult our Data Protection Declaration below.","title":"Analysis tools and tools provided by third parties"},{"location":"privacy-policy/#2-hosting-and-content-delivery-networks-cdn","text":"","title":"2. Hosting and Content Delivery Networks (CDN)"},{"location":"privacy-policy/#external-hosting","text":"This website is hosted by an external service provider (host). Personal data collected on this website are stored on the servers of the host. These may include, but are not limited to, IP addresses, contact requests, metadata and communications, contract information, contact information, names, web page access, and other data generated through a web site. The host is used for the purpose of fulfilling the contract with our potential and existing customers (Art. 6 para. 1 lit. b GDPR) and in the interest of secure, fast and efficient provision of our online services by a professional provider (Art. 6 para. 1 lit. f GDPR). Our host will only process your data to the extent necessary to fulfil its performance obligations and to follow our instructions with respect to such data. We are using the following host: Cloudflare Inc. 101 Townsend St, San Francisco, CA 94107 USA GitHub Inc. 88 Colin P Kelly Jr St, San Francisco, CA 94107 United States Hetzner Online GmbH Industriestr. 25 91710 Gunzenhausen Germany","title":"External Hosting"},{"location":"privacy-policy/#execution-of-a-contract-data-processing-agreement","text":"In order to guarantee processing in compliance with data protection regulations, we have concluded an order processing contract with our host.","title":"Execution of a contract data processing agreement"},{"location":"privacy-policy/#cloudflare","text":"We use the \"Cloudflare\" service provided by Cloudflare Inc., 101 Townsend St., San Francisco, CA 94107, USA. (hereinafter referred to as \"Cloudflare\"). Cloudflare offers a content delivery network with DNS that is available worldwide. As a result, the information transfer that occurs between your browser and our website is technically routed via Cloudflare\u2019s network. This enables Cloudflare to analyze data transactions between your browser and our website and to work as a filter between our servers and potentially malicious data traffic from the Internet. In this context, Cloudflare may also use cookies or other technologies deployed to recognize Internet users, which shall, however, only be used for the herein described purpose. The use of Cloudflare is based on our legitimate interest in a provision of our website offerings that is as error free and secure as possible (Art. 6 Sect. 1 lit. f GDPR). For more information on Cloudflare\u2019s security precautions and data privacy policies, please follow this link: https://www.cloudflare.com/privacypolicy/ .","title":"Cloudflare"},{"location":"privacy-policy/#3-general-information-and-mandatory-information","text":"","title":"3. General information and mandatory information"},{"location":"privacy-policy/#data-protection","text":"The operators of this website and its pages take the protection of your personal data very seriously. Hence, we handle your personal data as confidential information and in compliance with the statutory data protection regulations and this Data Protection Declaration. Whenever you use this website, a variety of personal information will be collected. Personal data comprises data that can be used to personally identify you. This Data Protection Declaration explains which data we collect as well as the purposes we use this data for. It also explains how, and for which purpose the information is collected. We herewith advise you that the transmission of data via the Internet (i.e. through e-mail communications) may be prone to security gaps. It is not possible to completely protect data against third-party access.","title":"Data protection"},{"location":"privacy-policy/#information-about-the-responsible-party-referred-to-as-the-controller-in-the-gdpr","text":"The data processing controller on this website is: Alexander Trost Postfach 11 75196 K\u00e4mpfelbach Germany Phone: +497232370228 E-mail: galexrt@googlemail.com The controller is the natural person or legal entity that single-handedly or jointly with others makes decisions as to the purposes of and resources for the processing of personal data (e.g. names, e-mail addresses, etc.).","title":"Information about the responsible party (referred to as the \"controller\" in the GDPR)"},{"location":"privacy-policy/#storage-duration","text":"Unless a more specific storage period has been specified in this privacy policy, your personal data will remain with us until the purpose for which it was collected no longer applies. If you assert a justified request for deletion or revoke your consent to data processing, your data will be deleted, unless we have other legally permissible reasons for storing your personal data (e.g. tax or commercial law retention periods); in the latter case, the deletion will take place after these reasons cease to apply.","title":"Storage duration"},{"location":"privacy-policy/#information-on-data-transfer-to-the-usa","text":"Our website uses, in particular, tools from companies based in the USA. When these tools are active, your personal information may be transferred to the US servers of these companies. We must point out that the USA is not a safe third country within the meaning of EU data protection law. US companies are required to release personal data to security authorities without you as the data subject being able to take legal action against this. The possibility cannot therefore be excluded that US authorities (e.g. secret services) may process, evaluate and permanently store your data on US servers for monitoring purposes. We have no influence over these processing activities.","title":"Information on data transfer to the USA"},{"location":"privacy-policy/#revocation-of-your-consent-to-the-processing-of-data","text":"A wide range of data processing transactions are possible only subject to your express consent. You can also revoke at any time any consent you have already given us. This shall be without prejudice to the lawfulness of any data collection that occurred prior to your revocation.","title":"Revocation of your consent to the processing of data"},{"location":"privacy-policy/#right-to-object-to-the-collection-of-data-in-special-cases-right-to-object-to-direct-advertising-art-21-gdpr","text":"IN THE EVENT THAT DATA ARE PROCESSED ON THE BASIS OF ART. 6 SECT. 1 LIT. E OR F GDPR, YOU HAVE THE RIGHT TO AT ANY TIME OBJECT TO THE PROCESSING OF YOUR PERSONAL DATA BASED ON GROUNDS ARISING FROM YOUR UNIQUE SITUATION. THIS ALSO APPLIES TO ANY PROFILING BASED ON THESE PROVISIONS. TO DETERMINE THE LEGAL BASIS, ON WHICH ANY PROCESSING OF DATA IS BASED, PLEASE CONSULT THIS DATA PROTECTION DECLARATION. IF YOU LOG AN OBJECTION, WE WILL NO LONGER PROCESS YOUR AFFECTED PERSONAL DATA, UNLESS WE ARE IN A POSITION TO PRESENT COMPELLING PROTECTION WORTHY GROUNDS FOR THE PROCESSING OF YOUR DATA, THAT OUTWEIGH YOUR INTERESTS, RIGHTS AND FREEDOMS OR IF THE PURPOSE OF THE PROCESSING IS THE CLAIMING, EXERCISING OR DEFENCE OF LEGAL ENTITLEMENTS (OBJECTION PURSUANT TO ART. 21 SECT. 1 GDPR). IF YOUR PERSONAL DATA IS BEING PROCESSED IN ORDER TO ENGAGE IN DIRECT ADVERTISING, YOU HAVE THE RIGHT TO AT ANY TIME OBJECT TO THE PROCESSING OF YOUR AFFECTED PERSONAL DATA FOR THE PURPOSES OF SUCH ADVERTISING. THIS ALSO APPLIES TO PROFILING TO THE EXTENT THAT IT IS AFFILIATED WITH SUCH DIRECT ADVERTISING. IF YOU OBJECT, YOUR PERSONAL DATA WILL SUBSEQUENTLY NO LONGER BE USED FOR DIRECT ADVERTISING PURPOSES (OBJECTION PURSUANT TO ART. 21 SECT. 2 GDPR).","title":"Right to object to the collection of data in special cases; right to object to direct advertising (Art. 21 GDPR)"},{"location":"privacy-policy/#right-to-log-a-complaint-with-the-competent-supervisory-agency","text":"In the event of violations of the GDPR, data subjects are entitled to log a complaint with a supervisory agency, in particular in the member state where they usually maintain their domicile, place of work or at the place where the alleged violation occurred. The right to log a complaint is in effect regardless of any other administrative or court proceedings available as legal recourses.","title":"Right to log a complaint with the competent supervisory agency"},{"location":"privacy-policy/#right-to-data-portability","text":"You have the right to demand that we hand over any data we automatically process on the basis of your consent or in order to fulfil a contract be handed over to you or a third party in a commonly used, machine readable format. If you should demand the direct transfer of the data to another controller, this will be done only if it is technically feasible.","title":"Right to data portability"},{"location":"privacy-policy/#ssl-andor-tls-encryption","text":"For security reasons and to protect the transmission of confidential content, such as purchase orders or inquiries you submit to us as the website operator, this website uses either an SSL or a TLS encryption program. You can recognize an encrypted connection by checking whether the address line of the browser switches from \"http://\" to \"https://\" and also by the appearance of the lock icon in the browser line. If the SSL or TLS encryption is activated, data you transmit to us cannot be read by third parties.","title":"SSL and/or TLS encryption"},{"location":"privacy-policy/#information-about-rectification-and-eradication-of-data","text":"Within the scope of the applicable statutory provisions, you have the right to at any time demand information about your archived personal data, their source and recipients as well as the purpose of the processing of your data. You may also have a right to have your data rectified or eradicated. If you have questions about this subject matter or any other questions about personal data, please do not hesitate to contact us at any time at the address provided in section \"Information Required by Law.\"","title":"Information about, rectification and eradication of data"},{"location":"privacy-policy/#right-to-demand-processing-restrictions","text":"You have the right to demand the imposition of restrictions as far as the processing of your personal data is concerned. To do so, you may contact us at any time at the address provided in section \"Information Required by Law.\" The right to demand restriction of processing applies in the following cases: In the event that you should dispute the correctness of your data archived by us, we will usually need some time to verify this claim. During the time that this investigation is ongoing, you have the right to demand that we restrict the processing of your personal data. If the processing of your personal data was/is conducted in an unlawful manner, you have the option to demand the restriction of the processing of your data in lieu of demanding the eradication of this data. If we do not need your personal data any longer and you need it to exercise, defend or claim legal entitlements, you have the right to demand the restriction of the processing of your personal data instead of its eradication. If you have raised an objection pursuant to Art. 21 Sect. 1 GDPR, your rights and our rights will have to be weighed against each other. As long as it has not been determined whose interests prevail, you have the right to demand a restriction of the processing of your personal data. If you have restricted the processing of your personal data, these data \u2013 with the exception of their archiving \u2013 may be processed only subject to your consent or to claim, exercise or defend legal entitlements or to protect the rights of other natural persons or legal entities or for important public interest reasons cited by the European Union or a member state of the EU.","title":"Right to demand processing restrictions"},{"location":"privacy-policy/#4-recording-of-data-on-this-website","text":"","title":"4. Recording of data on this website"},{"location":"privacy-policy/#cookies","text":"Our websites and pages use what the industry refers to as \"cookies.\" Cookies are small text files that do not cause any damage to your device. They are either stored temporarily for the duration of a session (session cookies) or they are permanently archived on your device (permanent cookies). Session cookies are automatically deleted once you terminate your visit. Permanent cookies remain archived on your device until you actively delete them or they are automatically eradicated by your web browser. In some cases, it is possible that third-party cookies are stored on your device once you enter our site (third-party cookies). These cookies enable you or us to take advantage of certain services offered by the third party (e.g. cookies for the processing of payment services). Cookies have a variety of functions. Many cookies are technically essential since certain website functions would not work in the absence of the cookies (e.g. the shopping cart function or the display of videos). The purpose of other cookies may be the analysis of user patterns or the display of promotional messages. Cookies, which are required for the performance of electronic communication transactions (required cookies) or for the provision of certain functions you want to use (functional cookies, e.g. for the shopping cart function) or those that are necessary for the optimization of the website (e.g. cookies that provide measurable insights into the web audience), shall be stored on the basis of Art. 6 Sect. 1 lit. f GDPR, unless a different legal basis is cited. The operator of the website has a legitimate interest in the storage of cookies to ensure the technically error free and optimized provision of the operator\u2019s services. If your consent to the storage of the cookies has been requested, the respective cookies are stored exclusively on the basis of the consent obtained (Art. 6 Sect. 1 lit. a GDPR); this consent may be revoked at any time. You have the option to set up your browser in such a manner that you will be notified any time cookies are placed and to permit the acceptance of cookies only in specific cases. You may also exclude the acceptance of cookies in certain cases or in general or activate the delete function for the automatic eradication of cookies when the browser closes. If cookies are deactivated, the functions of this website may be limited. In the event that third-party cookies are used or if cookies are used for analytical purposes, we will separately notify you in conjunction with this Data Protection Policy and, if applicable, ask for your consent.","title":"Cookies"},{"location":"privacy-policy/#server-log-files","text":"The provider of this website and its pages automatically collects and stores information in so-called server log files, which your browser communicates to us automatically. The information comprises: The type and version of browser used The used operating system Referrer URL The hostname of the accessing computer The time of the server inquiry The IP address This data is not merged with other data sources. This data is recorded on the basis of Art. 6 Sect. 1 lit. f GDPR. The operator of the website has a legitimate interest in the technically error free depiction and the optimization of the operator\u2019s website. In order to achieve this, server log files must be recorded.","title":"Server log files"},{"location":"privacy-policy/#contact-form","text":"If you submit inquiries to us via our contact form, the information provided in the contact form as well as any contact information provided therein will be stored by us in order to handle your inquiry and in the event that we have further questions. We will not share this information without your consent. The processing of these data is based on Art. 6 para. 1 lit. b GDPR, if your request is related to the execution of a contract or if it is necessary to carry out pre-contractual measures. In all other cases the processing is based on our legitimate interest in the effective processing of the requests addressed to us (Art. 6 Para. 1 lit. f GDPR) or on your agreement (Art. 6 Para. 1 lit. a GDPR) if this has been requested. The information you have entered into the contact form shall remain with us until you ask us to eradicate the data, revoke your consent to the archiving of data or if the purpose for which the information is being archived no longer exists (e.g. after we have concluded our response to your inquiry). This shall be without prejudice to any mandatory legal provisions \u2013 in particular retention periods.","title":"Contact form"},{"location":"privacy-policy/#request-by-e-mail-telephone-or-fax","text":"If you contact us by e-mail, telephone or fax, your request, including all resulting personal data (name, request) will be stored and processed by us for the purpose of processing your request. We do not pass these data on without your consent. These data are processed on the basis of Art. 6 Sect. 1 lit. b GDPR if your inquiry is related to the fulfillment of a contract or is required for the performance of pre-contractual measures. In all other cases, the data are processed on the basis of our legitimate interest in the effective handling of inquiries submitted to us (Art. 6 Sect. 1 lit. f GDPR) or on the basis of your consent (Art. 6 Sect. 1 lit. a GDPR) if it has been obtained. The data sent by you to us via contact requests remain with us until you request us to delete, revoke your consent to the storage or the purpose for the data storage lapses (e.g. after completion of your request). Mandatory statutory provisions - in particular statutory retention periods - remain unaffected.","title":"Request by e-mail, telephone or fax"},{"location":"privacy-policy/#datenschutzerklarung","text":"","title":"Datenschutz\u00aderkl\u00e4rung"},{"location":"privacy-policy/#1-datenschutz-auf-einen-blick","text":"","title":"1. Datenschutz auf einen Blick"},{"location":"privacy-policy/#allgemeine-hinweise","text":"Die folgenden Hinweise geben einen einfachen \u00dcberblick dar\u00fcber, was mit Ihren personenbezogenen Daten passiert, wenn Sie diese Website besuchen. Personenbezogene Daten sind alle Daten, mit denen Sie pers\u00f6nlich identifiziert werden k\u00f6nnen. Ausf\u00fchrliche Informationen zum Thema Datenschutz entnehmen Sie unserer unter diesem Text aufgef\u00fchrten Datenschutzerkl\u00e4rung.","title":"Allgemeine Hinweise"},{"location":"privacy-policy/#datenerfassung-auf-dieser-website","text":"","title":"Datenerfassung auf dieser Website"},{"location":"privacy-policy/#wer-ist-verantwortlich-fur-die-datenerfassung-auf-dieser-website","text":"Die Datenverarbeitung auf dieser Website erfolgt durch den Websitebetreiber. Dessen Kontaktdaten k\u00f6nnen Sie dem Impressum dieser Website entnehmen.","title":"Wer ist verantwortlich f\u00fcr die Datenerfassung auf dieser Website?"},{"location":"privacy-policy/#wie-erfassen-wir-ihre-daten","text":"Ihre Daten werden zum einen dadurch erhoben, dass Sie uns diese mitteilen. Hierbei kann es sich z. B. um Daten handeln, die Sie in ein Kontaktformular eingeben. Andere Daten werden automatisch oder nach Ihrer Einwilligung beim Besuch der Website durch unsere IT-Systeme erfasst. Das sind vor allem technische Daten (z. B. Internetbrowser, Betriebssystem oder Uhrzeit des Seitenaufrufs). Die Erfassung dieser Daten erfolgt automatisch, sobald Sie diese Website betreten.","title":"Wie erfassen wir Ihre Daten?"},{"location":"privacy-policy/#wofur-nutzen-wir-ihre-daten","text":"Ein Teil der Daten wird erhoben, um eine fehlerfreie Bereitstellung der Website zu gew\u00e4hrleisten. Andere Daten k\u00f6nnen zur Analyse Ihres Nutzerverhaltens verwendet werden.","title":"Wof\u00fcr nutzen wir Ihre Daten?"},{"location":"privacy-policy/#welche-rechte-haben-sie-bezuglich-ihrer-daten","text":"Sie haben jederzeit das Recht, unentgeltlich Auskunft \u00fcber Herkunft, Empf\u00e4nger und Zweck Ihrer gespeicherten personenbezogenen Daten zu erhalten. Sie haben au\u00dferdem ein Recht, die Berichtigung oder L\u00f6schung dieser Daten zu verlangen. Wenn Sie eine Einwilligung zur Datenverarbeitung erteilt haben, k\u00f6nnen Sie diese Einwilligung jederzeit f\u00fcr die Zukunft widerrufen. Au\u00dferdem haben Sie das Recht, unter bestimmten Umst\u00e4nden die Einschr\u00e4nkung der Verarbeitung Ihrer personenbezogenen Daten zu verlangen. Des Weiteren steht Ihnen ein Beschwerderecht bei der zust\u00e4ndigen Aufsichtsbeh\u00f6rde zu. Hierzu sowie zu weiteren Fragen zum Thema Datenschutz k\u00f6nnen Sie sich jederzeit unter der im Impressum angegebenen Adresse an uns wenden.","title":"Welche Rechte haben Sie bez\u00fcglich Ihrer Daten?"},{"location":"privacy-policy/#analyse-tools-und-tools-von-drittanbietern","text":"Beim Besuch dieser Website kann Ihr Surf-Verhalten statistisch ausgewertet werden. Das geschieht vor allem mit sogenannten Analyseprogrammen. Detaillierte Informationen zu diesen Analyseprogrammen finden Sie in der folgenden Datenschutzerkl\u00e4rung.","title":"Analyse-Tools und Tools von Dritt\u00adanbietern"},{"location":"privacy-policy/#2-hosting-und-content-delivery-networks-cdn","text":"","title":"2. Hosting und Content Delivery Networks (CDN)"},{"location":"privacy-policy/#externes-hosting","text":"Diese Website wird bei einem externen Dienstleister gehostet (Hoster). Die personenbezogenen Daten, die auf dieser Website erfasst werden, werden auf den Servern des Hosters gespeichert. Hierbei kann es sich v. a. um IP-Adressen, Kontaktanfragen, Meta- und Kommunikationsdaten, Vertragsdaten, Kontaktdaten, Namen, Websitezugriffe und sonstige Daten, die \u00fcber eine Website generiert werden, handeln. Der Einsatz des Hosters erfolgt zum Zwecke der Vertragserf\u00fcllung gegen\u00fcber unseren potenziellen und bestehenden Kunden (Art. 6 Abs. 1 lit. b DSGVO) und im Interesse einer sicheren, schnellen und effizienten Bereitstellung unseres Online-Angebots durch einen professionellen Anbieter (Art. 6 Abs. 1 lit. f DSGVO). Unser Hoster wird Ihre Daten nur insoweit verarbeiten, wie dies zur Erf\u00fcllung seiner Leistungspflichten erforderlich ist und unsere Weisungen in Bezug auf diese Daten befolgen. Wir setzen folgenden Hoster ein: Cloudflare Inc. 101 Townsend St, San Francisco, CA 94107 USA GitHub Inc. 88 Colin P Kelly Jr St, San Francisco, CA 94107 United States Hetzner Online GmbH Industriestr. 25 91710 Gunzenhausen Germany","title":"Externes Hosting"},{"location":"privacy-policy/#abschluss-eines-vertrages-uber-auftragsverarbeitung","text":"Um die datenschutzkonforme Verarbeitung zu gew\u00e4hrleisten, haben wir einen Vertrag \u00fcber Auftragsverarbeitung mit unserem Hoster geschlossen.","title":"Abschluss eines Vertrages \u00fcber Auftragsverarbeitung"},{"location":"privacy-policy/#cloudflare_1","text":"Wir nutzen den Service \u201eCloudflare\". Anbieter ist die Cloudflare Inc., 101 Townsend St., San Francisco, CA 94107, USA (im Folgenden \u201eCloudflare\"). Cloudflare bietet ein weltweit verteiltes Content Delivery Network mit DNS an. Dabei wird technisch der Informationstransfer zwischen Ihrem Browser und unserer Website \u00fcber das Netzwerk von Cloudflare geleitet. Das versetzt Cloudflare in die Lage, den Datenverkehr zwischen Ihrem Browser und unserer Website zu analysieren und als Filter zwischen unseren Servern und potenziell b\u00f6sartigem Datenverkehr aus dem Internet zu dienen. Hierbei kann Cloudflare auch Cookies oder sonstige Technologien zur Wiedererkennung von Internetnutzern einsetzen, die jedoch allein zum hier beschriebenen Zweck verwendet werden. Der Einsatz von Cloudflare beruht auf unserem berechtigten Interesse an einer m\u00f6glichst fehlerfreien und sicheren Bereitstellung unseres Webangebotes (Art. 6 Abs. 1 lit. f DSGVO). Weitere Informationen zum Thema Sicherheit und Datenschutz bei Cloudflare finden Sie hier: https://www.cloudflare.com/privacypolicy/ .","title":"Cloudflare"},{"location":"privacy-policy/#3-allgemeine-hinweise-und-pflichtinformationen","text":"","title":"3. Allgemeine Hinweise und Pflicht\u00adinformationen"},{"location":"privacy-policy/#datenschutz","text":"Die Betreiber dieser Seiten nehmen den Schutz Ihrer pers\u00f6nlichen Daten sehr ernst. Wir behandeln Ihre personenbezogenen Daten vertraulich und entsprechend der gesetzlichen Datenschutzvorschriften sowie dieser Datenschutzerkl\u00e4rung. Wenn Sie diese Website benutzen, werden verschiedene personenbezogene Daten erhoben. Personenbezogene Daten sind Daten, mit denen Sie pers\u00f6nlich identifiziert werden k\u00f6nnen. Die vorliegende Datenschutzerkl\u00e4rung erl\u00e4utert, welche Daten wir erheben und wof\u00fcr wir sie nutzen. Sie erl\u00e4utert auch, wie und zu welchem Zweck das geschieht. Wir weisen darauf hin, dass die Daten\u00fcbertragung im Internet (z. B. bei der Kommunikation per E-Mail) Sicherheitsl\u00fccken aufweisen kann. Ein l\u00fcckenloser Schutz der Daten vor dem Zugriff durch Dritte ist nicht m\u00f6glich.","title":"Datenschutz"},{"location":"privacy-policy/#hinweis-zur-verantwortlichen-stelle","text":"Die verantwortliche Stelle f\u00fcr die Datenverarbeitung auf dieser Website ist: Alexander Trost Kelterstr. 4/1 75196 Remchingen Germany Telefon: +497232370228 E-Mail: galexrt@googlemail.com Verantwortliche Stelle ist die nat\u00fcrliche oder juristische Person, die allein oder gemeinsam mit anderen \u00fcber die Zwecke und Mittel der Verarbeitung von personenbezogenen Daten (z. B. Namen, E-Mail-Adressen o. \u00c4.) entscheidet.","title":"Hinweis zur verantwortlichen Stelle"},{"location":"privacy-policy/#speicherdauer","text":"Soweit innerhalb dieser Datenschutzerkl\u00e4rung keine speziellere Speicherdauer genannt wurde, verbleiben Ihre personenbezogenen Daten bei uns, bis der Zweck f\u00fcr die Datenverarbeitung entf\u00e4llt. Wenn Sie ein berechtigtes L\u00f6schersuchen geltend machen oder eine Einwilligung zur Datenverarbeitung widerrufen, werden Ihre Daten gel\u00f6scht, sofern wir keinen anderen rechtlich zul\u00e4ssigen Gr\u00fcnde f\u00fcr die Speicherung Ihrer personenbezogenen Daten haben (z.B. steuer- oder handelsrechtliche Aufbewahrungsfristen); im letztgenannten Fall erfolgt die L\u00f6schung nach Fortfall dieser Gr\u00fcnde.","title":"Speicherdauer"},{"location":"privacy-policy/#hinweis-zur-datenweitergabe-in-die-usa","text":"Auf unserer Website sind unter anderem Tools von Unternehmen mit Sitz in den USA eingebunden. Wenn diese Tools aktiv sind, k\u00f6nnen Ihre personenbezogenen Daten an die US-Server der jeweiligen Unternehmen weitergegeben werden. Wir weisen darauf hin, dass die USA kein sicherer Drittstaat im Sinne des EU-Datenschutzrechts sind. US-Unternehmen sind dazu verpflichtet, personenbezogene Daten an Sicherheitsbeh\u00f6rden herauszugeben, ohne dass Sie als Betroffener hiergegen gerichtlich vorgehen k\u00f6nnten. Es kann daher nicht ausgeschlossen werden, dass US-Beh\u00f6rden (z.B. Geheimdienste) Ihre auf US-Servern befindlichen Daten zu \u00dcberwachungszwecken verarbeiten, auswerten und dauerhaft speichern. Wir haben auf diese Verarbeitungst\u00e4tigkeiten keinen Einfluss.","title":"Hinweis zur Datenweitergabe in die USA"},{"location":"privacy-policy/#widerruf-ihrer-einwilligung-zur-datenverarbeitung","text":"Viele Datenverarbeitungsvorg\u00e4nge sind nur mit Ihrer ausdr\u00fccklichen Einwilligung m\u00f6glich. Sie k\u00f6nnen eine bereits erteilte Einwilligung jederzeit widerrufen. Die Rechtm\u00e4\u00dfigkeit der bis zum Widerruf erfolgten Datenverarbeitung bleibt vom Widerruf unber\u00fchrt.","title":"Widerruf Ihrer Einwilligung zur Datenverarbeitung"},{"location":"privacy-policy/#widerspruchsrecht-gegen-die-datenerhebung-in-besonderen-fallen-sowie-gegen-direktwerbung-art-21-dsgvo","text":"WENN DIE DATENVERARBEITUNG AUF GRUNDLAGE VON ART. 6 ABS. 1 LIT. E ODER F DSGVO ERFOLGT, HABEN SIE JEDERZEIT DAS RECHT, AUS GR\u00dcNDEN, DIE SICH AUS IHRER BESONDEREN SITUATION ERGEBEN, GEGEN DIE VERARBEITUNG IHRER PERSONENBEZOGENEN DATEN WIDERSPRUCH EINZULEGEN; DIES GILT AUCH F\u00dcR EIN AUF DIESE BESTIMMUNGEN GEST\u00dcTZTES PROFILING. DIE JEWEILIGE RECHTSGRUNDLAGE, AUF DENEN EINE VERARBEITUNG BERUHT, ENTNEHMEN SIE DIESER DATENSCHUTZERKL\u00c4RUNG. WENN SIE WIDERSPRUCH EINLEGEN, WERDEN WIR IHRE BETROFFENEN PERSONENBEZOGENEN DATEN NICHT MEHR VERARBEITEN, ES SEI DENN, WIR K\u00d6NNEN ZWINGENDE SCHUTZW\u00dcRDIGE GR\u00dcNDE F\u00dcR DIE VERARBEITUNG NACHWEISEN, DIE IHRE INTERESSEN, RECHTE UND FREIHEITEN \u00dcBERWIEGEN ODER DIE VERARBEITUNG DIENT DER GELTENDMACHUNG, AUS\u00dcBUNG ODER VERTEIDIGUNG VON RECHTSANSPR\u00dcCHEN (WIDERSPRUCH NACH ART. 21 ABS. 1 DSGVO). WERDEN IHRE PERSONENBEZOGENEN DATEN VERARBEITET, UM DIREKTWERBUNG ZU BETREIBEN, SO HABEN SIE DAS RECHT, JEDERZEIT WIDERSPRUCH GEGEN DIE VERARBEITUNG SIE BETREFFENDER PERSONENBEZOGENER DATEN ZUM ZWECKE DERARTIGER WERBUNG EINZULEGEN; DIES GILT AUCH F\u00dcR DAS PROFILING, SOWEIT ES MIT SOLCHER DIREKTWERBUNG IN VERBINDUNG STEHT. WENN SIE WIDERSPRECHEN, WERDEN IHRE PERSONENBEZOGENEN DATEN ANSCHLIESSEND NICHT MEHR ZUM ZWECKE DER DIREKTWERBUNG VERWENDET (WIDERSPRUCH NACH ART. 21 ABS. 2 DSGVO).","title":"Widerspruchsrecht gegen die Datenerhebung in besonderen F\u00e4llen sowie gegen Direktwerbung (Art. 21 DSGVO)"},{"location":"privacy-policy/#beschwerderecht-bei-der-zustandigen-aufsichtsbehorde","text":"Im Falle von Verst\u00f6\u00dfen gegen die DSGVO steht den Betroffenen ein Beschwerderecht bei einer Aufsichtsbeh\u00f6rde, insbesondere in dem Mitgliedstaat ihres gew\u00f6hnlichen Aufenthalts, ihres Arbeitsplatzes oder des Orts des mutma\u00dflichen Versto\u00dfes zu. Das Beschwerderecht besteht unbeschadet anderweitiger verwaltungsrechtlicher oder gerichtlicher Rechtsbehelfe.","title":"Beschwerde\u00adrecht bei der zust\u00e4ndigen Aufsichts\u00adbeh\u00f6rde"},{"location":"privacy-policy/#recht-auf-datenubertragbarkeit","text":"Sie haben das Recht, Daten, die wir auf Grundlage Ihrer Einwilligung oder in Erf\u00fcllung eines Vertrags automatisiert verarbeiten, an sich oder an einen Dritten in einem g\u00e4ngigen, maschinenlesbaren Format aush\u00e4ndigen zu lassen. Sofern Sie die direkte \u00dcbertragung der Daten an einen anderen Verantwortlichen verlangen, erfolgt dies nur, soweit es technisch machbar ist.","title":"Recht auf Daten\u00ad\u00fcbertrag\u00adbarkeit"},{"location":"privacy-policy/#ssl-bzw-tls-verschlusselung","text":"Diese Seite nutzt aus Sicherheitsgr\u00fcnden und zum Schutz der \u00dcbertragung vertraulicher Inhalte, wie zum Beispiel Bestellungen oder Anfragen, die Sie an uns als Seitenbetreiber senden, eine SSL- bzw. TLS-Verschl\u00fcsselung. Eine verschl\u00fcsselte Verbindung erkennen Sie daran, dass die Adresszeile des Browsers von \u201ehttp://\" auf \u201ehttps://\" wechselt und an dem Schloss-Symbol in Ihrer Browserzeile. Wenn die SSL- bzw. TLS-Verschl\u00fcsselung aktiviert ist, k\u00f6nnen die Daten, die Sie an uns \u00fcbermitteln, nicht von Dritten mitgelesen werden.","title":"SSL- bzw. TLS-Verschl\u00fcsselung"},{"location":"privacy-policy/#auskunft-loschung-und-berichtigung","text":"Sie haben im Rahmen der geltenden gesetzlichen Bestimmungen jederzeit das Recht auf unentgeltliche Auskunft \u00fcber Ihre gespeicherten personenbezogenen Daten, deren Herkunft und Empf\u00e4nger und den Zweck der Datenverarbeitung und ggf. ein Recht auf Berichtigung oder L\u00f6schung dieser Daten. Hierzu sowie zu weiteren Fragen zum Thema personenbezogene Daten k\u00f6nnen Sie sich jederzeit unter der im Impressum angegebenen Adresse an uns wenden.","title":"Auskunft, L\u00f6schung und Berichtigung"},{"location":"privacy-policy/#recht-auf-einschrankung-der-verarbeitung","text":"Sie haben das Recht, die Einschr\u00e4nkung der Verarbeitung Ihrer personenbezogenen Daten zu verlangen. Hierzu k\u00f6nnen Sie sich jederzeit unter der im Impressum angegebenen Adresse an uns wenden. Das Recht auf Einschr\u00e4nkung der Verarbeitung besteht in folgenden F\u00e4llen: Wenn Sie die Richtigkeit Ihrer bei uns gespeicherten personenbezogenen Daten bestreiten, ben\u00f6tigen wir in der Regel Zeit, um dies zu \u00fcberpr\u00fcfen. F\u00fcr die Dauer der Pr\u00fcfung haben Sie das Recht, die Einschr\u00e4nkung der Verarbeitung Ihrer personenbezogenen Daten zu verlangen. Wenn die Verarbeitung Ihrer personenbezogenen Daten unrechtm\u00e4\u00dfig geschah/geschieht, k\u00f6nnen Sie statt der L\u00f6schung die Einschr\u00e4nkung der Datenverarbeitung verlangen. Wenn wir Ihre personenbezogenen Daten nicht mehr ben\u00f6tigen, Sie sie jedoch zur Aus\u00fcbung, Verteidigung oder Geltendmachung von Rechtsanspr\u00fcchen ben\u00f6tigen, haben Sie das Recht, statt der L\u00f6schung die Einschr\u00e4nkung der Verarbeitung Ihrer personenbezogenen Daten zu verlangen. Wenn Sie einen Widerspruch nach Art. 21 Abs. 1 DSGVO eingelegt haben, muss eine Abw\u00e4gung zwischen Ihren und unseren Interessen vorgenommen werden. Solange noch nicht feststeht, wessen Interessen \u00fcberwiegen, haben Sie das Recht, die Einschr\u00e4nkung der Verarbeitung Ihrer personenbezogenen Daten zu verlangen. Wenn Sie die Verarbeitung Ihrer personenbezogenen Daten eingeschr\u00e4nkt haben, d\u00fcrfen diese Daten \u2013 von ihrer Speicherung abgesehen \u2013 nur mit Ihrer Einwilligung oder zur Geltendmachung, Aus\u00fcbung oder Verteidigung von Rechtsanspr\u00fcchen oder zum Schutz der Rechte einer anderen nat\u00fcrlichen oder juristischen Person oder aus Gr\u00fcnden eines wichtigen \u00f6ffentlichen Interesses der Europ\u00e4ischen Union oder eines Mitgliedstaats verarbeitet werden.","title":"Recht auf Einschr\u00e4nkung der Verarbeitung"},{"location":"privacy-policy/#4-datenerfassung-auf-dieser-website","text":"","title":"4. Datenerfassung auf dieser Website"},{"location":"privacy-policy/#cookies_1","text":"Unsere Internetseiten verwenden so genannte \u201eCookies\". Cookies sind kleine Textdateien und richten auf Ihrem Endger\u00e4t keinen Schaden an. Sie werden entweder vor\u00fcbergehend f\u00fcr die Dauer einer Sitzung (Session-Cookies) oder dauerhaft (permanente Cookies) auf Ihrem Endger\u00e4t gespeichert. Session-Cookies werden nach Ende Ihres Besuchs automatisch gel\u00f6scht. Permanente Cookies bleiben auf Ihrem Endger\u00e4t gespeichert, bis Sie diese selbst l\u00f6schen oder eine automatische L\u00f6schung durch Ihren Webbrowser erfolgt. Teilweise k\u00f6nnen auch Cookies von Drittunternehmen auf Ihrem Endger\u00e4t gespeichert werden, wenn Sie unsere Seite betreten (Third-Party-Cookies). Diese erm\u00f6glichen uns oder Ihnen die Nutzung bestimmter Dienstleistungen des Drittunternehmens (z.B. Cookies zur Abwicklung von Zahlungsdienstleistungen). Cookies haben verschiedene Funktionen. Zahlreiche Cookies sind technisch notwendig, da bestimmte Websitefunktionen ohne diese nicht funktionieren w\u00fcrden (z.B. die Warenkorbfunktion oder die Anzeige von Videos). Andere Cookies dienen dazu, das Nutzerverhalten auszuwerten oder Werbung anzuzeigen. Cookies, die zur Durchf\u00fchrung des elektronischen Kommunikationsvorgangs (notwendige Cookies) oder zur Bereitstellung bestimmter, von Ihnen erw\u00fcnschter Funktionen (funktionale Cookies, z. B. f\u00fcr die Warenkorbfunktion) oder zur Optimierung der Website (z.B. Cookies zur Messung des Webpublikums) erforderlich sind, werden auf Grundlage von Art. 6 Abs. 1 lit. f DSGVO gespeichert, sofern keine andere Rechtsgrundlage angegeben wird. Der Websitebetreiber hat ein berechtigtes Interesse an der Speicherung von Cookies zur technisch fehlerfreien und optimierten Bereitstellung seiner Dienste. Sofern eine Einwilligung zur Speicherung von Cookies abgefragt wurde, erfolgt die Speicherung der betreffenden Cookies ausschlie\u00dflich auf Grundlage dieser Einwilligung (Art. 6 Abs. 1 lit. a DSGVO); die Einwilligung ist jederzeit widerrufbar. Sie k\u00f6nnen Ihren Browser so einstellen, dass Sie \u00fcber das Setzen von Cookies informiert werden und Cookies nur im Einzelfall erlauben, die Annahme von Cookies f\u00fcr bestimmte F\u00e4lle oder generell ausschlie\u00dfen sowie das automatische L\u00f6schen der Cookies beim Schlie\u00dfen des Browsers aktivieren. Bei der Deaktivierung von Cookies kann die Funktionalit\u00e4t dieser Website eingeschr\u00e4nkt sein. Soweit Cookies von Drittunternehmen oder zu Analysezwecken eingesetzt werden, werden wir Sie hier\u00fcber im Rahmen dieser Datenschutzerkl\u00e4rung gesondert informieren und ggf. eine Einwilligung abfragen.","title":"Cookies"},{"location":"privacy-policy/#server-log-dateien","text":"Der Provider der Seiten erhebt und speichert automatisch Informationen in so genannten Server-Log-Dateien, die Ihr Browser automatisch an uns \u00fcbermittelt. Dies sind: Browsertyp und Browserversion verwendetes Betriebssystem Referrer URL Hostname des zugreifenden Rechners Uhrzeit der Serveranfrage IP-Adresse Eine Zusammenf\u00fchrung dieser Daten mit anderen Datenquellen wird nicht vorgenommen. Die Erfassung dieser Daten erfolgt auf Grundlage von Art. 6 Abs. 1 lit. f DSGVO. Der Websitebetreiber hat ein berechtigtes Interesse an der technisch fehlerfreien Darstellung und der Optimierung seiner Website \u2013 hierzu m\u00fcssen die Server-Log-Files erfasst werden.","title":"Server-Log-Dateien"},{"location":"privacy-policy/#kontaktformular","text":"Wenn Sie uns per Kontaktformular Anfragen zukommen lassen, werden Ihre Angaben aus dem Anfrageformular inklusive der von Ihnen dort angegebenen Kontaktdaten zwecks Bearbeitung der Anfrage und f\u00fcr den Fall von Anschlussfragen bei uns gespeichert. Diese Daten geben wir nicht ohne Ihre Einwilligung weiter. Die Verarbeitung dieser Daten erfolgt auf Grundlage von Art. 6 Abs. 1 lit. b DSGVO, sofern Ihre Anfrage mit der Erf\u00fcllung eines Vertrags zusammenh\u00e4ngt oder zur Durchf\u00fchrung vorvertraglicher Ma\u00dfnahmen erforderlich ist. In allen \u00fcbrigen F\u00e4llen beruht die Verarbeitung auf unserem berechtigten Interesse an der effektiven Bearbeitung der an uns gerichteten Anfragen (Art. 6 Abs. 1 lit. f DSGVO) oder auf Ihrer Einwilligung (Art. 6 Abs. 1 lit. a DSGVO) sofern diese abgefragt wurde. Die von Ihnen im Kontaktformular eingegebenen Daten verbleiben bei uns, bis Sie uns zur L\u00f6schung auffordern, Ihre Einwilligung zur Speicherung widerrufen oder der Zweck f\u00fcr die Datenspeicherung entf\u00e4llt (z. B. nach abgeschlossener Bearbeitung Ihrer Anfrage). Zwingende gesetzliche Bestimmungen \u2013 insbesondere Aufbewahrungsfristen \u2013 bleiben unber\u00fchrt.","title":"Kontaktformular"},{"location":"privacy-policy/#anfrage-per-e-mail-telefon-oder-telefax","text":"Wenn Sie uns per E-Mail, Telefon oder Telefax kontaktieren, wird Ihre Anfrage inklusive aller daraus hervorgehenden personenbezogenen Daten (Name, Anfrage) zum Zwecke der Bearbeitung Ihres Anliegens bei uns gespeichert und verarbeitet. Diese Daten geben wir nicht ohne Ihre Einwilligung weiter. Die Verarbeitung dieser Daten erfolgt auf Grundlage von Art. 6 Abs. 1 lit. b DSGVO, sofern Ihre Anfrage mit der Erf\u00fcllung eines Vertrags zusammenh\u00e4ngt oder zur Durchf\u00fchrung vorvertraglicher Ma\u00dfnahmen erforderlich ist. In allen \u00fcbrigen F\u00e4llen beruht die Verarbeitung auf unserem berechtigten Interesse an der effektiven Bearbeitung der an uns gerichteten Anfragen (Art. 6 Abs. 1 lit. f DSGVO) oder auf Ihrer Einwilligung (Art. 6 Abs. 1 lit. a DSGVO) sofern diese abgefragt wurde. Die von Ihnen an uns per Kontaktanfragen \u00fcbersandten Daten verbleiben bei uns, bis Sie uns zur L\u00f6schung auffordern, Ihre Einwilligung zur Speicherung widerrufen oder der Zweck f\u00fcr die Datenspeicherung entf\u00e4llt (z. B. nach abgeschlossener Bearbeitung Ihres Anliegens). Zwingende gesetzliche Bestimmungen \u2013 insbesondere gesetzliche Aufbewahrungsfristen \u2013 bleiben unber\u00fchrt.","title":"Anfrage per E-Mail, Telefon oder Telefax"},{"location":"site-notice/","text":"The Privacy Policy / Datenschutz\u00aderkl\u00e4rung can be found here: Privacy Policy / Datenschutz\u00aderkl\u00e4rung page . German version of the Site Notice / Impressum below. Site Notice \u00b6 Information pursuant to Sect. 5 German Telemedia Act (TMG) \u00b6 Alexander Trost Postfach 11 75196 K\u00e4mpfelbach Germany Contact \u00b6 Phone: +497232370228 E-mail: galexrt@googlemail.com Responsible for the content according to Sect. 55, paragraph 2 RStV \u00b6 Alexander Trost, Kelterstr. 4/1, 75196 Remchingen, Germany Liability for Contents \u00b6 As service providers, we are liable for own contents of these websites according to Paragraph 7, Sect. 1 German Telemedia Act (TMG). However, according to Paragraphs 8 to 10 German Telemedia Act (TMG), service providers are not obligated to permanently monitor submitted or stored information or to search for evidences that indicate illegal activities. Legal obligations to removing information or to blocking the use of information remain unchallenged. In this case, liability is only possible at the time of knowledge about a specific violation of law. Illegal contents will be removed immediately at the time we get knowledge of them. Liability for Links \u00b6 Our offer includes links to external third-party websites. We have no influence on the contents of those websites, therefore we cannot guarantee for those contents. Providers or administrators of linked websites are always responsible for their own contents. The linked websites had been checked for possible violations of law at the time of the establishment of the link. Illegal contents were not detected at the time of the linking. A permanent monitoring of the contents of linked websites cannot be imposed without reasonable indications that there has been a violation of law. Illegal links will be removed immediately at the time we get knowledge of them. Copyright \u00b6 Contents and compilations published on these websites by the providers are subject to Creative Commons Attribution 4.0 International License . This work is licensed under a Creative Commons Attribution 4.0 International License . Impressum \u00b6 English version below Site Notice section . Angaben gem\u00e4\u00df \u00a7 5 TMG \u00b6 Alexander Trost Kelterstr. 4/1 75196 Remchingen Germany Kontakt \u00b6 Telefon: +497232370228 E-Mail: galexrt@googlemail.com Verantwortlich f\u00fcr den Inhalt nach \u00a7 55 Abs. 2 RStV \u00b6 Alexander Trost, Kelterstr. 4/1, 75196 Remchingen, Germany Haftung f\u00fcr Inhalte \u00b6 Als Diensteanbieter sind wir gem\u00e4\u00df \u00a7 7 Abs.1 TMG f\u00fcr eigene Inhalte auf diesen Seiten nach den allgemeinen Gesetzen verantwortlich. Nach \u00a7\u00a7 8 bis 10 TMG sind wir als Diensteanbieter jedoch nicht verpflichtet, \u00fcbermittelte oder gespeicherte fremde Informationen zu \u00fcberwachen oder nach Umst\u00e4nden zu forschen, die auf eine rechtswidrige T\u00e4tigkeit hinweisen. Verpflichtungen zur Entfernung oder Sperrung der Nutzung von Informationen nach den allgemeinen Gesetzen bleiben hiervon unber\u00fchrt. Eine diesbez\u00fcgliche Haftung ist jedoch erst ab dem Zeitpunkt der Kenntnis einer konkreten Rechtsverletzung m\u00f6glich. Bei Bekanntwerden von entsprechenden Rechtsverletzungen werden wir diese Inhalte umgehend entfernen. Haftung f\u00fcr Links \u00b6 Unser Angebot enth\u00e4lt Links zu externen Websites Dritter, auf deren Inhalte wir keinen Einfluss haben. Deshalb k\u00f6nnen wir f\u00fcr diese fremden Inhalte auch keine Gew\u00e4hr \u00fcbernehmen. F\u00fcr die Inhalte der verlinkten Seiten ist stets der jeweilige Anbieter oder Betreiber der Seiten verantwortlich. Die verlinkten Seiten wurden zum Zeitpunkt der Verlinkung auf m\u00f6gliche Rechtsverst\u00f6\u00dfe \u00fcberpr\u00fcft. Rechtswidrige Inhalte waren zum Zeitpunkt der Verlinkung nicht erkennbar. Eine permanente inhaltliche Kontrolle der verlinkten Seiten ist jedoch ohne konkrete Anhaltspunkte einer Rechtsverletzung nicht zumutbar. Bei Bekanntwerden von Rechtsverletzungen werden wir derartige Links umgehend entfernen. Urheberrecht \u00b6 Die durch die Seitenbetreiber erstellten Inhalte und Werke auf diesen Seiten unterliegen der Creative Commons Attribution 4.0 International License . Diese Arbeiten sind lizenziert unter der Creative Commons Attribution 4.0 International License .","title":"Impressum"},{"location":"site-notice/#site-notice","text":"","title":"Site Notice"},{"location":"site-notice/#information-pursuant-to-sect-5-german-telemedia-act-tmg","text":"Alexander Trost Postfach 11 75196 K\u00e4mpfelbach Germany","title":"Information pursuant to Sect. 5 German Telemedia Act (TMG)"},{"location":"site-notice/#contact","text":"Phone: +497232370228 E-mail: galexrt@googlemail.com","title":"Contact"},{"location":"site-notice/#responsible-for-the-content-according-to-sect-55-paragraph-2-rstv","text":"Alexander Trost, Kelterstr. 4/1, 75196 Remchingen, Germany","title":"Responsible for the content according to Sect. 55, paragraph 2 RStV"},{"location":"site-notice/#liability-for-contents","text":"As service providers, we are liable for own contents of these websites according to Paragraph 7, Sect. 1 German Telemedia Act (TMG). However, according to Paragraphs 8 to 10 German Telemedia Act (TMG), service providers are not obligated to permanently monitor submitted or stored information or to search for evidences that indicate illegal activities. Legal obligations to removing information or to blocking the use of information remain unchallenged. In this case, liability is only possible at the time of knowledge about a specific violation of law. Illegal contents will be removed immediately at the time we get knowledge of them.","title":"Liability for Contents"},{"location":"site-notice/#liability-for-links","text":"Our offer includes links to external third-party websites. We have no influence on the contents of those websites, therefore we cannot guarantee for those contents. Providers or administrators of linked websites are always responsible for their own contents. The linked websites had been checked for possible violations of law at the time of the establishment of the link. Illegal contents were not detected at the time of the linking. A permanent monitoring of the contents of linked websites cannot be imposed without reasonable indications that there has been a violation of law. Illegal links will be removed immediately at the time we get knowledge of them.","title":"Liability for Links"},{"location":"site-notice/#copyright","text":"Contents and compilations published on these websites by the providers are subject to Creative Commons Attribution 4.0 International License . This work is licensed under a Creative Commons Attribution 4.0 International License .","title":"Copyright"},{"location":"site-notice/#impressum","text":"English version below Site Notice section .","title":"Impressum"},{"location":"site-notice/#angaben-gema-5-tmg","text":"Alexander Trost Kelterstr. 4/1 75196 Remchingen Germany","title":"Angaben gem\u00e4\u00df \u00a7 5 TMG"},{"location":"site-notice/#kontakt","text":"Telefon: +497232370228 E-Mail: galexrt@googlemail.com","title":"Kontakt"},{"location":"site-notice/#verantwortlich-fur-den-inhalt-nach-55-abs-2-rstv","text":"Alexander Trost, Kelterstr. 4/1, 75196 Remchingen, Germany","title":"Verantwortlich f\u00fcr den Inhalt nach \u00a7 55 Abs. 2 RStV"},{"location":"site-notice/#haftung-fur-inhalte","text":"Als Diensteanbieter sind wir gem\u00e4\u00df \u00a7 7 Abs.1 TMG f\u00fcr eigene Inhalte auf diesen Seiten nach den allgemeinen Gesetzen verantwortlich. Nach \u00a7\u00a7 8 bis 10 TMG sind wir als Diensteanbieter jedoch nicht verpflichtet, \u00fcbermittelte oder gespeicherte fremde Informationen zu \u00fcberwachen oder nach Umst\u00e4nden zu forschen, die auf eine rechtswidrige T\u00e4tigkeit hinweisen. Verpflichtungen zur Entfernung oder Sperrung der Nutzung von Informationen nach den allgemeinen Gesetzen bleiben hiervon unber\u00fchrt. Eine diesbez\u00fcgliche Haftung ist jedoch erst ab dem Zeitpunkt der Kenntnis einer konkreten Rechtsverletzung m\u00f6glich. Bei Bekanntwerden von entsprechenden Rechtsverletzungen werden wir diese Inhalte umgehend entfernen.","title":"Haftung f\u00fcr Inhalte"},{"location":"site-notice/#haftung-fur-links","text":"Unser Angebot enth\u00e4lt Links zu externen Websites Dritter, auf deren Inhalte wir keinen Einfluss haben. Deshalb k\u00f6nnen wir f\u00fcr diese fremden Inhalte auch keine Gew\u00e4hr \u00fcbernehmen. F\u00fcr die Inhalte der verlinkten Seiten ist stets der jeweilige Anbieter oder Betreiber der Seiten verantwortlich. Die verlinkten Seiten wurden zum Zeitpunkt der Verlinkung auf m\u00f6gliche Rechtsverst\u00f6\u00dfe \u00fcberpr\u00fcft. Rechtswidrige Inhalte waren zum Zeitpunkt der Verlinkung nicht erkennbar. Eine permanente inhaltliche Kontrolle der verlinkten Seiten ist jedoch ohne konkrete Anhaltspunkte einer Rechtsverletzung nicht zumutbar. Bei Bekanntwerden von Rechtsverletzungen werden wir derartige Links umgehend entfernen.","title":"Haftung f\u00fcr Links"},{"location":"site-notice/#urheberrecht","text":"Die durch die Seitenbetreiber erstellten Inhalte und Werke auf diesen Seiten unterliegen der Creative Commons Attribution 4.0 International License . Diese Arbeiten sind lizenziert unter der Creative Commons Attribution 4.0 International License .","title":"Urheberrecht"},{"location":"databases/","text":"","title":"Index"},{"location":"databases/MySQL-Family/cheat-sheet/","text":"Re-Create the debian-sys-maint User \u00b6 1 mysqldump --complete-insert --extended-insert = 0 -u root -p mysql | grep 'debian-sys-maint'","title":"Cheat Sheet"},{"location":"databases/MySQL-Family/cheat-sheet/#re-create-the-debian-sys-maint-user","text":"1 mysqldump --complete-insert --extended-insert = 0 -u root -p mysql | grep 'debian-sys-maint'","title":"Re-Create the debian-sys-maint User"},{"location":"databases/MySQL-Family/event_scheduler/","text":"Ever wanted to run queries in a cronjob? Where to safely put the database credentials? MySQL / MariaDB can help out with that. Enable the event_scheduler System \u00b6 You must have the event_scheduler enabled, this can be done by running the following query: 1 SET GLOBAL event_scheduler = ON ; Note It is highly recommended to use the config file(s) of your MySQL / MariaDB server to enable the event_scheduler feature. Another (hacky) way is to use the MySQL server init_file option which runs a SQL script on server startup. Create an Event to run a SQL Query \u00b6 The CREATE EVENT query below would run the query after the DO every day at 02:00 in the exampledb database. 1 2 3 4 5 6 7 8 9 10 CREATE EVENT ` exampledb ` . ` my_cool_table_reset_userOption45 ` ON SCHEDULE EVERY 1 DAY STARTS CURRENT_DATE + INTERVAL 1 DAY + INTERVAL 2 HOUR DO UPDATE ` exampledb ` . ` my_cool_table ` SET userOption45 = '' WHERE userOption45 != '' AND STR_TO_DATE ( userOption45 , '%Y-%m-%d' ) <= NOW (); Show existing Events \u00b6 Note exampledb is the database name. 1 SHOW EVENTS FROM ` exampledb ` ; References \u00b6 MySQL 5.7 Reference - event_scheduler MySQL 5.7 Reference - CREATE EVENT Statement .","title":"event_scheduler"},{"location":"databases/MySQL-Family/event_scheduler/#enable-the-event_scheduler-system","text":"You must have the event_scheduler enabled, this can be done by running the following query: 1 SET GLOBAL event_scheduler = ON ; Note It is highly recommended to use the config file(s) of your MySQL / MariaDB server to enable the event_scheduler feature. Another (hacky) way is to use the MySQL server init_file option which runs a SQL script on server startup.","title":"Enable the event_scheduler System"},{"location":"databases/MySQL-Family/event_scheduler/#create-an-event-to-run-a-sql-query","text":"The CREATE EVENT query below would run the query after the DO every day at 02:00 in the exampledb database. 1 2 3 4 5 6 7 8 9 10 CREATE EVENT ` exampledb ` . ` my_cool_table_reset_userOption45 ` ON SCHEDULE EVERY 1 DAY STARTS CURRENT_DATE + INTERVAL 1 DAY + INTERVAL 2 HOUR DO UPDATE ` exampledb ` . ` my_cool_table ` SET userOption45 = '' WHERE userOption45 != '' AND STR_TO_DATE ( userOption45 , '%Y-%m-%d' ) <= NOW ();","title":"Create an Event to run a SQL Query"},{"location":"databases/MySQL-Family/event_scheduler/#show-existing-events","text":"Note exampledb is the database name. 1 SHOW EVENTS FROM ` exampledb ` ;","title":"Show existing Events"},{"location":"databases/MySQL-Family/event_scheduler/#references","text":"MySQL 5.7 Reference - event_scheduler MySQL 5.7 Reference - CREATE EVENT Statement .","title":"References"},{"location":"databases/MySQL-Family/init_file/","text":"Add the following paramter to the [mysqld] or [mariadb] section of your my.cnf file (depending on the OS, at /etc/mysql/my.cnf , /etc/my.cnf , other path): 1 init_file = /etc/mysql/init.sql The /etc/mysql/init.sql file can contain \"any\" SQL queries. Example to enable / install the MariaDB Query Response Time Plugin plugin : 1 2 3 4 INSTALL SONAME 'query_response_time' ; SET GLOBAL query_response_time_stats = 1 ; SET GLOBAL query_response_time_flush = 1 ; Note There are other ways to install the plugin, but to \"initially\" flush the plugin's data the SET GLOBAL query_response_time_ can be useful to be run. References \u00b6 https://mariadb.com/docs/reference/mdb/system-variables/init_file/ https://dev.mysql.com/doc/refman/8.0/en/server-system-variables.html#sysvar_init_file","title":"init_file: Run SQL file on startup"},{"location":"databases/MySQL-Family/init_file/#references","text":"https://mariadb.com/docs/reference/mdb/system-variables/init_file/ https://dev.mysql.com/doc/refman/8.0/en/server-system-variables.html#sysvar_init_file","title":"References"},{"location":"databases/MySQL-Family/pitfalls/","text":"mysql CLI Console Password Character \"Limit\" \u00b6 mysql command only reads in 79 characters of \"password\" from TTY / \"console\". In my case I was copy'n'pasting / the password store autotyping database passwords which doesn't work then (leads to Access denied for user errors because the password is wrong). This \"limit\" doesn't seem to be documented anywhere I looked, so here we go. If you want to login to your users only use passwords 79 characters long when \"typing\" them on the console. Thanks to this Jira ticket (MXS-1766) to pointing to the code behind the \"limitation\"! I also need to give a shoutout to the people with which I reflected the issue and then stumbled upon the mysql CLI limitation. Code references : GitHub MariaDB/server - 10.3/client/mysql.cc Line 1959 GitHub MariaDB/server - 10.3/mysys/get_password.c Line 63 Replication User Password Character Limit \u00b6 The password for a replication user must be a maximum of 32 characters long. MySQL Memory / Storage Size Suffixes \u00b6 TL;DR The suffixes are in \"Bytes\". E.g., 16M = 16777216 Bytes. 1 2 mysql --max_allowed_packet=16777216 mysql --max_allowed_packet=16M The first command specifies the value in bytes. The second specifies the value in megabytes. For variables that take a numeric value, the value can be given with a suffix of K, M, or G to indicate a multiplier of 1024, 10242 or 10243. (For example, when used to set max_allowed_packet, the suffixes indicate units of kilobytes, megabytes, or gigabytes.) As of MySQL 8.0.14, a suffix can also be T, P, and E to indicate a multiplier of 10244, 10245 or 10246. Suffix letters can be uppercase or lowercase. Quoted from: MySQL 8.0 Reference Manual - 4.2.2.5 Using Options to Set Program Variables","title":"Pitfalls"},{"location":"databases/MySQL-Family/pitfalls/#mysql-cli-console-password-character-limit","text":"mysql command only reads in 79 characters of \"password\" from TTY / \"console\". In my case I was copy'n'pasting / the password store autotyping database passwords which doesn't work then (leads to Access denied for user errors because the password is wrong). This \"limit\" doesn't seem to be documented anywhere I looked, so here we go. If you want to login to your users only use passwords 79 characters long when \"typing\" them on the console. Thanks to this Jira ticket (MXS-1766) to pointing to the code behind the \"limitation\"! I also need to give a shoutout to the people with which I reflected the issue and then stumbled upon the mysql CLI limitation. Code references : GitHub MariaDB/server - 10.3/client/mysql.cc Line 1959 GitHub MariaDB/server - 10.3/mysys/get_password.c Line 63","title":"mysql CLI Console Password Character \"Limit\""},{"location":"databases/MySQL-Family/pitfalls/#replication-user-password-character-limit","text":"The password for a replication user must be a maximum of 32 characters long.","title":"Replication User Password Character Limit"},{"location":"databases/MySQL-Family/pitfalls/#mysql-memory-storage-size-suffixes","text":"TL;DR The suffixes are in \"Bytes\". E.g., 16M = 16777216 Bytes. 1 2 mysql --max_allowed_packet=16777216 mysql --max_allowed_packet=16M The first command specifies the value in bytes. The second specifies the value in megabytes. For variables that take a numeric value, the value can be given with a suffix of K, M, or G to indicate a multiplier of 1024, 10242 or 10243. (For example, when used to set max_allowed_packet, the suffixes indicate units of kilobytes, megabytes, or gigabytes.) As of MySQL 8.0.14, a suffix can also be T, P, and E to indicate a multiplier of 10244, 10245 or 10246. Suffix letters can be uppercase or lowercase. Quoted from: MySQL 8.0 Reference Manual - 4.2.2.5 Using Options to Set Program Variables","title":"MySQL Memory / Storage Size Suffixes"},{"location":"databases/MySQL-Family/slow-log/","text":"Enable Slow Query Log to File \u00b6 1 2 3 4 5 SET @@ global . slow_query_log_use_global_control = long_query_time , min_examined_row_limit , log_slow_verbosity ; SET GLOBAL slow_query_log_file = '/var/log/mysql/slow_log.log' ; SET GLOBAL min_examined_row_limit = 0 ; SET GLOBAL long_query_time = 0 ; SET GLOBAL slow_query_log = 1 ; Disable Slow Query Log \u00b6 It is important to log slow queries, so set it to something like 3 seconds. 1 SET GLOBAL long_query_time = 3 ;","title":"Slow Log"},{"location":"databases/MySQL-Family/slow-log/#enable-slow-query-log-to-file","text":"1 2 3 4 5 SET @@ global . slow_query_log_use_global_control = long_query_time , min_examined_row_limit , log_slow_verbosity ; SET GLOBAL slow_query_log_file = '/var/log/mysql/slow_log.log' ; SET GLOBAL min_examined_row_limit = 0 ; SET GLOBAL long_query_time = 0 ; SET GLOBAL slow_query_log = 1 ;","title":"Enable Slow Query Log to File"},{"location":"databases/MySQL-Family/slow-log/#disable-slow-query-log","text":"It is important to log slow queries, so set it to something like 3 seconds. 1 SET GLOBAL long_query_time = 3 ;","title":"Disable Slow Query Log"},{"location":"general/tools-utilities/","text":"Network \u00b6 DNS \u00b6 https://github.com/DNS-OARC/flamethrower - \"a DNS performance and functional testing utility supporting UDP, TCP, DoT and DoH (by @ns1)\" Kubernetes (K8S) \u00b6 Client \u00b6 https://github.com/kubernetes-sigs/krew/ - Find and install kubectl plugins - krew.dev https://github.com/stern/stern - Multi pod and container log tailing for Kubernetes with regex support for selection of Pods and Pods' containers. Network \u00b6 https://github.com/inovex/illuminatio - The kubernetes network policy validator. https://github.com/aquasecurity/kube-bench - Checks whether Kubernetes is deployed according to security best practices as defined in the CIS Kubernetes Benchmark Monitoring with Prometheus \u00b6 See Monitoring->Prometheus->Exporters page .","title":"Projects, Tools and Utilites"},{"location":"general/tools-utilities/#network","text":"","title":"Network"},{"location":"general/tools-utilities/#dns","text":"https://github.com/DNS-OARC/flamethrower - \"a DNS performance and functional testing utility supporting UDP, TCP, DoT and DoH (by @ns1)\"","title":"DNS"},{"location":"general/tools-utilities/#kubernetes-k8s","text":"","title":"Kubernetes (K8S)"},{"location":"general/tools-utilities/#client","text":"https://github.com/kubernetes-sigs/krew/ - Find and install kubectl plugins - krew.dev https://github.com/stern/stern - Multi pod and container log tailing for Kubernetes with regex support for selection of Pods and Pods' containers.","title":"Client"},{"location":"general/tools-utilities/#network_1","text":"https://github.com/inovex/illuminatio - The kubernetes network policy validator. https://github.com/aquasecurity/kube-bench - Checks whether Kubernetes is deployed according to security best practices as defined in the CIS Kubernetes Benchmark","title":"Network"},{"location":"general/tools-utilities/#monitoring-with-prometheus","text":"See Monitoring->Prometheus->Exporters page .","title":"Monitoring with Prometheus"},{"location":"general/useful-online-tools/","text":"Diagrams \u00b6 https://app.diagrams.net/ - Previously named draw.io . https://isoflow.io/ - \"Create beautiful cloud diagrams in minutes\" Image Resizing \u00b6 http://waifu2x.udp.jp - Free variant (less zoom). https://mng.waifu2x.me - Paid (not much, billed per minute).","title":"Online Tools"},{"location":"general/useful-online-tools/#diagrams","text":"https://app.diagrams.net/ - Previously named draw.io . https://isoflow.io/ - \"Create beautiful cloud diagrams in minutes\"","title":"Diagrams"},{"location":"general/useful-online-tools/#image-resizing","text":"http://waifu2x.udp.jp - Free variant (less zoom). https://mng.waifu2x.me - Paid (not much, billed per minute).","title":"Image Resizing"},{"location":"kubernetes/cheat-sheet/","text":"Update StatefulSet volumeClaimTemplates And Other Uneditable Sections \u00b6 Get the latest YAML of the StatefulSet you want to update (e.g., change size of a volumeClaimTemplates entry). Make the changes to the YAML file. Run kubectl delete statefulset STATEFULSET_NAME --cascade=orphan The important thing here is the --cascade=orphan flag, it stops the ControllerRevisions objects (+ the Pods) to not be deleted (no downtime). Now just apply your StatefulSet YAML and if needed trigger a rolling update of the StatefulSet using the kubectl rollout restart statefulset STATEFULSET_NAME command. Quickly trigger Rolling Update of Deployment, StatefulSet, DaemonSet, etc \u00b6 New Way \u00b6 Example for StatefulSet and Deployment below: 1 2 kubectl rollout restart statefulset STATEFULSET_NAME kubectl rollout restart deployment DEPLOYMENT_NAME Old Way \u00b6 1 kubectl patch -n kube-system ds kube-proxy -p \"{\\\"spec\\\":{\\\"template\\\":{\\\"metadata\\\":{\\\"annotations\\\":{\\\"date\\\":\\\"$(date +'%s')\\\"}}}}}\" Running kubectl replace / kubectl apply on an object which the command above was used on, will always trigger a rolling update again. This is due to the change to the annotations. Debug Pod manifest to \"escape\" to the node \u00b6 The Pods manifest assumes that you are allowed to run privileged Pods in your cluster. If you are using you may need to set a ServiceAccount which is allowed \"all the things\" (e.g. privileged , hostNetwork , and so on). 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 kind : Pod apiVersion : v1 metadata : name : debug-pod labels : app : debug spec : hostNetwork : true tolerations : - key : node-role.kubernetes.io/master effect : NoSchedule - key : \"CriticalAddonsOnly\" operator : \"Exists\" restartPolicy : Never hostIPC : true hostPID : true # nodeName: SPECIFIC_TARGET_NODE priorityClassName : \"system-cluster-critical\" containers : - name : debug-pod image : busybox command : [ \"/bin/sleep\" , \"36000\" ] securityContext : privileged : true allowPrivilegeEscalation : true kubectl exec -it POD_NAME -- sh into the Pod and use nsenter to escape the container's namespace: 1 $ nsenter -t 1 -m -u -n -i sh Role Label for Node objects \u00b6 The node-role.kubernetes.io/ can take \"anything\" as a role. Meaning that node-role.kubernetes.io/my-cool-role (any value) will cause the kubectl get nodes output to display my-cool-role (and other such role labels) as the Node role. kubectl: Context Switching \u00b6 Switch Namespace \u00b6 1 kubectl config set-context --current --namespace NAMESPACE Switch Context \u00b6 1 kubectl config use-context CONTEXT_NAME Show Contexts \u00b6 1 kubectl config get-contexts Run A CronJob Manually \u00b6 1 2 kubectl create job --from=cronjob/CRONJOB_NAME JOB_NAME kubectl create job --from=cronjob/curator curator-manual-run","title":"Cheat Sheet"},{"location":"kubernetes/cheat-sheet/#update-statefulset-volumeclaimtemplates-and-other-uneditable-sections","text":"Get the latest YAML of the StatefulSet you want to update (e.g., change size of a volumeClaimTemplates entry). Make the changes to the YAML file. Run kubectl delete statefulset STATEFULSET_NAME --cascade=orphan The important thing here is the --cascade=orphan flag, it stops the ControllerRevisions objects (+ the Pods) to not be deleted (no downtime). Now just apply your StatefulSet YAML and if needed trigger a rolling update of the StatefulSet using the kubectl rollout restart statefulset STATEFULSET_NAME command.","title":"Update StatefulSet volumeClaimTemplates And Other Uneditable Sections"},{"location":"kubernetes/cheat-sheet/#quickly-trigger-rolling-update-of-deployment-statefulset-daemonset-etc","text":"","title":"Quickly trigger Rolling Update of Deployment, StatefulSet, DaemonSet, etc"},{"location":"kubernetes/cheat-sheet/#new-way","text":"Example for StatefulSet and Deployment below: 1 2 kubectl rollout restart statefulset STATEFULSET_NAME kubectl rollout restart deployment DEPLOYMENT_NAME","title":"New Way"},{"location":"kubernetes/cheat-sheet/#old-way","text":"1 kubectl patch -n kube-system ds kube-proxy -p \"{\\\"spec\\\":{\\\"template\\\":{\\\"metadata\\\":{\\\"annotations\\\":{\\\"date\\\":\\\"$(date +'%s')\\\"}}}}}\" Running kubectl replace / kubectl apply on an object which the command above was used on, will always trigger a rolling update again. This is due to the change to the annotations.","title":"Old Way"},{"location":"kubernetes/cheat-sheet/#debug-pod-manifest-to-escape-to-the-node","text":"The Pods manifest assumes that you are allowed to run privileged Pods in your cluster. If you are using you may need to set a ServiceAccount which is allowed \"all the things\" (e.g. privileged , hostNetwork , and so on). 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 kind : Pod apiVersion : v1 metadata : name : debug-pod labels : app : debug spec : hostNetwork : true tolerations : - key : node-role.kubernetes.io/master effect : NoSchedule - key : \"CriticalAddonsOnly\" operator : \"Exists\" restartPolicy : Never hostIPC : true hostPID : true # nodeName: SPECIFIC_TARGET_NODE priorityClassName : \"system-cluster-critical\" containers : - name : debug-pod image : busybox command : [ \"/bin/sleep\" , \"36000\" ] securityContext : privileged : true allowPrivilegeEscalation : true kubectl exec -it POD_NAME -- sh into the Pod and use nsenter to escape the container's namespace: 1 $ nsenter -t 1 -m -u -n -i sh","title":"Debug Pod manifest to \"escape\" to the node"},{"location":"kubernetes/cheat-sheet/#role-label-for-node-objects","text":"The node-role.kubernetes.io/ can take \"anything\" as a role. Meaning that node-role.kubernetes.io/my-cool-role (any value) will cause the kubectl get nodes output to display my-cool-role (and other such role labels) as the Node role.","title":"Role Label for Node objects"},{"location":"kubernetes/cheat-sheet/#kubectl-context-switching","text":"","title":"kubectl: Context Switching"},{"location":"kubernetes/cheat-sheet/#switch-namespace","text":"1 kubectl config set-context --current --namespace NAMESPACE","title":"Switch Namespace"},{"location":"kubernetes/cheat-sheet/#switch-context","text":"1 kubectl config use-context CONTEXT_NAME","title":"Switch Context"},{"location":"kubernetes/cheat-sheet/#show-contexts","text":"1 kubectl config get-contexts","title":"Show Contexts"},{"location":"kubernetes/cheat-sheet/#run-a-cronjob-manually","text":"1 2 kubectl create job --from=cronjob/CRONJOB_NAME JOB_NAME kubectl create job --from=cronjob/curator curator-manual-run","title":"Run A CronJob Manually"},{"location":"kubernetes/cluster-components-upgrade-order/","text":"The following is a recommended Upgrade Order for the Components of a Kubernetes cluster: Master Components kube-apiserver kube-controller-manager If used, cloud-controller-manager kube-scheduler Node Components kubelet kube-proxy Other components of a Kubernetes cluster can mostly be updated in any order, as long as the documentation of the component doesn't state otherwise: CNI (e.g., Calico, Cillium) etcd Operators Be aware of potential changes in the operator causing unwanted \"results\".","title":"Cluster Components Upgrade Order"},{"location":"kubernetes/kubeadm/","text":"System Preparations \u00b6 1 2 3 4 5 6 7 echo \"br_netfilter\" > /etc/modules-load.d/br_netfilter.conf modprobe br_netfilter cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 EOF sudo sysctl --system Container Runtime Install \u00b6 1 2 3 4 5 6 7 8 9 10 11 dnf module list cri-o # Check if a CRI-O version with the same version as K8S is available, if not one version lower can also be okay. dnf module enable cri-o:1.18 -y dnf install -y cri-o sed -i 's/^cgroup_manager =.*$/cgroup_manager = \"systemd\"/g' /etc/crio/crio.conf rm -f /etc/cni/net.d/100-crio-bridge.conf /etc/cni/net.d/200-loopback.conf systemctl enable crio systemctl start crio Kubernetes Installation \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 cat <<EOF | sudo tee /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-\\$ basearch enabled=1 gpgcheck=1 repo_gpgcheck=1 gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg exclude=kubelet kubeadm kubectl EOF # Set SELinux in permissive mode ( effectively disabling it ) sudo setenforce 0 sudo sed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config sudo yum install -y kubelet kubeadm kubectl --disableexcludes=kubernetes sudo systemctl enable --now kubelet","title":"kubeadm"},{"location":"kubernetes/kubeadm/#system-preparations","text":"1 2 3 4 5 6 7 echo \"br_netfilter\" > /etc/modules-load.d/br_netfilter.conf modprobe br_netfilter cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 EOF sudo sysctl --system","title":"System Preparations"},{"location":"kubernetes/kubeadm/#container-runtime-install","text":"1 2 3 4 5 6 7 8 9 10 11 dnf module list cri-o # Check if a CRI-O version with the same version as K8S is available, if not one version lower can also be okay. dnf module enable cri-o:1.18 -y dnf install -y cri-o sed -i 's/^cgroup_manager =.*$/cgroup_manager = \"systemd\"/g' /etc/crio/crio.conf rm -f /etc/cni/net.d/100-crio-bridge.conf /etc/cni/net.d/200-loopback.conf systemctl enable crio systemctl start crio","title":"Container Runtime Install"},{"location":"kubernetes/kubeadm/#kubernetes-installation","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 cat <<EOF | sudo tee /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-\\$ basearch enabled=1 gpgcheck=1 repo_gpgcheck=1 gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg exclude=kubelet kubeadm kubectl EOF # Set SELinux in permissive mode ( effectively disabling it ) sudo setenforce 0 sudo sed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config sudo yum install -y kubelet kubeadm kubectl --disableexcludes=kubernetes sudo systemctl enable --now kubelet","title":"Kubernetes Installation"},{"location":"kubernetes/system-requirements/","text":"Recommendations for Requirements These are recommendations for requirements of Kubernetes Masters and Nodes. General \u00b6 Network: Bandwidth: at the very least 1G, recommended for smaller, lower traffic environents is 10G, 25G or more. Note: To reduce costs you get away with just having a single interface, instead of, e.g., 2 bonded interfaces, though having 2 will allow for more performance and also the server still being reachable in case of failure. Master \u00b6 Storage \u00b6 SSDs or even NVMe based storage is more expensive but your ETCD will love and need it! DO NOT USE HDDs nor any kind of networked storage for ETCD! Even a fast Ceph RBD (e.g., when running in VMs) can look good in the beginning but might \"kill\" the ETCD performance in the end! Too many users of Kubernetes or OpenShift do that and end up with slow performing clusters in many different ways, simply because the ETCD is slow (even though the kube-apiservers are caching a lot) Use SSDs or any other storage with low latencies (LOW)! ETCD is pretty much latency bound, it needs its \"few\" IOPS as well but latency is the killer (sequential writing, e.g., WAL and DB).","title":"System Requirements"},{"location":"kubernetes/system-requirements/#general","text":"Network: Bandwidth: at the very least 1G, recommended for smaller, lower traffic environents is 10G, 25G or more. Note: To reduce costs you get away with just having a single interface, instead of, e.g., 2 bonded interfaces, though having 2 will allow for more performance and also the server still being reachable in case of failure.","title":"General"},{"location":"kubernetes/system-requirements/#master","text":"","title":"Master"},{"location":"kubernetes/system-requirements/#storage","text":"SSDs or even NVMe based storage is more expensive but your ETCD will love and need it! DO NOT USE HDDs nor any kind of networked storage for ETCD! Even a fast Ceph RBD (e.g., when running in VMs) can look good in the beginning but might \"kill\" the ETCD performance in the end! Too many users of Kubernetes or OpenShift do that and end up with slow performing clusters in many different ways, simply because the ETCD is slow (even though the kube-apiservers are caching a lot) Use SSDs or any other storage with low latencies (LOW)! ETCD is pretty much latency bound, it needs its \"few\" IOPS as well but latency is the killer (sequential writing, e.g., WAL and DB).","title":"Storage"},{"location":"kubernetes/ETCD/cheat-sheet/","text":"Check ETCD performance \"status\" quickly \u00b6 1 2 3 ETCDCTL_API = 3 etcdctl \\ [ YOUR_FLAGS ] \\ check perf Example \u00b6 Kubernetes ( kubeadm ) \u00b6 1 2 3 4 5 ETCDCTL_API = 3 etcdctl \\ --cacert = /etc/kubernetes/pki/etcd/ca.crt \\ --cert = /etc/kubernetes/pki/etcd/server.crt \\ --key = /etc/kubernetes/pki/etcd/server.key \\ check perf Get Metrics from ETCD using curl \u00b6 Note The /etc/etcd/etcd.conf was actively used in OpenShift 3.x installations and some older Kubernetes deployment \"methods\". 1 2 3 4 # Should you still have a `etcd.conf` source it source /etc/etcd/etcd.conf # Otherwise replace each `$ETCD_PEER_*` with the according path curl --cacert = $ETCD_PEER_CA_FILE --cert = $ETCD_PEER_CERT_FILE --key = $ETCD_PEER_KEY_FILE -L https://127.0.0.1:2379/metrics -XGET -v Show ETCD Cluster Members \u00b6 1 2 3 4 5 ETCDCTL_API = 3 etcdctl \\ --cacert = /etc/kubernetes/pki/etcd/ca.crt \\ --cert = /etc/kubernetes/pki/etcd/server.crt \\ --key = /etc/kubernetes/pki/etcd/server.key \\ member list","title":"Cheat Sheet"},{"location":"kubernetes/ETCD/cheat-sheet/#check-etcd-performance-status-quickly","text":"1 2 3 ETCDCTL_API = 3 etcdctl \\ [ YOUR_FLAGS ] \\ check perf","title":"Check ETCD performance \"status\" quickly"},{"location":"kubernetes/ETCD/cheat-sheet/#example","text":"","title":"Example"},{"location":"kubernetes/ETCD/cheat-sheet/#kubernetes-kubeadm","text":"1 2 3 4 5 ETCDCTL_API = 3 etcdctl \\ --cacert = /etc/kubernetes/pki/etcd/ca.crt \\ --cert = /etc/kubernetes/pki/etcd/server.crt \\ --key = /etc/kubernetes/pki/etcd/server.key \\ check perf","title":"Kubernetes (kubeadm)"},{"location":"kubernetes/ETCD/cheat-sheet/#get-metrics-from-etcd-using-curl","text":"Note The /etc/etcd/etcd.conf was actively used in OpenShift 3.x installations and some older Kubernetes deployment \"methods\". 1 2 3 4 # Should you still have a `etcd.conf` source it source /etc/etcd/etcd.conf # Otherwise replace each `$ETCD_PEER_*` with the according path curl --cacert = $ETCD_PEER_CA_FILE --cert = $ETCD_PEER_CERT_FILE --key = $ETCD_PEER_KEY_FILE -L https://127.0.0.1:2379/metrics -XGET -v","title":"Get Metrics from ETCD using curl"},{"location":"kubernetes/ETCD/cheat-sheet/#show-etcd-cluster-members","text":"1 2 3 4 5 ETCDCTL_API = 3 etcdctl \\ --cacert = /etc/kubernetes/pki/etcd/ca.crt \\ --cert = /etc/kubernetes/pki/etcd/server.crt \\ --key = /etc/kubernetes/pki/etcd/server.key \\ member list","title":"Show ETCD Cluster Members"},{"location":"kubernetes/ETCD/editing-kubernetes-objects/","text":"Danger You should be 100% sure what you are doing and should have at least a snapshot of the etcd you want to edit as things can and will possibly go wrong! Do this at your own risk! Requirements \u00b6 ETCD cluster running. etcdctl can reach it (you need to know which flags to provide, e.g., for tls certs and so on). Golang installed ( glide installed in PATH , e.g., go get -u github.com/Masterminds/glide ). PATH includes the $GOPATH/bin ( export PATH=\"$GOPATH/bin/:$PATH\" ) Steps \u00b6 Step 1 - Prepare Environment \u00b6 1 2 3 4 5 go get -u github.com/jpbetz/auger cd $GOPATH /src/github.com/jpbetz/auger go get -u github.com/Masterminds/glide make vendor # Stay in this directory Warning Be sure to stop all API servers, before continuing with the next steps. Be sure to stop all Controller Manager servers, before continuing with the next steps. Depending on the cluster setup, you may just need move out the according manifest from /etc/kubernetes/manifests directory. Step 2 - Locate object path \u00b6 Note The etcdctl probably needs to be run inside the etcd container on one of the Kubernetes masters. 1 2 ETCDCTL_API = 3 etcdctl \\ get /registry/ --keys-only --prefix I recommend you to keep the session on the server for etcdctl open and after finding the correct key to export it using export YOUR_OBJECT_PATH=__PATH__ as it will be used like this later on. Step 3 - Get object from ETCD \u00b6 Note The etcdctl probably needs to be run inside the etcd container on one of the Kubernetes masters. Replace $YOUR_OBJECT_PATH with the path of the object or set it as a variable. 1 2 3 4 5 6 ETCDCTL_API = 3 etcdctl \\ --endpoints = https:// [ 127 .0.0.1 ] :2379 \\ --cacert = /var/lib/minikube/certs//etcd/ca.crt \\ --cert = /var/lib/minikube/certs//etcd/healthcheck-client.crt \\ --key = /var/lib/minikube/certs//etcd/healthcheck-client.key \\ get $YOUR_OBJECT_PATH > etcd-data-old.bin Copy etcd-data-old.bin to the host, e.g.: 1 scp $SSH_USER @ $SSH_HOST :etcd-data-old.bin . Step 4 - Decode and edit the produced output as you need \u00b6 1 2 cat etcd-data-old.bin | \\ go run main.go decode > object.yaml Now edit the object.yaml as you need. Step 5 - Encode and save data to ETCD \u00b6 1 2 cat object.yaml | \\ go run main.go encode > etcd-data-new.bin Copy the etcd-data-new.bin to the host, e.g.: 1 scp etcd-data-new.bin $SSH_USER @ $SSH_HOST : Note The etcdctl probably needs to be run inside the etcd container on one of the Kubernetes masters. 1 2 3 4 5 6 7 cat etcd-data-new.bin | \\ ETCDCTL_API = 3 etcdctl \\ --endpoints = https:// [ 127 .0.0.1 ] :2379 \\ --cacert = /var/lib/minikube/certs//etcd/ca.crt \\ --cert = /var/lib/minikube/certs//etcd/healthcheck-client.crt \\ --key = /var/lib/minikube/certs//etcd/healthcheck-client.key \\ put $YOUR_OBJECT_PATH Step 6 - Verify the object is valid for Kubernetes \u00b6 Just run kubectl get OBJECT_KIND OBJECT_NAME -o yaml on the object you just edited to ensure it is still in working order. If it returns the objects YAML, you are fine. In case of errors, such as illegal bytes or so, you should restore a backup ASAP!","title":"Editing Kubernetes Objects"},{"location":"kubernetes/ETCD/editing-kubernetes-objects/#requirements","text":"ETCD cluster running. etcdctl can reach it (you need to know which flags to provide, e.g., for tls certs and so on). Golang installed ( glide installed in PATH , e.g., go get -u github.com/Masterminds/glide ). PATH includes the $GOPATH/bin ( export PATH=\"$GOPATH/bin/:$PATH\" )","title":"Requirements"},{"location":"kubernetes/ETCD/editing-kubernetes-objects/#steps","text":"","title":"Steps"},{"location":"kubernetes/ETCD/editing-kubernetes-objects/#step-1-prepare-environment","text":"1 2 3 4 5 go get -u github.com/jpbetz/auger cd $GOPATH /src/github.com/jpbetz/auger go get -u github.com/Masterminds/glide make vendor # Stay in this directory Warning Be sure to stop all API servers, before continuing with the next steps. Be sure to stop all Controller Manager servers, before continuing with the next steps. Depending on the cluster setup, you may just need move out the according manifest from /etc/kubernetes/manifests directory.","title":"Step 1 - Prepare Environment"},{"location":"kubernetes/ETCD/editing-kubernetes-objects/#step-2-locate-object-path","text":"Note The etcdctl probably needs to be run inside the etcd container on one of the Kubernetes masters. 1 2 ETCDCTL_API = 3 etcdctl \\ get /registry/ --keys-only --prefix I recommend you to keep the session on the server for etcdctl open and after finding the correct key to export it using export YOUR_OBJECT_PATH=__PATH__ as it will be used like this later on.","title":"Step 2 - Locate object path"},{"location":"kubernetes/ETCD/editing-kubernetes-objects/#step-3-get-object-from-etcd","text":"Note The etcdctl probably needs to be run inside the etcd container on one of the Kubernetes masters. Replace $YOUR_OBJECT_PATH with the path of the object or set it as a variable. 1 2 3 4 5 6 ETCDCTL_API = 3 etcdctl \\ --endpoints = https:// [ 127 .0.0.1 ] :2379 \\ --cacert = /var/lib/minikube/certs//etcd/ca.crt \\ --cert = /var/lib/minikube/certs//etcd/healthcheck-client.crt \\ --key = /var/lib/minikube/certs//etcd/healthcheck-client.key \\ get $YOUR_OBJECT_PATH > etcd-data-old.bin Copy etcd-data-old.bin to the host, e.g.: 1 scp $SSH_USER @ $SSH_HOST :etcd-data-old.bin .","title":"Step 3 - Get object from ETCD"},{"location":"kubernetes/ETCD/editing-kubernetes-objects/#step-4-decode-and-edit-the-produced-output-as-you-need","text":"1 2 cat etcd-data-old.bin | \\ go run main.go decode > object.yaml Now edit the object.yaml as you need.","title":"Step 4 - Decode and edit the produced output as you need"},{"location":"kubernetes/ETCD/editing-kubernetes-objects/#step-5-encode-and-save-data-to-etcd","text":"1 2 cat object.yaml | \\ go run main.go encode > etcd-data-new.bin Copy the etcd-data-new.bin to the host, e.g.: 1 scp etcd-data-new.bin $SSH_USER @ $SSH_HOST : Note The etcdctl probably needs to be run inside the etcd container on one of the Kubernetes masters. 1 2 3 4 5 6 7 cat etcd-data-new.bin | \\ ETCDCTL_API = 3 etcdctl \\ --endpoints = https:// [ 127 .0.0.1 ] :2379 \\ --cacert = /var/lib/minikube/certs//etcd/ca.crt \\ --cert = /var/lib/minikube/certs//etcd/healthcheck-client.crt \\ --key = /var/lib/minikube/certs//etcd/healthcheck-client.key \\ put $YOUR_OBJECT_PATH","title":"Step 5 - Encode and save data to ETCD"},{"location":"kubernetes/ETCD/editing-kubernetes-objects/#step-6-verify-the-object-is-valid-for-kubernetes","text":"Just run kubectl get OBJECT_KIND OBJECT_NAME -o yaml on the object you just edited to ensure it is still in working order. If it returns the objects YAML, you are fine. In case of errors, such as illegal bytes or so, you should restore a backup ASAP!","title":"Step 6 - Verify the object is valid for Kubernetes"},{"location":"kubernetes/ETCD/snapshots-save-restore/","text":"Warning This page is only for ETCD version 3.x and higher! For the original commands and more information on ETCD, see https://coreos.com/etcd/docs/latest/op-guide/recovery.html . Take a snapshot \u00b6 1 2 3 ETCDCTL_API = 3 etcdctl \\ --endpoints $ETCD_ENDPOINT \\ snapshot save snapshot.db (where snapshot.db is the name of the snapshot file to be created) Restore a snapshot \u00b6 Danger Before restoring a snapshot, all ETCDs in the cluster must be stopped! You must rename/remove the current data dir (probably /var/lib/etcd ). Be sure to provide all flags that are specified in, e.g., systemd unit file, Kubespray: /etc/etcd.env and others otherwise you may create issues for the ETCD cluster! The command is looking about like that depending on what flags are used for your ETCD node: 1 2 3 4 5 6 7 8 9 10 # Run the command as `root` user after that use `chown` to correct ownership of files ETCDCTL_API = 3 etcdctl \\ snapshot restore snapshot.db \\ --name m1 \\ --initial-cluster m1 = http://host1:2380,m2 = http://host2:2380,m3 = http://host3:2380 \\ --initial-cluster-token etcd-cluster-1 \\ --initial-advertise-peer-urls http://host1:2380 --data-dir = /var/lib/etcd # chown etcd:etcd -R /var/lib/etcd This has to be done on all ETCD servers one by one with each having their own name given by flag as they were when the snapshot was taken.","title":"Snapshots: Save & Restore"},{"location":"kubernetes/ETCD/snapshots-save-restore/#take-a-snapshot","text":"1 2 3 ETCDCTL_API = 3 etcdctl \\ --endpoints $ETCD_ENDPOINT \\ snapshot save snapshot.db (where snapshot.db is the name of the snapshot file to be created)","title":"Take a snapshot"},{"location":"kubernetes/ETCD/snapshots-save-restore/#restore-a-snapshot","text":"Danger Before restoring a snapshot, all ETCDs in the cluster must be stopped! You must rename/remove the current data dir (probably /var/lib/etcd ). Be sure to provide all flags that are specified in, e.g., systemd unit file, Kubespray: /etc/etcd.env and others otherwise you may create issues for the ETCD cluster! The command is looking about like that depending on what flags are used for your ETCD node: 1 2 3 4 5 6 7 8 9 10 # Run the command as `root` user after that use `chown` to correct ownership of files ETCDCTL_API = 3 etcdctl \\ snapshot restore snapshot.db \\ --name m1 \\ --initial-cluster m1 = http://host1:2380,m2 = http://host2:2380,m3 = http://host3:2380 \\ --initial-cluster-token etcd-cluster-1 \\ --initial-advertise-peer-urls http://host1:2380 --data-dir = /var/lib/etcd # chown etcd:etcd -R /var/lib/etcd This has to be done on all ETCD servers one by one with each having their own name given by flag as they were when the snapshot was taken.","title":"Restore a snapshot"},{"location":"kubernetes/logging/regex/","text":"Info In general make sure to forward the audit logs to your log store of choice. Kubernetes docs: https://kubernetes.io/docs/tasks/debug-application-cluster/audit/ Some neat regex to parse certain messages from Kubernetes logs. RBAC DENY Messages \u00b6 Good to setup some log alerting on those messages to make sure the applications are not hammering the API servers with \"bad\" RBAC. Match message (should be enough for matching): 1 \\] RBAC DENY: Rewriting into a comma separated list + showing occurence counts: 1 perl -n -e '/\\] RBAC DENY: user \"(.+)\" groups \\[(\".+\")\\] cannot \"([a-zA-Z]+)\" resource \"([a-zA-Z._-]+)\" in namespace \"([a-zA-Z-_]+)\"/ && print \"ns=$5,verb=$3,resource=$4,user=$1,groups=$2\\n\"' | sort | uniq -c","title":"Regex"},{"location":"kubernetes/logging/regex/#rbac-deny-messages","text":"Good to setup some log alerting on those messages to make sure the applications are not hammering the API servers with \"bad\" RBAC. Match message (should be enough for matching): 1 \\] RBAC DENY: Rewriting into a comma separated list + showing occurence counts: 1 perl -n -e '/\\] RBAC DENY: user \"(.+)\" groups \\[(\".+\")\\] cannot \"([a-zA-Z]+)\" resource \"([a-zA-Z._-]+)\" in namespace \"([a-zA-Z-_]+)\"/ && print \"ns=$5,verb=$3,resource=$4,user=$1,groups=$2\\n\"' | sort | uniq -c","title":"RBAC DENY Messages"},{"location":"kubernetes/monitoring/","text":"Use Prometheus for in-cluster monitoring! If you need a centralized view of multiple Kubernetes clusters, checkout Thanos.","title":"Monitoring"},{"location":"kubernetes/monitoring/components/","text":"Infrastructure / Cluster components should be monitored separately from your applications. This allows you to \"kill\" the application Prometheus in case you have screwed up in some way (e.g., messed up application metrics causing to have a billion labeled metrics). Master Components \u00b6 etcd \u00b6 Summary: \"Why do my kubectl commands take so long?\" Port: 2379/TCP Path: /metrics Auth: Client Certificate Notes: Thanks to etcd having a role concept, can be a \"separate\" user with just metrics access. Additionally one might want to run a Kubernetes authenticated OAuth proxy in front of th etcd so that the (one or more) Prometheus can be granted access to it by RoleBinding a Role to the ServiceAccount. E.g., you can create a Secret with the ETCD certificate and key using the kubectl create secret generic --from-file=.../ca.crt --from-file=.../monitoring.crt --from-file=.../monitoring.key Be sure to mount that Secret inside your Prometheus instance and adjust the path(s) according to the mountPath . Prometheus Config Scrape Job Reference, see References Prometheus Kubernetes ETCD Scrape Job Config . What can the Metrics tell us: ETCD disk write latencies (they should be low, very low). ETCD Quorum status as well (+ some other metrics, e.g., how many requests / streams). Golang Process information (e.g., CPU, memory, GC status). kube-apiserver \u00b6 Summary: \"Why do my kubectl commands take so long?\" Port: 6443/TCP (depends on your installation) Path: /metrics Auth: ServiceAccount token. Note(s): RBAC ClusterRole needed, see References Prometheus ClusterRole and References Prometheus Per-Namespace Role . What can the Metrics tell us: API Request latencies (kubectl, operators, scheduler, controller, etc). Best used with histogram_quantile() Prometheus func over some time. Admission step and webhook controller durations and latencies. ETCD request metrics. API server cache metrics. Kubernetes API Rest Client latency, requests and duration metrics (own requests made). Golang Process information (e.g., CPU, memory, GC status). kube-scheduler \u00b6 Summary: \"How long do my Pods take to be scheudled?\" (Hints at \"you need to change your scheduler config\") Port: 443/TCP (depends on your installation) Path: /metrics Auth: ServiceAccount token. Note(s): RBAC ClusterRole needed, see References Prometheus ClusterRole and References Prometheus Per-Namespace Role . The kube-scheduler needs to either listen on :: ( 0.0.0.0 ) or have a proxy which is available to Prometheus for scraping running. What can the Metrics tell us: \"How long do my Pods take to be scheudled?\" ( scheduler_binding_* ) Pod Preemption Algorithm latencies and more. Volume scheduling duration. Kubernetes API Rest Client latency, requests and duration metrics (own requests made to the API). Golang Process information (e.g., CPU, memory, GC status). kube-controller-manager \u00b6 Info If the cloud controller is used, it should also be monitored / scraped for metrics. Summary: \"How long do my Pods take to be scheudled?\" (Hints at \"you need to change your scheduler config\") Port: 443/TCP (depends on your installation) Path: /metrics Auth: ServiceAccount token. Note(s): RBAC ClusterRole needed, see References Prometheus ClusterRole and References Prometheus Per-Namespace Role . The kube-controller-manager needs to either listen on :: ( 0.0.0.0 ) or have a proxy which is available to Prometheus for scraping running. What can the Metrics tell us: Work queue item count and duration, and lease holder status. Kubernetes API Rest Client latency, requests and duration metrics (own requests made to the API). Golang Process information (e.g., CPU, memory, GC status). kubelet (+ cadvisor) \u00b6 See Node Components - kubelet . kube-proxy \u00b6 See Node Components - kube-proxy . SDN (e.g., Calico, Cilium) \u00b6 See Node Components - kube-proxy . Node Components \u00b6 kubelet (+ \"cadvisor\") \u00b6 Summary: \"How long do my Pods take to be scheudled?\" (Hints at \"you need to change your scheduler config\") Port: 443/TCP (depends on your installation) Path: /metrics Auth: ServiceAccount token. Note(s): RBAC ClusterRole needed, see References Prometheus ClusterRole and References Prometheus Per-Namespace Role . kubelet needs to have the following flags active: --authorization-mode=Webhook and --authentication-token-webhook=true . What can the Metrics tell us: kubelet_node_config_error good to know if the latest config works. Kubelet PLEG (\"container runtime status\") metrics. \"Pod Start times\" (use histogram_quantile() ). CGroup metrics of containers (cpu, memory, network) with Namespace and Pod labels. Kubernetes API Rest Client latency, requests and duration metrics (own requests made to the API). Golang Process information (e.g., CPU, memory, GC status). kube-proxy \u00b6 Summary: \"How long do my Pods take to be scheudled?\" (Hints at \"you need to change your scheduler config\") Port: 10250/TCP (depends on your installation) Path: /metrics Auth: ServiceAccount token. Note(s): RBAC ClusterRole needed, see References Prometheus ClusterRole and References Prometheus Per-Namespace Role . What can the Metrics tell us: iptables and / or ipvs sync information (~= how long does it take for Service changes to be reflected in the \"routing\" rules). Kubernetes API Rest Client latency, requests and duration metrics (own requests made to the API). Golang Process information (e.g., CPU, memory, GC status). SDN / CNI (e.g., Calico, Cilium) \u00b6 Depends on the SDN / CNI used, if there are metrics available. Calico for example can expose metrics, but that must be enabled through a environemnt variable on the Calico Node DaemonSet. For other SDNs, e.g., OpenVSwitch you may need to use an \"external\" exporter when available: https://github.com/digitalocean/openvswitch_exporter https://github.com/ovnworks/ovn_exporter What can the Metrics tell us: Depending on the exporter, at least how much traffic is flowing and / or if there are issues with the daemon. The Nodes themself \u00b6 node_exporter \u00b6 See Monitoring/Prometheus/Exporters - node_exporter . ethtool_exporter \u00b6 See Monitoring/Prometheus/Exporters - ethtool_exporter . Additional In-Cluster Components \u00b6 Other components that are in and / or around a Kubernetes cluster. metrics-server (previously named heapster) \u00b6 (More information https://kubernetes.io/docs/tasks/debug-application-cluster/resource-metrics-pipeline/ ) Summary: \"How long do my Pods take to be scheudled?\" (Hints at \"you need to change your scheduler config\") Port: 443/TCP Path: /metrics Auth: None. (Recommendation: Add Kubernetes OAuth Proxy in front) What can the Metrics tell us: Possibly metrics on how Golang Process information (e.g., CPU, memory, GC status). kube-state-metrics \u00b6 Summary: \"How long do my Pods take to be scheudled?\" (Hints at \"you need to change your scheduler config\") Port: 8080/TCP for \"cluster\" metrics and 8081/TCP for kube-state-metrics metrics. (Recommended to scrape both) Path: /metrics Auth: ServiceAccount token. Note(s): RBAC ClusterRole needed, see References Prometheus ClusterRole and References Prometheus Per-Namespace Role . What can the Metrics tell us: Metrics about Deployments, StatefulSets, Jobs, CronJobs, and basically any other objects Status (that is in the official Kubernetes APIs). Golang Process information (e.g., CPU, memory, GC status). Other Components \u00b6 Elasticsearch \u00b6 Elasticsearch is not providing Prometheus metrics itself, but there is a well written exporter GitHub justwatchcom/elasticsearch_exporter . (There are some other exporters also available, though I have used mainly used this one for the amount of metrics I'm able to get from Elasticsearch with it) Summary: \"Is my Elasticsearch able to ingest the amount of logs? Do I need to add more data nodes and / or resources?\" Port: 9114/TCP (depends on your installation) Path: /metrics Auth: ServiceAccount token Note(s): RBAC ClusterRole needed, see References Prometheus ClusterRole and References Prometheus Per-Namespace Role . What can the Metrics tell us: Elasticsearch cluster status. Resources (CPU and Memory) and also Storage usage. Elasticsearch Indices status (when enabled (can be filtered in different ways)). Elasticsearch JVM info (GC, memory, etc). Checkout the metrics list for a list of all Metrics available: https://github.com/justwatchcom/elasticsearch_exporter#metrics Golang Process information (e.g., CPU, memory, GC status). Prometheus \u00b6 Summary: \"Does my Prometheus have enough resources? Can I take in another X-thousand / million metrics?\" Port: 9090/TCP (depends on your installation) Path: /metrics Auth: None. (Recommendation: Add Kubernetes OAuth Proxy in front) What can the Metrics tell us: Is my Prometheus doing \"Okay\", e.g., have enough resources Can be used to see if the Prometheus is able to take in another X-thousand / million metrics. Golang Process information (e.g., CPU, memory, GC status). References \u00b6 Prometheus ClusterRole \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : prometheus-infra rules : - apiGroups : - \"\" resources : - nodes/metrics verbs : - get - nonResourceURLs : - /metrics - /metrics/cadvisor verbs : - get Prometheus Per-Namespace Role \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : prometheus namespace : YOUR_NAMESPACE rules : - apiGroups : - \"\" resources : - services - endpoints - pods verbs : - get - list - watch Prometheus Kubernetes ETCD Scrape Job Config \u00b6 Info This selects the master Nodes based on the node-role.kubernetes.io/master label. So be sure to have it set on the master Nodes. For more information see Kubernetes Cheat Sheet - Role Label for Node objects . 1 TODO","title":"Components"},{"location":"kubernetes/monitoring/components/#master-components","text":"","title":"Master Components"},{"location":"kubernetes/monitoring/components/#etcd","text":"Summary: \"Why do my kubectl commands take so long?\" Port: 2379/TCP Path: /metrics Auth: Client Certificate Notes: Thanks to etcd having a role concept, can be a \"separate\" user with just metrics access. Additionally one might want to run a Kubernetes authenticated OAuth proxy in front of th etcd so that the (one or more) Prometheus can be granted access to it by RoleBinding a Role to the ServiceAccount. E.g., you can create a Secret with the ETCD certificate and key using the kubectl create secret generic --from-file=.../ca.crt --from-file=.../monitoring.crt --from-file=.../monitoring.key Be sure to mount that Secret inside your Prometheus instance and adjust the path(s) according to the mountPath . Prometheus Config Scrape Job Reference, see References Prometheus Kubernetes ETCD Scrape Job Config . What can the Metrics tell us: ETCD disk write latencies (they should be low, very low). ETCD Quorum status as well (+ some other metrics, e.g., how many requests / streams). Golang Process information (e.g., CPU, memory, GC status).","title":"etcd"},{"location":"kubernetes/monitoring/components/#kube-apiserver","text":"Summary: \"Why do my kubectl commands take so long?\" Port: 6443/TCP (depends on your installation) Path: /metrics Auth: ServiceAccount token. Note(s): RBAC ClusterRole needed, see References Prometheus ClusterRole and References Prometheus Per-Namespace Role . What can the Metrics tell us: API Request latencies (kubectl, operators, scheduler, controller, etc). Best used with histogram_quantile() Prometheus func over some time. Admission step and webhook controller durations and latencies. ETCD request metrics. API server cache metrics. Kubernetes API Rest Client latency, requests and duration metrics (own requests made). Golang Process information (e.g., CPU, memory, GC status).","title":"kube-apiserver"},{"location":"kubernetes/monitoring/components/#kube-scheduler","text":"Summary: \"How long do my Pods take to be scheudled?\" (Hints at \"you need to change your scheduler config\") Port: 443/TCP (depends on your installation) Path: /metrics Auth: ServiceAccount token. Note(s): RBAC ClusterRole needed, see References Prometheus ClusterRole and References Prometheus Per-Namespace Role . The kube-scheduler needs to either listen on :: ( 0.0.0.0 ) or have a proxy which is available to Prometheus for scraping running. What can the Metrics tell us: \"How long do my Pods take to be scheudled?\" ( scheduler_binding_* ) Pod Preemption Algorithm latencies and more. Volume scheduling duration. Kubernetes API Rest Client latency, requests and duration metrics (own requests made to the API). Golang Process information (e.g., CPU, memory, GC status).","title":"kube-scheduler"},{"location":"kubernetes/monitoring/components/#kube-controller-manager","text":"Info If the cloud controller is used, it should also be monitored / scraped for metrics. Summary: \"How long do my Pods take to be scheudled?\" (Hints at \"you need to change your scheduler config\") Port: 443/TCP (depends on your installation) Path: /metrics Auth: ServiceAccount token. Note(s): RBAC ClusterRole needed, see References Prometheus ClusterRole and References Prometheus Per-Namespace Role . The kube-controller-manager needs to either listen on :: ( 0.0.0.0 ) or have a proxy which is available to Prometheus for scraping running. What can the Metrics tell us: Work queue item count and duration, and lease holder status. Kubernetes API Rest Client latency, requests and duration metrics (own requests made to the API). Golang Process information (e.g., CPU, memory, GC status).","title":"kube-controller-manager"},{"location":"kubernetes/monitoring/components/#kubelet-cadvisor","text":"See Node Components - kubelet .","title":"kubelet (+ cadvisor)"},{"location":"kubernetes/monitoring/components/#kube-proxy","text":"See Node Components - kube-proxy .","title":"kube-proxy"},{"location":"kubernetes/monitoring/components/#sdn-eg-calico-cilium","text":"See Node Components - kube-proxy .","title":"SDN (e.g., Calico, Cilium)"},{"location":"kubernetes/monitoring/components/#node-components","text":"","title":"Node Components"},{"location":"kubernetes/monitoring/components/#kubelet-cadvisor_1","text":"Summary: \"How long do my Pods take to be scheudled?\" (Hints at \"you need to change your scheduler config\") Port: 443/TCP (depends on your installation) Path: /metrics Auth: ServiceAccount token. Note(s): RBAC ClusterRole needed, see References Prometheus ClusterRole and References Prometheus Per-Namespace Role . kubelet needs to have the following flags active: --authorization-mode=Webhook and --authentication-token-webhook=true . What can the Metrics tell us: kubelet_node_config_error good to know if the latest config works. Kubelet PLEG (\"container runtime status\") metrics. \"Pod Start times\" (use histogram_quantile() ). CGroup metrics of containers (cpu, memory, network) with Namespace and Pod labels. Kubernetes API Rest Client latency, requests and duration metrics (own requests made to the API). Golang Process information (e.g., CPU, memory, GC status).","title":"kubelet (+ \"cadvisor\")"},{"location":"kubernetes/monitoring/components/#kube-proxy_1","text":"Summary: \"How long do my Pods take to be scheudled?\" (Hints at \"you need to change your scheduler config\") Port: 10250/TCP (depends on your installation) Path: /metrics Auth: ServiceAccount token. Note(s): RBAC ClusterRole needed, see References Prometheus ClusterRole and References Prometheus Per-Namespace Role . What can the Metrics tell us: iptables and / or ipvs sync information (~= how long does it take for Service changes to be reflected in the \"routing\" rules). Kubernetes API Rest Client latency, requests and duration metrics (own requests made to the API). Golang Process information (e.g., CPU, memory, GC status).","title":"kube-proxy"},{"location":"kubernetes/monitoring/components/#sdn-cni-eg-calico-cilium","text":"Depends on the SDN / CNI used, if there are metrics available. Calico for example can expose metrics, but that must be enabled through a environemnt variable on the Calico Node DaemonSet. For other SDNs, e.g., OpenVSwitch you may need to use an \"external\" exporter when available: https://github.com/digitalocean/openvswitch_exporter https://github.com/ovnworks/ovn_exporter What can the Metrics tell us: Depending on the exporter, at least how much traffic is flowing and / or if there are issues with the daemon.","title":"SDN / CNI (e.g., Calico, Cilium)"},{"location":"kubernetes/monitoring/components/#the-nodes-themself","text":"","title":"The Nodes themself"},{"location":"kubernetes/monitoring/components/#node_exporter","text":"See Monitoring/Prometheus/Exporters - node_exporter .","title":"node_exporter"},{"location":"kubernetes/monitoring/components/#ethtool_exporter","text":"See Monitoring/Prometheus/Exporters - ethtool_exporter .","title":"ethtool_exporter"},{"location":"kubernetes/monitoring/components/#additional-in-cluster-components","text":"Other components that are in and / or around a Kubernetes cluster.","title":"Additional In-Cluster Components"},{"location":"kubernetes/monitoring/components/#metrics-server-previously-named-heapster","text":"(More information https://kubernetes.io/docs/tasks/debug-application-cluster/resource-metrics-pipeline/ ) Summary: \"How long do my Pods take to be scheudled?\" (Hints at \"you need to change your scheduler config\") Port: 443/TCP Path: /metrics Auth: None. (Recommendation: Add Kubernetes OAuth Proxy in front) What can the Metrics tell us: Possibly metrics on how Golang Process information (e.g., CPU, memory, GC status).","title":"metrics-server (previously named heapster)"},{"location":"kubernetes/monitoring/components/#kube-state-metrics","text":"Summary: \"How long do my Pods take to be scheudled?\" (Hints at \"you need to change your scheduler config\") Port: 8080/TCP for \"cluster\" metrics and 8081/TCP for kube-state-metrics metrics. (Recommended to scrape both) Path: /metrics Auth: ServiceAccount token. Note(s): RBAC ClusterRole needed, see References Prometheus ClusterRole and References Prometheus Per-Namespace Role . What can the Metrics tell us: Metrics about Deployments, StatefulSets, Jobs, CronJobs, and basically any other objects Status (that is in the official Kubernetes APIs). Golang Process information (e.g., CPU, memory, GC status).","title":"kube-state-metrics"},{"location":"kubernetes/monitoring/components/#other-components","text":"","title":"Other Components"},{"location":"kubernetes/monitoring/components/#elasticsearch","text":"Elasticsearch is not providing Prometheus metrics itself, but there is a well written exporter GitHub justwatchcom/elasticsearch_exporter . (There are some other exporters also available, though I have used mainly used this one for the amount of metrics I'm able to get from Elasticsearch with it) Summary: \"Is my Elasticsearch able to ingest the amount of logs? Do I need to add more data nodes and / or resources?\" Port: 9114/TCP (depends on your installation) Path: /metrics Auth: ServiceAccount token Note(s): RBAC ClusterRole needed, see References Prometheus ClusterRole and References Prometheus Per-Namespace Role . What can the Metrics tell us: Elasticsearch cluster status. Resources (CPU and Memory) and also Storage usage. Elasticsearch Indices status (when enabled (can be filtered in different ways)). Elasticsearch JVM info (GC, memory, etc). Checkout the metrics list for a list of all Metrics available: https://github.com/justwatchcom/elasticsearch_exporter#metrics Golang Process information (e.g., CPU, memory, GC status).","title":"Elasticsearch"},{"location":"kubernetes/monitoring/components/#prometheus","text":"Summary: \"Does my Prometheus have enough resources? Can I take in another X-thousand / million metrics?\" Port: 9090/TCP (depends on your installation) Path: /metrics Auth: None. (Recommendation: Add Kubernetes OAuth Proxy in front) What can the Metrics tell us: Is my Prometheus doing \"Okay\", e.g., have enough resources Can be used to see if the Prometheus is able to take in another X-thousand / million metrics. Golang Process information (e.g., CPU, memory, GC status).","title":"Prometheus"},{"location":"kubernetes/monitoring/components/#references","text":"","title":"References"},{"location":"kubernetes/monitoring/components/#prometheus-clusterrole","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : prometheus-infra rules : - apiGroups : - \"\" resources : - nodes/metrics verbs : - get - nonResourceURLs : - /metrics - /metrics/cadvisor verbs : - get","title":"Prometheus ClusterRole"},{"location":"kubernetes/monitoring/components/#prometheus-per-namespace-role","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : prometheus namespace : YOUR_NAMESPACE rules : - apiGroups : - \"\" resources : - services - endpoints - pods verbs : - get - list - watch","title":"Prometheus Per-Namespace Role"},{"location":"kubernetes/monitoring/components/#prometheus-kubernetes-etcd-scrape-job-config","text":"Info This selects the master Nodes based on the node-role.kubernetes.io/master label. So be sure to have it set on the master Nodes. For more information see Kubernetes Cheat Sheet - Role Label for Node objects . 1 TODO","title":"Prometheus Kubernetes ETCD Scrape Job Config"},{"location":"kubernetes/networking/benchmarking/","text":"Network \u00b6 https://github.com/kubernetes/perf-tests/tree/master/network/benchmarks/netperf DNS \u00b6 https://github.com/DNS-OARC/flamethrower","title":"Benchmarking"},{"location":"kubernetes/networking/benchmarking/#network","text":"https://github.com/kubernetes/perf-tests/tree/master/network/benchmarks/netperf","title":"Network"},{"location":"kubernetes/networking/benchmarking/#dns","text":"https://github.com/DNS-OARC/flamethrower","title":"DNS"},{"location":"kubernetes/networking/explained/","text":"Assumptions \u00b6 Basic network knowledge (e.g. CIDR, Source and Destination NAT) Basic iptables knowledge Pod/Cluster CIDR: 100.64.0.0/13 Every node gets a /24 podCIDR . Service IP CIDR: 100.72.0.0/16 Node IP CIDR: 10.10.10.0/24 More info on IP Classless Inter-Domain Routing (short CIDR): IPv4 Classless Inter-Domain Routing IPv6 Classless Inter-Domain Routing The source for the diagrams, can be found as .graphml at the same path as the images. Example : kubernetes-networking-explained-network_stack.svg -> kubernetes-networking-explained-network_stack.graphml Network Stack \u00b6 Network Overview caption Traffic Flow \u00b6 Pod to Pod \u00b6 Pod to Pod Traffic Pod to Service IP \u00b6 Pod to Service IP Service IP iptables \u00b6 Service IP iptables NodePort to Service IP to Pod \u00b6 NodePort to Service IP to Pod","title":"Explained"},{"location":"kubernetes/networking/explained/#assumptions","text":"Basic network knowledge (e.g. CIDR, Source and Destination NAT) Basic iptables knowledge Pod/Cluster CIDR: 100.64.0.0/13 Every node gets a /24 podCIDR . Service IP CIDR: 100.72.0.0/16 Node IP CIDR: 10.10.10.0/24 More info on IP Classless Inter-Domain Routing (short CIDR): IPv4 Classless Inter-Domain Routing IPv6 Classless Inter-Domain Routing The source for the diagrams, can be found as .graphml at the same path as the images. Example : kubernetes-networking-explained-network_stack.svg -> kubernetes-networking-explained-network_stack.graphml","title":"Assumptions"},{"location":"kubernetes/networking/explained/#network-stack","text":"Network Overview caption","title":"Network Stack"},{"location":"kubernetes/networking/explained/#traffic-flow","text":"","title":"Traffic Flow"},{"location":"kubernetes/networking/explained/#pod-to-pod","text":"Pod to Pod Traffic","title":"Pod to Pod"},{"location":"kubernetes/networking/explained/#pod-to-service-ip","text":"Pod to Service IP","title":"Pod to Service IP"},{"location":"kubernetes/networking/explained/#service-ip-iptables","text":"Service IP iptables","title":"Service IP iptables"},{"location":"kubernetes/networking/explained/#nodeport-to-service-ip-to-pod","text":"NodePort to Service IP to Pod","title":"NodePort to Service IP to Pod"},{"location":"kubernetes/networking/troubleshooting/","text":"General connectivity test \u00b6 Kuberang is a simple but efficient way to quickly check cluster network connectivity. To quote from the project's README: Quote from GitHub apprenda/kuberang README.md : Has kubectl installed correctly with access controls Has active kubernetes namespace (if specified) Has available workers Has working pod & service networks Has working pod <-> pod DNS Has working master(s) Has the ability to access pods and services from the node you run it on. You can run it: 1 kuberang This will start the test Deployments and Services. DNS Server \u00b6 https://github.com/DNS-OARC/flamethrower","title":"Troubleshooting"},{"location":"kubernetes/networking/troubleshooting/#general-connectivity-test","text":"Kuberang is a simple but efficient way to quickly check cluster network connectivity. To quote from the project's README: Quote from GitHub apprenda/kuberang README.md : Has kubectl installed correctly with access controls Has active kubernetes namespace (if specified) Has available workers Has working pod & service networks Has working pod <-> pod DNS Has working master(s) Has the ability to access pods and services from the node you run it on. You can run it: 1 kuberang This will start the test Deployments and Services.","title":"General connectivity test"},{"location":"kubernetes/networking/troubleshooting/#dns-server","text":"https://github.com/DNS-OARC/flamethrower","title":"DNS Server"},{"location":"linux/mdam/","text":"Note Don't forget to keep your mdadm.conf uptodate when creating, modifiying, deleting mdadm arrays. Generate mdadm.conf \u00b6 1 mdadm --detail --scan >> /etc/mdadm.conf Grow RAID 5 to RAID 6 \u00b6 Danger DON'T FORGET TO SPECIFY THE --backup-file=FILE for mdadm --grow operations! Otherwise if the host is (forced) shutdowned (e.g., power failure), data can / will be lost. (This \"backup file\" should be on different disk / storage, not on the mdadm array you are growing!) Speed up RAID rebuild \u00b6 Note This may or may not improve your mdadm RAID rebuild performance. This assumes your disks are sda , sdb and sdc , and the RAID array is md0 ( /dev/md0 ). 1 2 3 4 5 6 7 8 9 10 11 for disk in sd { a..c } ; do blockdev --setra 16384 \"/dev/ ${ disk } \" echo 1024 > \"/sys/block/ ${ disk } /queue/read_ahead_kb\" echo 256 > \"/sys/block/ ${ disk } /queue/nr_requests\" # Disable NCQ on all disks. echo 1 > \"/sys/block/ ${ disk } /device/queue_depth\" done # Set read-ahead to 64 MiB for /dev/md0 blockdev --setra 65536 /dev/md0 # Set stripe_cache_size to 16 MiB for /dev/md0 echo 16384 > /sys/block/md0/md/stripe_cache_size References \u00b6 mdadm man page","title":"mdadm"},{"location":"linux/mdam/#generate-mdadmconf","text":"1 mdadm --detail --scan >> /etc/mdadm.conf","title":"Generate mdadm.conf"},{"location":"linux/mdam/#grow-raid-5-to-raid-6","text":"Danger DON'T FORGET TO SPECIFY THE --backup-file=FILE for mdadm --grow operations! Otherwise if the host is (forced) shutdowned (e.g., power failure), data can / will be lost. (This \"backup file\" should be on different disk / storage, not on the mdadm array you are growing!)","title":"Grow RAID 5 to RAID 6"},{"location":"linux/mdam/#speed-up-raid-rebuild","text":"Note This may or may not improve your mdadm RAID rebuild performance. This assumes your disks are sda , sdb and sdc , and the RAID array is md0 ( /dev/md0 ). 1 2 3 4 5 6 7 8 9 10 11 for disk in sd { a..c } ; do blockdev --setra 16384 \"/dev/ ${ disk } \" echo 1024 > \"/sys/block/ ${ disk } /queue/read_ahead_kb\" echo 256 > \"/sys/block/ ${ disk } /queue/nr_requests\" # Disable NCQ on all disks. echo 1 > \"/sys/block/ ${ disk } /device/queue_depth\" done # Set read-ahead to 64 MiB for /dev/md0 blockdev --setra 65536 /dev/md0 # Set stripe_cache_size to 16 MiB for /dev/md0 echo 16384 > /sys/block/md0/md/stripe_cache_size","title":"Speed up RAID rebuild"},{"location":"linux/mdam/#references","text":"mdadm man page","title":"References"},{"location":"linux/quick-commands/","text":"Archives \u00b6 Extract all '*.zip' files into a directory named after the zip's filename \u00b6 1 find -name '*.zip' -exec sh -c 'unzip -d \"${1%.*}\" \"$1\"' _ {} \\; Extract all '*.rar' files into a directry named after the rar's filename \u00b6 1 find -name '*.rar' -exec sh -c 'mkdir \"${1%.*}\"; unrar e \"$1\" \"${1%.*}\"' _ {} \\; Extract all '*.7z' files into a directry named after the rar's filename \u00b6 1 find -name '*.7z' -exec sh -c 'mkdir \"${1%.*}\"; 7z x \"$1\" -o\"${1%.*}\"' _ {} \\; Extrat all '*.tar' files into a directory named after the tar's filename \u00b6 1 find -name '*.tar' -exec sh -c 'mkdir -p \"${1%.*}\"; tar -C \"${1%.*}\" -xvf \"$1\"' _ {} \\; Extrat all '*.tar.gz' files into a directory named after the tar's filename \u00b6 1 find -name '*.tar.gz' -or -name '*.tgz' -exec sh -c 'mkdir -p \"${1%.*}\"; tar -C \"${1%.*}\" -xvzf \"$1\"' _ {} \\; Music \u00b6 Convert all FLAC to MP3 (same directory) \u00b6 1 find -name \"*.flac\" -print | parallel -j 14 ffmpeg -i {} -acodec libmp3lame -ab 192k { . } .mp3 \\; Convert all OGG to FLAC (same directory) \u00b6 1 find -name \"*.ogg\" -print | parallel -j 14 ffmpeg -i {} -c:a flac { . } .flac \\; Documents \u00b6 Convert PDFs to PNGs (each page is its own image) \u00b6 1 2 3 4 5 for file in *.pdf ; do echo \"Processing file: $file ...\" mkdir -p \" $( basename \" $file \" .pdf ) \" pdftoppm -png \" $file \" \" $( basename \" $file \" .pdf ) /page\" done Run tesseract OCR on all converted Pages \u00b6 1 2 3 4 for file in */*.png ; do echo \"Processing file: $file ...\" tesseract -l deu+eng \" $file \" \" $( echo \" $file \" | sed 's/\\.png$//g' ) \" done Info The -l deu+eng are the languages to use. In this case deu+eng means \"Deutsch\" (German) and \"English\". Convert all '*.docx' files into PDFs (using LibreOffice's lowriter ) \u00b6 1 2 3 4 5 find . -name '*.docx' -print0 | while IFS = read -r -d $'\\0' line ; do echo \"Processing file: $line ...\" lowriter --convert-to pdf \" $line \" --outdir \" $( dirname \" $line \" ) \" done Disks \u00b6 Get UUID for partition \u00b6 1 blkid /dev/sdXY -s UUID -o value Where /dev/sdXY could be, /dev/sda2 , /dev/nvme0n1p1 , and so on.","title":"Quick Commands"},{"location":"linux/quick-commands/#archives","text":"","title":"Archives"},{"location":"linux/quick-commands/#extract-all-zip-files-into-a-directory-named-after-the-zips-filename","text":"1 find -name '*.zip' -exec sh -c 'unzip -d \"${1%.*}\" \"$1\"' _ {} \\;","title":"Extract all '*.zip' files into a directory named after the zip's filename"},{"location":"linux/quick-commands/#extract-all-rar-files-into-a-directry-named-after-the-rars-filename","text":"1 find -name '*.rar' -exec sh -c 'mkdir \"${1%.*}\"; unrar e \"$1\" \"${1%.*}\"' _ {} \\;","title":"Extract all '*.rar' files into a directry named after the rar's filename"},{"location":"linux/quick-commands/#extract-all-7z-files-into-a-directry-named-after-the-rars-filename","text":"1 find -name '*.7z' -exec sh -c 'mkdir \"${1%.*}\"; 7z x \"$1\" -o\"${1%.*}\"' _ {} \\;","title":"Extract all '*.7z' files into a directry named after the rar's filename"},{"location":"linux/quick-commands/#extrat-all-tar-files-into-a-directory-named-after-the-tars-filename","text":"1 find -name '*.tar' -exec sh -c 'mkdir -p \"${1%.*}\"; tar -C \"${1%.*}\" -xvf \"$1\"' _ {} \\;","title":"Extrat all '*.tar' files into a directory named after the tar's filename"},{"location":"linux/quick-commands/#extrat-all-targz-files-into-a-directory-named-after-the-tars-filename","text":"1 find -name '*.tar.gz' -or -name '*.tgz' -exec sh -c 'mkdir -p \"${1%.*}\"; tar -C \"${1%.*}\" -xvzf \"$1\"' _ {} \\;","title":"Extrat all '*.tar.gz' files into a directory named after the tar's filename"},{"location":"linux/quick-commands/#music","text":"","title":"Music"},{"location":"linux/quick-commands/#convert-all-flac-to-mp3-same-directory","text":"1 find -name \"*.flac\" -print | parallel -j 14 ffmpeg -i {} -acodec libmp3lame -ab 192k { . } .mp3 \\;","title":"Convert all FLAC to MP3 (same directory)"},{"location":"linux/quick-commands/#convert-all-ogg-to-flac-same-directory","text":"1 find -name \"*.ogg\" -print | parallel -j 14 ffmpeg -i {} -c:a flac { . } .flac \\;","title":"Convert all OGG to FLAC (same directory)"},{"location":"linux/quick-commands/#documents","text":"","title":"Documents"},{"location":"linux/quick-commands/#convert-pdfs-to-pngs-each-page-is-its-own-image","text":"1 2 3 4 5 for file in *.pdf ; do echo \"Processing file: $file ...\" mkdir -p \" $( basename \" $file \" .pdf ) \" pdftoppm -png \" $file \" \" $( basename \" $file \" .pdf ) /page\" done","title":"Convert PDFs to PNGs (each page is its own image)"},{"location":"linux/quick-commands/#run-tesseract-ocr-on-all-converted-pages","text":"1 2 3 4 for file in */*.png ; do echo \"Processing file: $file ...\" tesseract -l deu+eng \" $file \" \" $( echo \" $file \" | sed 's/\\.png$//g' ) \" done Info The -l deu+eng are the languages to use. In this case deu+eng means \"Deutsch\" (German) and \"English\".","title":"Run tesseract OCR on all converted Pages"},{"location":"linux/quick-commands/#convert-all-docx-files-into-pdfs-using-libreoffices-lowriter","text":"1 2 3 4 5 find . -name '*.docx' -print0 | while IFS = read -r -d $'\\0' line ; do echo \"Processing file: $line ...\" lowriter --convert-to pdf \" $line \" --outdir \" $( dirname \" $line \" ) \" done","title":"Convert all '*.docx' files into PDFs (using LibreOffice's lowriter)"},{"location":"linux/quick-commands/#disks","text":"","title":"Disks"},{"location":"linux/quick-commands/#get-uuid-for-partition","text":"1 blkid /dev/sdXY -s UUID -o value Where /dev/sdXY could be, /dev/sda2 , /dev/nvme0n1p1 , and so on.","title":"Get UUID for partition"},{"location":"linux/sysctl/","text":"See GitHub Gist 90-edenmal-custom.conf for more information on the used sysctl settings / values. The sysctl can be easily using the following command: 1 2 curl -L https://gist.githubusercontent.com/galexrt/8faa48a05bab303ec922bd89e8f7adc5/raw/90-edenmal-custom.conf -o /etc/sysctl.d/90-edenmal-custom.conf sysctl --system Info The below list might be outdated, please check the GitHub Gist linked above for the latest version. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 fs.aio_max_nr = 1048576 fs.file-max = 2097152 fs.inotify.max_user_instances = 5120 fs.inotify.max_user_watches = 1572864 fs.nr_open = 3145728 fs.suid_dumpable = 0 kernel.core_uses_pid = 1 kernel.dmesg_restrict = 1 kernel.exec-shield = 2 kernel.panic_on_oops = 1 kernel.panic = 10 kernel.pid_max = 4194303 kernel.randomize_va_space = 2 kernel.sched_autogroup_enabled = 0 kernel.sched_migration_cost = 5000000 kernel.sysrq = 0 net.core.default_qdisc = fq net.core.netdev_budget = 600 net.core.netdev_max_backlog = 65536 net.core.optmem_max = 4048000 net.core.rmem_default = 266240 net.core.rmem_max = 4048000 net.core.somaxconn = 65536 net.core.wmem_default = 266240 net.core.wmem_max = 4048000 net.ipv4.conf.all.accept_redirects = 0 net.ipv4.conf.all.accept_source_route = 0 net.ipv4.conf.all.bootp_relay = 0 net.ipv4.conf.all.forwarding = 1 net.ipv4.conf.all.igmpv2_unsolicited_report_interval = 10000 net.ipv4.conf.all.igmpv3_unsolicited_report_interval = 1000 net.ipv4.conf.all.ignore_routes_with_linkdown = 0 net.ipv4.conf.all.log_martians = 1 net.ipv4.conf.all.proxy_arp = 0 net.ipv4.conf.all.rp_filter = 2 net.ipv4.conf.all.secure_redirects = 1 net.ipv4.conf.all.send_redirects = 0 net.ipv4.conf.default.accept_redirects = 0 net.ipv4.conf.default.accept_source_route = 0 net.ipv4.conf.default.forwarding = 1 net.ipv4.conf.default.log_martians = 1 net.ipv4.conf.default.rp_filter = 2 net.ipv4.conf.default.secure_redirects = 1 net.ipv4.conf.default.send_redirects = 0 net.ipv4.conf.lo.accept_source_route = 1 net.ipv4.fwmark_reflect = 0 net.ipv4.icmp_echo_ignore_all = 0 net.ipv4.icmp_echo_ignore_broadcasts = 1 net.ipv4.icmp_ignore_bogus_error_responses = 1 net.ipv4.icmp_msgs_burst = 50 net.ipv4.icmp_msgs_per_sec = 1000 net.ipv4.ip_forward = 1 net.ipv4.ip_local_port_range = 1024 65535 net.ipv4.ipfrag_secret_interval = 600 net.ipv4.neigh.default.gc_thresh1 = 4048 net.ipv4.neigh.default.gc_thresh2 = 6144 net.ipv4.neigh.default.gc_thresh3 = 8192 net.ipv4.netfilter.ip_conntrack_tcp_timeout_syn_recv = 45 net.ipv4.netfilter.nf_conntrack_generic_timeout = 300 net.ipv4.netfilter.nf_conntrack_tcp_timeout_time_wait = 60 net.ipv4.tcp_abort_on_overflow = 1 net.ipv4.tcp_congestion_control = bbr net.ipv4.tcp_fin_timeout = 10 net.ipv4.tcp_keepalive_intvl = 25 net.ipv4.tcp_keepalive_probes = 5 net.ipv4.tcp_keepalive_time = 420 net.ipv4.tcp_max_syn_backlog = 4096 net.ipv4.tcp_max_tw_buckets = 160000 net.ipv4.tcp_moderate_rcvbuf = 1 net.ipv4.tcp_no_metrics_save = 1 net.ipv4.tcp_notsent_lowat = 16384 net.ipv4.tcp_rfc1337 = 1 net.ipv4.tcp_rmem = 4096 87380 8388608 net.ipv4.tcp_sack = 1 net.ipv4.tcp_slow_start_after_idle = 0 net.ipv4.tcp_syn_retries = 2 net.ipv4.tcp_synack_retries = 3 net.ipv4.tcp_syncookies = 1 net.ipv4.tcp_timestamps = 1 net.ipv4.tcp_tw_recycle = 0 net.ipv4.tcp_tw_reuse = 1 net.ipv4.tcp_window_scaling = 1 net.ipv4.tcp_wmem = 4096 87380 8388608 net.ipv4.udp_rmem_min = 16384 net.ipv4.udp_wmem_min = 16384 net.ipv4.vs.conn_reuse_mode = 1 net.ipv4.vs.conntrack = 1 net.ipv4.vs.expire_nodest_conn = 1 net.ipv4.vs.sloppy_tcp = 1 net.ipv6.conf.all.accept_ra_defrtr = 0 net.ipv6.conf.all.accept_ra_pinfo = 0 net.ipv6.conf.all.accept_ra = 0 net.ipv6.conf.all.accept_redirects = 0 net.ipv6.conf.all.accept_source_route = 0 net.ipv6.conf.all.forwarding = 1 net.ipv6.conf.default.accept_ra_defrtr = 0 net.ipv6.conf.default.accept_ra_pinfo = 0 net.ipv6.conf.default.accept_ra_rtr_pref = 0 net.ipv6.conf.default.accept_redirects = 0 net.ipv6.conf.default.accept_source_route = 0 net.ipv6.conf.default.autoconf = 0 net.ipv6.conf.default.dad_transmits = 0 net.ipv6.conf.default.forwarding = 1 net.ipv6.conf.default.max_addresses = 16 net.ipv6.conf.default.router_solicitations = 0 net.ipv6.ip6frag_secret_interval = 600 net.ipv6.route.max_size = 16384 net.ipv6.xfrm6_gc_thresh = 32768 net.netfilter.nf_conntrack_expect_max = 2048 net.netfilter.nf_conntrack_max = 1024000 net.netfilter.nf_conntrack_tcp_timeout_established = 600 net.nf_conntrack_max = 1024000 vm.mmap_rnd_bits = 32 vm.mmap_rnd_compat_bits = 16 vm.overcommit_memory = 1 vm.overcommit_ratio = 20 vm.panic_on_oom = 0","title":"sysctl"},{"location":"linux/GRUB/preparations-for-boot-xyz-file/","text":"Danger Only run the following commands if you know what they are doing! If you already have GRUB installed and working, you probably just need to edit your grub.cfg file (for most OSes in the /boot directory). The disk to be used should be: Boot Target in BIOS / UEFI First in the BIOS / UEFI boot order The disk used in this example is /dev/sda . Prepare Disk partition layout Create two partitions on the disk (use, e.g., fdisk , parted , etc) 1MB (flags: boot ) 10G or more as you want / need (flags: boot ), ext4 formatted Run blkid /dev/sda2 -s UUID -o value to get the UUID of the \"first\" disk's second partition. Save the UUID of the \"first\" disk down. The UUID of the \"first\" disk will be used in form of the __BOOT_PART_UUID__ later on. Mount \"boot\" Partition Mount the second created partition ( /dev/sda2 ): mount /dev/sda2 /boot . Download Fedora vmlinuz , initramfs and install.img to /boot directory. Prefix the downloaded files with fedora- (or whatever you want as long as you change it in the upcoming steps as well) Grub Installation For GRUB2: 1 grub2-install --no-floppy /dev/sda2 For GRUB: 1 grub-install --no-floppy /dev/sda2 Create GRUB boot config file For GRUB2 the path is /boot/grub2/grub.cfg . For GRUB the path is /boot/grub/grub.cfg . Optional steps: Copy your Kickstart and / or config file to the /boot directory This assumes the \"system\" you are using is able to open the /boot mounted partition and read the file from there, e.g., Kickstart can do it like that inst.ks=\"hd:UUID=__BOOT_PART_UUID__:/ks.cfg\" (where the __BOOT_PART_UUID__ is the partition UUID). Reboot and enjoy!","title":"Preparations for 'Boot XYZ FIle'"},{"location":"linux/GRUB/Boot-XYZ-File/img-File/","text":"Info You must have a working GRUB installation already, if not checkout the Preparations for Boot . Example grub.cfg (for Fedora CoreOS installation): 1 2 3 4 5 6 7 8 9 set default = 0 set timeout = 5 # Fedora Kickstart Install menuentry \"Fedora Kickstart Install\" { search --no-floppy --fs-uuid __BOOT_PART_UUID__ --set root # Add aditional kernel command line arguments at the end of the next line linux /fedora-vmlinuz selinux = 0 inst.resolution=800x600 inst.ks=\"hd:UUID=__BOOT_PART_UUID__:/ks.cfg\" inst.stage2=\"hd:UUID=__BOOT_PART_UUID__:/fedora-install.img\" initrd /fedora-initrd.img }","title":".img File"},{"location":"linux/GRUB/Boot-XYZ-File/iso-File/","text":"Info You must have a working GRUB installation already, if not checkout the Preparations for Boot . Why would you need this? When you don't have a KVM or just don't want to use ILO, iDRAC, iPMI or other management tools to mount an ISO. This snippet is from a try to install Fedora CoreOS from an ISO file named install.io from the /boot partition. Example grub.cfg contents ( coreos.inst.install_dev=sda would use the sda device for installation in case of *CoreOS): 1 2 3 4 5 6 7 8 9 10 set default = 0 set timeout = 5 # Fedora Kickstart Install menuentry \"Fedora Kickstart Install\" { search --no-floppy --fs-uuid __BOOT_PART_UUID__ --set root # Booting an ISO loopback loop /install.iso linux (loop)/images/vmlinuz coreos.inst = yes coreos.inst.install_dev=sda coreos.inst.ignition_url=http://example.com/example.ign initrd (loop)/images/initramfs.img } It worked fine though the original case was to load the Ignition file from the boot disk which didn't work, a Webserver / Matchbox server was required to load the Ignition file from.","title":".iso File"},{"location":"logging/loki/","text":"Coming Soon If you can use Loki instead of Elasticsearch, just do it! From an operational perspective Loki can be run with a lot less resources than a whole EFK (Elasticsearch Fluentd Kibana) stack. It just feels better not having to \"spend\" 3 servers to just run an Elasticsearch cluster for \"some logs\" which maybe \"once a month\" someone needs to take a look to investigate an issue with a Pod.","title":"Loki"},{"location":"monitoring/thanos/","text":"More info coming Soon Important Hints \u00b6 Thanos Compactor As of writing this, the compactor unless \"configured\" in a certain way must only be run once at the same time ( NO PARALLEL )!","title":"Thanos"},{"location":"monitoring/thanos/#important-hints","text":"Thanos Compactor As of writing this, the compactor unless \"configured\" in a certain way must only be run once at the same time ( NO PARALLEL )!","title":"Important Hints"},{"location":"monitoring/prometheus/overview/","text":"","title":"Overview"},{"location":"monitoring/prometheus/tips/","text":"Multiple Exporter on one Server but only one port? \u00b6 (Let's not talk about the reasons, why one only has \"one\" port for such a case..) The following projects can be quite useful to accomplish this: https://github.com/rebuy-de/exporter-merger","title":"Tips"},{"location":"monitoring/prometheus/tips/#multiple-exporter-on-one-server-but-only-one-port","text":"(Let's not talk about the reasons, why one only has \"one\" port for such a case..) The following projects can be quite useful to accomplish this: https://github.com/rebuy-de/exporter-merger","title":"Multiple Exporter on one Server but only one port?"},{"location":"monitoring/prometheus/exporters/dellhw_exporter/","text":"Website / Source Code: https://github.com/galexrt/dellhw_exporter Port: 9137/TCP Path: /metrics Auth: None. (Recommendation: Add (Kubernetes) OAuth Proxy in front) What can the Metrics tell us: Dell HW status through the use of Dell's OMSA omreport tool. The dellhw_exporter uses Dell Open Manage Server Administrator omreport to get metrics from Dell hardware. For more info, visit the GitHub repository: https://github.com/galexrt/dellhw_exporter . Info The exporter is written by the project contributors, initially created by Alexander Trost .","title":"dellhw_exporter by galexrt"},{"location":"monitoring/prometheus/exporters/ethtool_exporter/","text":"Info \u00b6 Website / Source Code: https://github.com/Showmax/prometheus-ethtool-exporter Port: 9417/TCP (can also export to prom textilfe format) Path: /metrics Auth: None. (Recommendation: Add (Kubernetes) OAuth Proxy in front) What can the Metrics tell us: Interface information like SFP status / information and other interesting network interface details. Exports interface metrics using the ethtool tool. E.g., reports metrics such as temperature, dampening, etc., of SFP interfaces (the SFPs need to have / support Digital Diagnostic Monitoring (DDM) ).","title":"ethtool_exporter by Showmax"},{"location":"monitoring/prometheus/exporters/ethtool_exporter/#info","text":"Website / Source Code: https://github.com/Showmax/prometheus-ethtool-exporter Port: 9417/TCP (can also export to prom textilfe format) Path: /metrics Auth: None. (Recommendation: Add (Kubernetes) OAuth Proxy in front) What can the Metrics tell us: Interface information like SFP status / information and other interesting network interface details. Exports interface metrics using the ethtool tool. E.g., reports metrics such as temperature, dampening, etc., of SFP interfaces (the SFPs need to have / support Digital Diagnostic Monitoring (DDM) ).","title":"Info"},{"location":"monitoring/prometheus/exporters/node_exporter/","text":"Website / Source Code: https://github.com/prometheus/node_exporter Port: 9100/TCP Path: /metrics Auth: None. (Recommendation: Add (Kubernetes) OAuth Proxy in front) What can the Metrics tell us: OS metrics (e.g., cpu , loadavg , meminfo and many more). The exporter can export a ton of metrics for Linux based systems. Can export metrics for the following OSes: Darwin, Dragonfly, FreeBSD, Linux, NetBSD, OpenBSD, Solaris. Not all metrics are available for each OS (e.g., only Linux has mdadm metrics). For a list of metrics per OS, see https://github.com/prometheus/node_exporter#enabled-by-default and https://github.com/prometheus/node_exporter#disabled-by-default . Special point about the node_exporter is that it can export metrics from textiles that are in a certain format, see https://github.com/prometheus/node_exporter#textfile-collector . SMART metrics are normally exported like this (e.g., https://github.com/galexrt/docker-node_exporter-smartmon for a container image that runs the smartctl script every X time).","title":"node_exporter by Prometheus Project"},{"location":"monitoring/prometheus/exporters/others/","text":"Checkout the official Prometheus exporters list for many more exporters.","title":"Other exporters"},{"location":"name-schemas/kubernetes/","text":"Hostname Schema \u00b6 Domain TLD Usage \u00b6 Services: example.services Servers (bare metal, VMs, doesn't matter): example.systems Network hardware: example.network Schema Parts \u00b6 COUNTRY - ISO 3166-1 alpha-3 code, see ISO 3166-1 alpha-3 - Wikipedia and Online Browsing Platform (OBP) - ISO . PROVIDER - 3 character long abbreviation of provider name. DC - Optional info about datacenter/region (e.g., FSN1-DC1 ). CLUSTER - Cluster designation (in case of Kubernetes, should always have k8s in the beginning) and if there can be multiple a number added (with two digits, e.g., 01 , 12 ). ROLE - In case of Kubernetes, e.g., master , etcd , node (other \"special\" roles could be, e.g., ingress , stora (could have a suffix per storage software, e.g. storaceph )). COUNT_OR_ID - A count is a special \"type\". It can for servers that are known to have only a maximum of n machines at maximum, the number of servers padded with zeroes (e.g., max 12 servers results in COUNT for the third machine being 03 ), in case of nodes where there can be an undefined amount of them it should be a shortid. To generate a \"random\" ID ( requires bashids to be installed ): 1 2 3 4 5 bashids \\ -e \\ -s 'B_r1KMOASvn_5A1hDKCdPXJfrIBcddpwOnT5orXYaQPV4Ixb1zNSpa-nF6HOw8mii3pqovZUtsnGZ5pqbf59wPfeMp9XagGXc8ViJreL_5J1kvSnDCPfqvuV2bmGsx4DrVV_ef3Gr3MgCMrX86TGUjCDeJmM3LONAfKIH_vv0ZR9WWcJJbLCc5xnxWh7Is8qNq95ORIHS6iU4gKZNV-LIxdYxd7WyO2fKeOn8kApv0FFD2ydkJXdz4KjqBEcN5Fu' \\ $ ( date +%s%6N ) | \\ tr '[:upper:]' '[:lower:]' Servers / VMs Schema \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 # When country, provider and dc should be omited: { CLUSTER } - { ROLE } - { COUNT_OR_ID } .example.systems # Examples: ## Master / Primary Kubernetes node k8s02-master-01.example.systems ## Node / Worker Kubernetes node k8s02-node-4ua16bzb6r7.example.systems # With country, provider and dc in the name: { CLUSTER } - { ROLE } - { COUNT_OR_ID } - { PROVIDER } - { COUNTRY }{ DC } .example.systems # Examples: ## `deu-fsn1dc1` translates to Germany, FSN1-DC14 (Falkenstein) k8s02-master-01-htz-deu-fsn1dc14.example.systems ## `deu-fsn1dc1` translates to Germany, FSN1-DC1 (Falkenstein) k8s02-node-4ua16bzb6r7-htz-deu-fsn1dc1.example.systems ## `eee-west2` translates to GCP Region `europe-west2` k8s02-master-02-gcp-eee-west2.example.systems ## `usa-west1` translates to AWS Region `us-west1` k8s02-node-7ca16bnb2r1-aws-usa-west1.example.systems Script: Gernate Hostname (+ ID) \u00b6 1 2 3 4 5 6 7 8 9 10 CLUSTER = \"k8s02\" ROLE = \"node\" COUNT_OR_ID = \" $( sleep 0 .00001 ; \\ bashids \\ -e \\ -s 'B_r1KMOASvn_5A1hDKCdPXJfrIBcddpwOnT5orXYaQPV4Ixb1zNSpa-nF6HOw8mii3pqovZUtsnGZ5pqbf59wPfeMp9XagGXc8ViJreL_5J1kvSnDCPfqvuV2bmGsx4DrVV_ef3Gr3MgCMrX86TGUjCDeJmM3LONAfKIH_vv0ZR9WWcJJbLCc5xnxWh7Is8qNq95ORIHS6iU4gKZNV-LIxdYxd7WyO2fKeOn8kApv0FFD2ydkJXdz4KjqBEcN5Fu' \\ $( date +%s%6N ) | \\ tr '[:upper:]' '[:lower:]' ) \" echo \" $CLUSTER - $ROLE - $COUNT_OR_ID .example.systems\" echo \" $CLUSTER - $ROLE - $COUNT_OR_ID -htz-deu-fsn1dc1.example.systems\" Note The sleep 0.00001 is used to try to prevent \"duplicates\" when bashids is run in parallel to generate IDs. Services Schema \u00b6 1 2 3 4 5 6 7 8 9 {CLUSTER}-({OWNER}-){ROLE}.example.services # Examples ## Kubernetes cluster \"k8s02\" Loadbalancer owned by the \"system\" k8s02-system-lb.example.services # or the owner can be omitted in such cases k8s02-lb.example.services ## Kubernetes cluster \"k8s02\" hosted TeamSpeak service owned by customer \"gamer\" k8s02-gamer-ts3.example.services","title":"Kubernetes Name Schemas"},{"location":"name-schemas/kubernetes/#hostname-schema","text":"","title":"Hostname Schema"},{"location":"name-schemas/kubernetes/#domain-tld-usage","text":"Services: example.services Servers (bare metal, VMs, doesn't matter): example.systems Network hardware: example.network","title":"Domain TLD Usage"},{"location":"name-schemas/kubernetes/#schema-parts","text":"COUNTRY - ISO 3166-1 alpha-3 code, see ISO 3166-1 alpha-3 - Wikipedia and Online Browsing Platform (OBP) - ISO . PROVIDER - 3 character long abbreviation of provider name. DC - Optional info about datacenter/region (e.g., FSN1-DC1 ). CLUSTER - Cluster designation (in case of Kubernetes, should always have k8s in the beginning) and if there can be multiple a number added (with two digits, e.g., 01 , 12 ). ROLE - In case of Kubernetes, e.g., master , etcd , node (other \"special\" roles could be, e.g., ingress , stora (could have a suffix per storage software, e.g. storaceph )). COUNT_OR_ID - A count is a special \"type\". It can for servers that are known to have only a maximum of n machines at maximum, the number of servers padded with zeroes (e.g., max 12 servers results in COUNT for the third machine being 03 ), in case of nodes where there can be an undefined amount of them it should be a shortid. To generate a \"random\" ID ( requires bashids to be installed ): 1 2 3 4 5 bashids \\ -e \\ -s 'B_r1KMOASvn_5A1hDKCdPXJfrIBcddpwOnT5orXYaQPV4Ixb1zNSpa-nF6HOw8mii3pqovZUtsnGZ5pqbf59wPfeMp9XagGXc8ViJreL_5J1kvSnDCPfqvuV2bmGsx4DrVV_ef3Gr3MgCMrX86TGUjCDeJmM3LONAfKIH_vv0ZR9WWcJJbLCc5xnxWh7Is8qNq95ORIHS6iU4gKZNV-LIxdYxd7WyO2fKeOn8kApv0FFD2ydkJXdz4KjqBEcN5Fu' \\ $ ( date +%s%6N ) | \\ tr '[:upper:]' '[:lower:]'","title":"Schema Parts"},{"location":"name-schemas/kubernetes/#servers-vms-schema","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 # When country, provider and dc should be omited: { CLUSTER } - { ROLE } - { COUNT_OR_ID } .example.systems # Examples: ## Master / Primary Kubernetes node k8s02-master-01.example.systems ## Node / Worker Kubernetes node k8s02-node-4ua16bzb6r7.example.systems # With country, provider and dc in the name: { CLUSTER } - { ROLE } - { COUNT_OR_ID } - { PROVIDER } - { COUNTRY }{ DC } .example.systems # Examples: ## `deu-fsn1dc1` translates to Germany, FSN1-DC14 (Falkenstein) k8s02-master-01-htz-deu-fsn1dc14.example.systems ## `deu-fsn1dc1` translates to Germany, FSN1-DC1 (Falkenstein) k8s02-node-4ua16bzb6r7-htz-deu-fsn1dc1.example.systems ## `eee-west2` translates to GCP Region `europe-west2` k8s02-master-02-gcp-eee-west2.example.systems ## `usa-west1` translates to AWS Region `us-west1` k8s02-node-7ca16bnb2r1-aws-usa-west1.example.systems","title":"Servers / VMs Schema"},{"location":"name-schemas/kubernetes/#script-gernate-hostname-id","text":"1 2 3 4 5 6 7 8 9 10 CLUSTER = \"k8s02\" ROLE = \"node\" COUNT_OR_ID = \" $( sleep 0 .00001 ; \\ bashids \\ -e \\ -s 'B_r1KMOASvn_5A1hDKCdPXJfrIBcddpwOnT5orXYaQPV4Ixb1zNSpa-nF6HOw8mii3pqovZUtsnGZ5pqbf59wPfeMp9XagGXc8ViJreL_5J1kvSnDCPfqvuV2bmGsx4DrVV_ef3Gr3MgCMrX86TGUjCDeJmM3LONAfKIH_vv0ZR9WWcJJbLCc5xnxWh7Is8qNq95ORIHS6iU4gKZNV-LIxdYxd7WyO2fKeOn8kApv0FFD2ydkJXdz4KjqBEcN5Fu' \\ $( date +%s%6N ) | \\ tr '[:upper:]' '[:lower:]' ) \" echo \" $CLUSTER - $ROLE - $COUNT_OR_ID .example.systems\" echo \" $CLUSTER - $ROLE - $COUNT_OR_ID -htz-deu-fsn1dc1.example.systems\" Note The sleep 0.00001 is used to try to prevent \"duplicates\" when bashids is run in parallel to generate IDs.","title":"Script: Gernate Hostname (+ ID)"},{"location":"name-schemas/kubernetes/#services-schema","text":"1 2 3 4 5 6 7 8 9 {CLUSTER}-({OWNER}-){ROLE}.example.services # Examples ## Kubernetes cluster \"k8s02\" Loadbalancer owned by the \"system\" k8s02-system-lb.example.services # or the owner can be omitted in such cases k8s02-lb.example.services ## Kubernetes cluster \"k8s02\" hosted TeamSpeak service owned by customer \"gamer\" k8s02-gamer-ts3.example.services","title":"Services Schema"},{"location":"networking/cloudflare/","text":"Create IPv4 and IPv6 IPSets \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 # Create ipsets for IPv4 and IPv6 ipset create cf4 hash:net family inet ipset create cf6 hash:net family inet6 # Create ipset for both lists, so both IP versions can use the same list name ` cf ` ipset create cf list:set cf4 cf6 # Get the current Cloudflare IP lists for ip in $(curl https://www.cloudflare.com/ips-v4); do ipset add cf4 \"$ip\"; done for ip in $(curl https://www.cloudflare.com/ips-v6); do ipset add cf6 \"$ip\"; done Allow 80/tcp (http) and 443/tcp (https) Access to Cloudflare IPs only \u00b6 Note These iptables rules are for a stateful firewall! 1 2 3 4 iptables -A INPUT -m set --match-set cf4 src -p tcp -m multiport --dports http,https -m state --state NEW -j ACCEPT iptables -A INPUT -p tcp -m multiport --dports http,https -m state --state NEW -j DROP ip6tables -A INPUT -m set --match-set cf6 src -p tcp -m multiport --dports http,https -m state --state NEW -j ACCEPT ip6tables -A INPUT -p tcp -m multiport --dports http,https -m state --state NEW -j DROP","title":"Cloudflare"},{"location":"networking/cloudflare/#create-ipv4-and-ipv6-ipsets","text":"1 2 3 4 5 6 7 8 9 10 11 12 # Create ipsets for IPv4 and IPv6 ipset create cf4 hash:net family inet ipset create cf6 hash:net family inet6 # Create ipset for both lists, so both IP versions can use the same list name ` cf ` ipset create cf list:set cf4 cf6 # Get the current Cloudflare IP lists for ip in $(curl https://www.cloudflare.com/ips-v4); do ipset add cf4 \"$ip\"; done for ip in $(curl https://www.cloudflare.com/ips-v6); do ipset add cf6 \"$ip\"; done","title":"Create IPv4 and IPv6 IPSets"},{"location":"networking/cloudflare/#allow-80tcp-http-and-443tcp-https-access-to-cloudflare-ips-only","text":"Note These iptables rules are for a stateful firewall! 1 2 3 4 iptables -A INPUT -m set --match-set cf4 src -p tcp -m multiport --dports http,https -m state --state NEW -j ACCEPT iptables -A INPUT -p tcp -m multiport --dports http,https -m state --state NEW -j DROP ip6tables -A INPUT -m set --match-set cf6 src -p tcp -m multiport --dports http,https -m state --state NEW -j ACCEPT ip6tables -A INPUT -p tcp -m multiport --dports http,https -m state --state NEW -j DROP","title":"Allow 80/tcp (http) and 443/tcp (https) Access to Cloudflare IPs only"},{"location":"networking/cisco/acls/","text":"ACLs \u00b6 1 2 3 4 5 6 7 8 # faculty auf faculty access-list 100 permit tcp 172 .17.10.0 0 .0.0.255 172 .17.0.0 0 .0.255.255 access-list 100 permit udp 172 .17.10.0 0 .0.0.255 172 .17.0.0 0 .0.255.255 # Students auf http and https faculty access-list 101 permit tcp 172 .17.20.0 0 .0.0.255 host 172 .17.10.2 eq 80 access-list 101 permit tcp 172 .17.20.0 0 .0.0.255 host 172 .17.10.2 eq 443 # Guests block all access access-list 102 deny tcp 172 .17.30.0 0 .0.0.255 any Enter a \"sub\" interface \u00b6 1 ip access-group NUMBER in","title":"ACLs"},{"location":"networking/cisco/acls/#acls","text":"1 2 3 4 5 6 7 8 # faculty auf faculty access-list 100 permit tcp 172 .17.10.0 0 .0.0.255 172 .17.0.0 0 .0.255.255 access-list 100 permit udp 172 .17.10.0 0 .0.0.255 172 .17.0.0 0 .0.255.255 # Students auf http and https faculty access-list 101 permit tcp 172 .17.20.0 0 .0.0.255 host 172 .17.10.2 eq 80 access-list 101 permit tcp 172 .17.20.0 0 .0.0.255 host 172 .17.10.2 eq 443 # Guests block all access access-list 102 deny tcp 172 .17.30.0 0 .0.0.255 any","title":"ACLs"},{"location":"networking/cisco/acls/#enter-a-sub-interface","text":"1 ip access-group NUMBER in","title":"Enter a \"sub\" interface"},{"location":"networking/cisco/cheat-sheet/","text":"Allow unsupported Transceivers to be used \u00b6 1 2 3 4 5 enable configure terminal no errdisable detect cause gbic-invalid service unsupported-transceiver exit Quick setup \"cheap\" network \u00b6 More to come here to get a Cisco switch running with \"cheap\" network equipment. 1 2 3 4 5 enable configure terminal no errdisable detect cause gbic-invalid service unsupported-transceiver exit","title":"Cheat Sheet"},{"location":"networking/cisco/cheat-sheet/#allow-unsupported-transceivers-to-be-used","text":"1 2 3 4 5 enable configure terminal no errdisable detect cause gbic-invalid service unsupported-transceiver exit","title":"Allow unsupported Transceivers to be used"},{"location":"networking/cisco/cheat-sheet/#quick-setup-cheap-network","text":"More to come here to get a Cisco switch running with \"cheap\" network equipment. 1 2 3 4 5 enable configure terminal no errdisable detect cause gbic-invalid service unsupported-transceiver exit","title":"Quick setup \"cheap\" network"},{"location":"networking/cisco/switch-configuration/","text":"Basic Commands \u00b6 enable - Privileged mode. configure terminal - Enter global config mode hostname NAME - Set a hostname configure terminal - Enable config mode. no ip domain lookup - Disable accidental DNS lookup (in priv and non priv mode). exit - Go one mode back. Config Mode \u00b6 Enter config mode using configure terminal . line console 0 - Enter line console 0 \"interface\". interface Gi 0/48 - Enter Interface Gigabit 0/48 interface. ip address IP_ADDR SUBNET_MASK - Set IP_ADDR and the SUBNET_MASK for an interface. Make interface dedicated for mgmt \u00b6 1 2 3 4 5 6 7 configure terminal interface Gi 0 /48 no interface port int vlan1 shutdown exit ip default-gateway DEFAULT_GATEWAY Set IP address on interface \u00b6 1 2 3 configure terminal interface Gi 0 /48 ip address IP_ADDR SUBNET_MASK \"Activate\" SSH RSA Key \u00b6 1 crypto key generate rsa general-keys modulus 4096 label sw-azubi-1 Show interface status \u00b6 1 do show interface status Add passwords to console login thingy \u00b6 1 2 3 4 5 6 7 configure terminal line console 0 password YOUR_PASSWORD line vty 0 4 login password YOUR_PASSWORD exit In the global config mode: 1 enable secret YOUR_PASSWORD","title":"Switch Configuration"},{"location":"networking/cisco/switch-configuration/#basic-commands","text":"enable - Privileged mode. configure terminal - Enter global config mode hostname NAME - Set a hostname configure terminal - Enable config mode. no ip domain lookup - Disable accidental DNS lookup (in priv and non priv mode). exit - Go one mode back.","title":"Basic Commands"},{"location":"networking/cisco/switch-configuration/#config-mode","text":"Enter config mode using configure terminal . line console 0 - Enter line console 0 \"interface\". interface Gi 0/48 - Enter Interface Gigabit 0/48 interface. ip address IP_ADDR SUBNET_MASK - Set IP_ADDR and the SUBNET_MASK for an interface.","title":"Config Mode"},{"location":"networking/cisco/switch-configuration/#make-interface-dedicated-for-mgmt","text":"1 2 3 4 5 6 7 configure terminal interface Gi 0 /48 no interface port int vlan1 shutdown exit ip default-gateway DEFAULT_GATEWAY","title":"Make interface dedicated for mgmt"},{"location":"networking/cisco/switch-configuration/#set-ip-address-on-interface","text":"1 2 3 configure terminal interface Gi 0 /48 ip address IP_ADDR SUBNET_MASK","title":"Set IP address on interface"},{"location":"networking/cisco/switch-configuration/#activate-ssh-rsa-key","text":"1 crypto key generate rsa general-keys modulus 4096 label sw-azubi-1","title":"\"Activate\" SSH RSA Key"},{"location":"networking/cisco/switch-configuration/#show-interface-status","text":"1 do show interface status","title":"Show interface status"},{"location":"networking/cisco/switch-configuration/#add-passwords-to-console-login-thingy","text":"1 2 3 4 5 6 7 configure terminal line console 0 password YOUR_PASSWORD line vty 0 4 login password YOUR_PASSWORD exit In the global config mode: 1 enable secret YOUR_PASSWORD","title":"Add passwords to console login thingy"},{"location":"networking/fiber/cheat-sheet/","text":"RJ45 (Copper) T-Base SFP / SFP+ \u00b6 As long as they support one of the T-Base standards (e.g., 1000T-Base), they should be able to be used like a \"normal RJ45 ethernet network\" port. T-Base standard For the SFP / SFP+ to work with, e.g, 10 / 100 / 1000, each of the nT-Base standards must be \"implemented\" in them. Simplex and Multimode fiber cables and transceivers \u00b6 You should not mix simplex with multimode fibers and transceivers, and the other way around. \"You'll never have a second go for APC connectors\" \u00b6 Due to the polishing (angle 8\u00b0) of APC fiber \"ends\", if you plugin the fiber connector the wrong way, you'll \"break\" them (slight crunch noise). E.g., plugging SC APC connectors in the wrong way. Make sure to check the orientation markers and with that always plug APC connectors in the correct way! UPC connectors are having no polish at all so no need to worry about the way to plug them in.","title":"Cheat Sheet"},{"location":"networking/fiber/cheat-sheet/#rj45-copper-t-base-sfp-sfp","text":"As long as they support one of the T-Base standards (e.g., 1000T-Base), they should be able to be used like a \"normal RJ45 ethernet network\" port. T-Base standard For the SFP / SFP+ to work with, e.g, 10 / 100 / 1000, each of the nT-Base standards must be \"implemented\" in them.","title":"RJ45 (Copper) T-Base SFP / SFP+"},{"location":"networking/fiber/cheat-sheet/#simplex-and-multimode-fiber-cables-and-transceivers","text":"You should not mix simplex with multimode fibers and transceivers, and the other way around.","title":"Simplex and Multimode fiber cables and transceivers"},{"location":"networking/fiber/cheat-sheet/#youll-never-have-a-second-go-for-apc-connectors","text":"Due to the polishing (angle 8\u00b0) of APC fiber \"ends\", if you plugin the fiber connector the wrong way, you'll \"break\" them (slight crunch noise). E.g., plugging SC APC connectors in the wrong way. Make sure to check the orientation markers and with that always plug APC connectors in the correct way! UPC connectors are having no polish at all so no need to worry about the way to plug them in.","title":"\"You'll never have a second go for APC connectors\""},{"location":"networking/fiber/glossar/","text":"OS1 and OS2 \u00b6 Mode : Singlemode. Bi-Directional, only one line / cable core needed. \"Only one side can sent at once\", used for longer distances. TODO OM1 / OM2 / OM3 / OM4 / OM5 \u00b6 Mode : Multimode. \"Two lines / cable cores each for 'one way' of data\", used for \"short\" distances. TODO Simplex vs Duplex fiber \u00b6 Simplex cable are consisting of a single cable strand. Duplex cable are consisting of two cable strands (\"glued\" together). Singlemode vs Multimode \u00b6 Singlemode only allows one light mode to pass through at a time. Wikipedia Advantages: Much further distances (e.g., OS2 up to 10 kilometers for 10G). Good for WAN network \"applications\". Disadvantages: Higher cost (depending on where you buy, up to 6-times more expensive). Multimode have shorter reach though are because of their price better suited, for, e.g., in datacenter usage. Wikipedia Advantages: Lower cost (in comparison with Singlemode). Compatible with many different data protocols, e.g., ethernet. Disadvantages: Nowhere near the distance of Singlemode (e.g., OM4 10G distance is \"only\" up to 550 meters).","title":"Glossar"},{"location":"networking/fiber/glossar/#os1-and-os2","text":"Mode : Singlemode. Bi-Directional, only one line / cable core needed. \"Only one side can sent at once\", used for longer distances. TODO","title":"OS1 and OS2"},{"location":"networking/fiber/glossar/#om1-om2-om3-om4-om5","text":"Mode : Multimode. \"Two lines / cable cores each for 'one way' of data\", used for \"short\" distances. TODO","title":"OM1 / OM2 / OM3 / OM4 / OM5"},{"location":"networking/fiber/glossar/#simplex-vs-duplex-fiber","text":"Simplex cable are consisting of a single cable strand. Duplex cable are consisting of two cable strands (\"glued\" together).","title":"Simplex vs Duplex fiber"},{"location":"networking/fiber/glossar/#singlemode-vs-multimode","text":"Singlemode only allows one light mode to pass through at a time. Wikipedia Advantages: Much further distances (e.g., OS2 up to 10 kilometers for 10G). Good for WAN network \"applications\". Disadvantages: Higher cost (depending on where you buy, up to 6-times more expensive). Multimode have shorter reach though are because of their price better suited, for, e.g., in datacenter usage. Wikipedia Advantages: Lower cost (in comparison with Singlemode). Compatible with many different data protocols, e.g., ethernet. Disadvantages: Nowhere near the distance of Singlemode (e.g., OM4 10G distance is \"only\" up to 550 meters).","title":"Singlemode vs Multimode"},{"location":"networking/mikrotik/cheat-sheet/","text":"Quick Run Snippets \u00b6 Automatic OS and Firmware Update \u00b6 1 2 3 4 5 6 7 8 /system routerboard settings set auto-upgrade = yes /system package update check-for-updates once :delay 3s ; :if ( [ get status ] = \"New version is available\" ) do ={ install } :delay 1s: /system routerboard upgrade /system reboot Config \u00b6 Set Hostname / Identity \u00b6 1 2 /system identity set name = HOSTNAME Set Timezone \u00b6 To Europe/Berlin . 1 2 /system clock set time-zone-name = Europe/Berlin Disable Ports beginning with ether \u00b6 1 :foreach i in =[ /interface find name~ \"ether\" ] do ={ /interface ethernet set $i disabled = yes } \"Advertise\" 10G on SFP+ Ports \u00b6 1 :foreach i in =[ /interface find name~ \"sfp-sfpplus\" ] do ={ /interface ethernet set $i advertise = 10000M-full ; } Enable Graphs / Graphing \u00b6 1 2 3 4 5 6 7 /tool graphing set page-refresh = 240 /tool graphing interface add /tool graphing resource # Put your admin / configuration network here add allow-address = 172 .16.0.0/24 Set Boot target of Device \u00b6 1 2 /system routerboard settings set boot-os = router-os","title":"Cheat Sheet"},{"location":"networking/mikrotik/cheat-sheet/#quick-run-snippets","text":"","title":"Quick Run Snippets"},{"location":"networking/mikrotik/cheat-sheet/#automatic-os-and-firmware-update","text":"1 2 3 4 5 6 7 8 /system routerboard settings set auto-upgrade = yes /system package update check-for-updates once :delay 3s ; :if ( [ get status ] = \"New version is available\" ) do ={ install } :delay 1s: /system routerboard upgrade /system reboot","title":"Automatic OS and Firmware Update"},{"location":"networking/mikrotik/cheat-sheet/#config","text":"","title":"Config"},{"location":"networking/mikrotik/cheat-sheet/#set-hostname-identity","text":"1 2 /system identity set name = HOSTNAME","title":"Set Hostname / Identity"},{"location":"networking/mikrotik/cheat-sheet/#set-timezone","text":"To Europe/Berlin . 1 2 /system clock set time-zone-name = Europe/Berlin","title":"Set Timezone"},{"location":"networking/mikrotik/cheat-sheet/#disable-ports-beginning-with-ether","text":"1 :foreach i in =[ /interface find name~ \"ether\" ] do ={ /interface ethernet set $i disabled = yes }","title":"Disable Ports beginning with ether"},{"location":"networking/mikrotik/cheat-sheet/#advertise-10g-on-sfp-ports","text":"1 :foreach i in =[ /interface find name~ \"sfp-sfpplus\" ] do ={ /interface ethernet set $i advertise = 10000M-full ; }","title":"\"Advertise\" 10G on SFP+ Ports"},{"location":"networking/mikrotik/cheat-sheet/#enable-graphs-graphing","text":"1 2 3 4 5 6 7 /tool graphing set page-refresh = 240 /tool graphing interface add /tool graphing resource # Put your admin / configuration network here add allow-address = 172 .16.0.0/24","title":"Enable Graphs / Graphing"},{"location":"networking/mikrotik/cheat-sheet/#set-boot-target-of-device","text":"1 2 /system routerboard settings set boot-os = router-os","title":"Set Boot target of Device"},{"location":"networking/mikrotik/example-configs/","text":"VLANs + VLAN Ingress Filtering \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 /interface bridge # DO NOT SET `vlan-filtering=yes` here already! Otherwise you would lock yourself out. add dhcp-snooping = yes frame-types = admit-only-vlan-tagged igmp-snooping = yes ingress-filtering = no name = bridge1 pvid = 4094 vlan-filtering = no /interface vlan add interface = bridge1 name = vlan_10_misc vlan-id = 10 add interface = bridge1 name = vlan_20_seccam vlan-id = 20 add interface = bridge1 name = vlan_30_iot vlan-id = 30 add interface = bridge1 name = vlan_100_guest vlan-id = 100 add interface = bridge1 name = vlan_4093_admin vlan-id = 4093 add interface = bridge1 name = vlan_4094_netmgmt vlan-id = 4094 /interface bridge port add bridge = bridge1 frame-types = admit-only-vlan-tagged hw = yes ingress-filtering = yes interface = bonding1 pvid = 4094 trusted = yes add bridge = bridge1 frame-types = admit-only-vlan-tagged hw = yes ingress-filtering = yes interface = sfp-sfpplus1 pvid = 4094 trusted = yes add bridge = bridge1 frame-types = admit-only-vlan-tagged hw = yes ingress-filtering = yes interface = sfp-sfpplus2 pvid = 4094 trusted = yes add bridge = bridge1 frame-types = admit-only-vlan-tagged hw = yes ingress-filtering = yes interface = sfp-sfpplus3 pvid = 4094 trusted = yes add bridge = bridge1 frame-types = admit-only-untagged-and-priority-tagged hw = yes ingress-filtering = yes interface = sfp-sfpplus4 pvid = 10 add bridge = bridge1 frame-types = admit-only-untagged-and-priority-tagged hw = yes ingress-filtering = yes interface = sfp-sfpplus5 pvid = 4093 add bridge = bridge1 frame-types = admit-only-vlan-tagged hw = yes ingress-filtering = yes interface = sfp-sfpplus6 pvid = 4094 trusted = yes /interface bridge vlan add bridge = bridge1 tagged = bonding1,sfp-sfpplus1,sfp-sfpplus6 untagged = sfp-sfpplus4 vlan-ids = 10 add bridge = bridge1 tagged = bonding1,sfp-sfpplus1,sfp-sfpplus6 vlan-ids = 20 add bridge = bridge1 tagged = bonding1,sfp-sfpplus1,sfp-sfpplus6 vlan-ids = 30 add bridge = bridge1 tagged = bonding1,sfp-sfpplus1,sfp-sfpplus6 vlan-ids = 100 add bridge = bridge1 tagged = bonding1,sfp-sfpplus1,sfp-sfpplus2,sfp-sfpplus3,sfp-sfpplus6 untagged = sfp-sfpplus5 vlan-ids = 4093 add bridge = bridge1 tagged = bonding1,bridge1,sfp-sfpplus1,sfp-sfpplus2,sfp-sfpplus3,sfp-sfpplus6 vlan-ids = 4094 # Get an IP for the network management interface /ip dhcp-client add dhcp-options = hostname,clientid disabled = no interface = vlan_4094_netmgmt # Wait for everything to \"settle down\" :delay 3 # Enable VLAN Filtering /interface bridge set bridge1 ingress-filtering = yes vlan-filtering = yes Hardware Offloading ( hw=yes ) \u00b6 It depends on the Router OS version and if the Switch Chip in your MikroTik device supports hardware offloading. See MikroTik Wiki - Manual:Switch Chip Features . Bonding \u00b6 This bonds interfaces sfp-sfpplus7 and sfp-sfpplus8 together as bonding1 interface: 1 2 /interface bonding add lacp-rate = 1sec name = bonding1 slaves = sfp-sfpplus7,sfp-sfpplus8 transmit-hash-policy = layer-2-and-3","title":"Example Configs"},{"location":"networking/mikrotik/example-configs/#vlans-vlan-ingress-filtering","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 /interface bridge # DO NOT SET `vlan-filtering=yes` here already! Otherwise you would lock yourself out. add dhcp-snooping = yes frame-types = admit-only-vlan-tagged igmp-snooping = yes ingress-filtering = no name = bridge1 pvid = 4094 vlan-filtering = no /interface vlan add interface = bridge1 name = vlan_10_misc vlan-id = 10 add interface = bridge1 name = vlan_20_seccam vlan-id = 20 add interface = bridge1 name = vlan_30_iot vlan-id = 30 add interface = bridge1 name = vlan_100_guest vlan-id = 100 add interface = bridge1 name = vlan_4093_admin vlan-id = 4093 add interface = bridge1 name = vlan_4094_netmgmt vlan-id = 4094 /interface bridge port add bridge = bridge1 frame-types = admit-only-vlan-tagged hw = yes ingress-filtering = yes interface = bonding1 pvid = 4094 trusted = yes add bridge = bridge1 frame-types = admit-only-vlan-tagged hw = yes ingress-filtering = yes interface = sfp-sfpplus1 pvid = 4094 trusted = yes add bridge = bridge1 frame-types = admit-only-vlan-tagged hw = yes ingress-filtering = yes interface = sfp-sfpplus2 pvid = 4094 trusted = yes add bridge = bridge1 frame-types = admit-only-vlan-tagged hw = yes ingress-filtering = yes interface = sfp-sfpplus3 pvid = 4094 trusted = yes add bridge = bridge1 frame-types = admit-only-untagged-and-priority-tagged hw = yes ingress-filtering = yes interface = sfp-sfpplus4 pvid = 10 add bridge = bridge1 frame-types = admit-only-untagged-and-priority-tagged hw = yes ingress-filtering = yes interface = sfp-sfpplus5 pvid = 4093 add bridge = bridge1 frame-types = admit-only-vlan-tagged hw = yes ingress-filtering = yes interface = sfp-sfpplus6 pvid = 4094 trusted = yes /interface bridge vlan add bridge = bridge1 tagged = bonding1,sfp-sfpplus1,sfp-sfpplus6 untagged = sfp-sfpplus4 vlan-ids = 10 add bridge = bridge1 tagged = bonding1,sfp-sfpplus1,sfp-sfpplus6 vlan-ids = 20 add bridge = bridge1 tagged = bonding1,sfp-sfpplus1,sfp-sfpplus6 vlan-ids = 30 add bridge = bridge1 tagged = bonding1,sfp-sfpplus1,sfp-sfpplus6 vlan-ids = 100 add bridge = bridge1 tagged = bonding1,sfp-sfpplus1,sfp-sfpplus2,sfp-sfpplus3,sfp-sfpplus6 untagged = sfp-sfpplus5 vlan-ids = 4093 add bridge = bridge1 tagged = bonding1,bridge1,sfp-sfpplus1,sfp-sfpplus2,sfp-sfpplus3,sfp-sfpplus6 vlan-ids = 4094 # Get an IP for the network management interface /ip dhcp-client add dhcp-options = hostname,clientid disabled = no interface = vlan_4094_netmgmt # Wait for everything to \"settle down\" :delay 3 # Enable VLAN Filtering /interface bridge set bridge1 ingress-filtering = yes vlan-filtering = yes","title":"VLANs + VLAN Ingress Filtering"},{"location":"networking/mikrotik/example-configs/#hardware-offloading-hwyes","text":"It depends on the Router OS version and if the Switch Chip in your MikroTik device supports hardware offloading. See MikroTik Wiki - Manual:Switch Chip Features .","title":"Hardware Offloading (hw=yes)"},{"location":"networking/mikrotik/example-configs/#bonding","text":"This bonds interfaces sfp-sfpplus7 and sfp-sfpplus8 together as bonding1 interface: 1 2 /interface bonding add lacp-rate = 1sec name = bonding1 slaves = sfp-sfpplus7,sfp-sfpplus8 transmit-hash-policy = layer-2-and-3","title":"Bonding"},{"location":"software/crio/","text":"Default CNI configs cause Cluster Network Issues \u00b6 On most OSes where CRI-O is availabe as a packge, CRI-O comes with some default CNI configs located at /etc/cni/net.d/ . If you, e.g., will be running Kubernetes with a (separate) CNI (e.g., Calico, Cilium, Flannel, etc.), you must remove those files and make sure the CRI-O service has been restarted. On a new Fedora 32 installation, the following files should be removed and after that the CRI-O service restarted: /etc/cni/net.d/100-crio-bridge.conf /etc/cni/net.d/200-loopback.conf 1 # systemctl restart crio Tip The CRI-O service must only be restarted if the CRI-O has already been started with those CNI config files in the directory.","title":"CRI-O"},{"location":"software/crio/#default-cni-configs-cause-cluster-network-issues","text":"On most OSes where CRI-O is availabe as a packge, CRI-O comes with some default CNI configs located at /etc/cni/net.d/ . If you, e.g., will be running Kubernetes with a (separate) CNI (e.g., Calico, Cilium, Flannel, etc.), you must remove those files and make sure the CRI-O service has been restarted. On a new Fedora 32 installation, the following files should be removed and after that the CRI-O service restarted: /etc/cni/net.d/100-crio-bridge.conf /etc/cni/net.d/200-loopback.conf 1 # systemctl restart crio Tip The CRI-O service must only be restarted if the CRI-O has already been started with those CNI config files in the directory.","title":"Default CNI configs cause Cluster Network Issues"},{"location":"software/docker-registry/","text":"Garbage Collection doesn't work with non AWS S3 stores \u00b6 (Non AWS S3 stores, e.g., Ceph RGW, Minio, Linode Object store and other similar stores) This is due to GitHub docker/distribution - Issue failed to garbage collect #3200 . Note This workaround / \"fix\" is based on @thomasf (Thomas Fr\u00f6ssman) 's comment in the issue . There is an issue in the s3aws.Walk() function which fails for (most) non AWS S3 storages. To make the garbage collection work, an empty file needs to be created in the bucket at the following path (default S3 bucket settings used in the docker-registry itself) BUCKET_NAME/docker/registry/v2/repositories/ . Info Replace the placeholders ( MC_HOST_CONFIG_NAME , BUCKET_NAME ) according to your docker-registry S3 storage configuration. Using the Minio client mc (compatible with most / all S3 based storages) the following command should \"fix\" the issue: 1 2 3 4 5 6 # Create empty file touch workaround_s3aws_walk_issue_github_docker_distribtuion_issue_3200 # Upload the empty file mc cp workaround_s3aws_walk_issue_github_docker_distribtuion_issue_3200 MC_HOST_CONFIG_NAME/BUCKET_NAME/docker/registry/v2/repositories/ # Verify that the file has been uploaded mc ls MC_HOST_CONFIG_NAME/BUCKET_NAME/docker/registry/v2/repositories/ Configuration through Environment Variables Fails \u00b6 Sometimes when \"deeply nesting\" environment variables to configure a certain aspect of the docker-registry configuration, e.g., REGISTRY_STORAGE_MAINTENANCE_READONLY_ENABLED , need to be specified as YAML or JSON starting from \"a few levels\" further below of the config structure: 1 2 3 REGISTRY_STORAGE_MAINTENANCE : |- uploadpurging: enabled: false This has been posted for a similar config situation by @0rax in GitHub docker/distribution - Registry - Upload purging environment overrides crash registry at startup Issue #1736 .","title":"Docker Registry"},{"location":"software/docker-registry/#garbage-collection-doesnt-work-with-non-aws-s3-stores","text":"(Non AWS S3 stores, e.g., Ceph RGW, Minio, Linode Object store and other similar stores) This is due to GitHub docker/distribution - Issue failed to garbage collect #3200 . Note This workaround / \"fix\" is based on @thomasf (Thomas Fr\u00f6ssman) 's comment in the issue . There is an issue in the s3aws.Walk() function which fails for (most) non AWS S3 storages. To make the garbage collection work, an empty file needs to be created in the bucket at the following path (default S3 bucket settings used in the docker-registry itself) BUCKET_NAME/docker/registry/v2/repositories/ . Info Replace the placeholders ( MC_HOST_CONFIG_NAME , BUCKET_NAME ) according to your docker-registry S3 storage configuration. Using the Minio client mc (compatible with most / all S3 based storages) the following command should \"fix\" the issue: 1 2 3 4 5 6 # Create empty file touch workaround_s3aws_walk_issue_github_docker_distribtuion_issue_3200 # Upload the empty file mc cp workaround_s3aws_walk_issue_github_docker_distribtuion_issue_3200 MC_HOST_CONFIG_NAME/BUCKET_NAME/docker/registry/v2/repositories/ # Verify that the file has been uploaded mc ls MC_HOST_CONFIG_NAME/BUCKET_NAME/docker/registry/v2/repositories/","title":"Garbage Collection doesn't work with non AWS S3 stores"},{"location":"software/docker-registry/#configuration-through-environment-variables-fails","text":"Sometimes when \"deeply nesting\" environment variables to configure a certain aspect of the docker-registry configuration, e.g., REGISTRY_STORAGE_MAINTENANCE_READONLY_ENABLED , need to be specified as YAML or JSON starting from \"a few levels\" further below of the config structure: 1 2 3 REGISTRY_STORAGE_MAINTENANCE : |- uploadpurging: enabled: false This has been posted for a similar config situation by @0rax in GitHub docker/distribution - Registry - Upload purging environment overrides crash registry at startup Issue #1736 .","title":"Configuration through Environment Variables Fails"},{"location":"software/harbor-registry/","text":"Garbage Collection (GC) not working with non-AWS S3 storage? \u00b6 (Non AWS S3 stores, e.g., Ceph RGW, Minio, Linode Object store and other similar stores) See Docker Registry - Garbage Collection doesn't work with non AWS S3 stores . Notes \u00b6 [Security] : Images are not scanned on push by default. This option must be enabled per project / group as of today, 28.08.2020. [Kubernetes] : The Job Service Deployment 's PersistentVolumeClaim must be of type ReadWriteMany . Otherwise having more than replicas: 1 will not work!","title":"Harbor Registry"},{"location":"software/harbor-registry/#garbage-collection-gc-not-working-with-non-aws-s3-storage","text":"(Non AWS S3 stores, e.g., Ceph RGW, Minio, Linode Object store and other similar stores) See Docker Registry - Garbage Collection doesn't work with non AWS S3 stores .","title":"Garbage Collection (GC) not working with non-AWS S3 storage?"},{"location":"software/harbor-registry/#notes","text":"[Security] : Images are not scanned on push by default. This option must be enabled per project / group as of today, 28.08.2020. [Kubernetes] : The Job Service Deployment 's PersistentVolumeClaim must be of type ReadWriteMany . Otherwise having more than replicas: 1 will not work!","title":"Notes"},{"location":"storage/NFS/common-issues/","text":"PHP Applications hanging / timing out (e.g., Nextcloud) \u00b6 In case of Nextcloud on NFS, the application was stuck / hanging at a flock syscall. To \"workaround\" the issue, if applicable for the application (should be for most PHP applications), the NFS must be mounted with the nolock mount option added.","title":"Common Issues"},{"location":"storage/NFS/common-issues/#php-applications-hanging-timing-out-eg-nextcloud","text":"In case of Nextcloud on NFS, the application was stuck / hanging at a flock syscall. To \"workaround\" the issue, if applicable for the application (should be for most PHP applications), the NFS must be mounted with the nolock mount option added.","title":"PHP Applications hanging / timing out (e.g., Nextcloud)"},{"location":"storage/ceph/","text":"Ceph \u00b6 Ceph offers block storage (RBD), network filesystem (CephFS), object storage (RGW, S3, SWIFT) and key value storage (librados). Glossary \u00b6 Daemons MON: Monitor Daemon MGR: Manager Daemon OSD: Object Storage Daemon RGW: Rados Gateway \"Things\" Pool: Group of PGs, separation . PG: Placement Group, group of objects in a pool. Object: A single object \"somewhere in the cluster\". Part of a PG. Pool -> PG -> Object \u00b6 graph LR subgraph Pool subgraph PG-1[PG 1] Object-1[Object 1] Object-2[Object 2] end subgraph PG-2[PG 2] Object-3[Object 3] Object-4[Object 4] end end Ceph Cluster Components \u00b6 MON - Monitor Daemon \u00b6 The MONs are keeping a map of the current MONs, OSDs, PGs, Pools and so on. MONs require a quorum to function, meaning that you should always run at least 3 mons for production 5 can be a good idea as well. graph LR MON-A MON-A -.- MON-B MON-A -.- MON-C MON-B MON-B -.- MON-A MON-B -.- MON-C MON-C MON-C -.- MON-B MON-C -.- MON-C Man Page : https://docs.ceph.com/en/latest/man/8/ceph-mon/ Quorum \u00b6 Simply put, if you run with 3 MONs you can lose 1 MON before the cluster will come to a halt as the quorum would be lost. MONs How many can be lost? 1 0 2 0 3 1 4 1 5 2 6 2 7 3 For more information regarding quorum, checkout the following link from the ETCD documentation: ETCD v3.3. - FAQ Why an odd number of cluster members? . ETCD is a good example of an infrastructure critical application (for Kubernetes) which requires a Quorum for (most; depending on your settings) operations. MGR - Manager Daemon \u00b6 The MGR daemon(s) \"provide additional monitoring and interfaces to external monitoring and management systems\". You need to have at least one running as otherwise certain status information in, e.g., ceph status ( ceph -s ), will not be shown. It is recommended to run at least 1. The MGR daemon(s) talk with the MON, OSD MDS, and even RGW. graph LR MGR --> MON MGR --> OSD MGR --> MDS MGR --> RGW The MGR daemon(s) have many modules to for example provide metrics for Prometheus, Zabbix and others. In addition to that a Ceph dashboard can be activated which contains some basic information and even an integration with Prometheus and Grafana. Man Page : https://docs.ceph.com/en/latest/mgr/index.html OSD - Object Storage Daemon \u00b6 All OSDs normally talk with each other for hearbeat checking and data replication (actual data and also for data \"scrubbing\" operations). graph LR OSD-0 OSD-0 -.- OSD-1 OSD-0 -.- OSD-2 OSD-0 -.- OSD-n OSD-1 OSD-1 -.- OSD-0 OSD-1 -.- OSD-2 OSD-1 -.- OSD-n OSD-2 OSD-2 -.- OSD-0 OSD-2 -.- OSD-1 OSD-2 -.- OSD-n OSD-n OSD-n -.- OSD-0 OSD-n -.- OSD-1 OSD-n -.- OSD-2 If a client writes data, the data is replicated by the OSD and not the client. If a client reads data, the data can be read from multiple OSDs at the same time (as long as they have a replica of the data). Side note : That is the reason why read speeds are so fast and writes can be so slow. One slow OSD can \"ruin your day\" because of that. Man Page : https://docs.ceph.com/en/latest/man/8/ceph-osd/ MDS - Metadata Server Daemon \u00b6 The MDS is the metadata server for the CephFilesystem (CephFS). It talks with the OSDs and coordinates the filesystem access. Clients still need to access the OSDs, but the MDS is the \"gateway\" to know where to go so to say. graph LR Client-A --> MDS-1 Client-B --> MDS-1 Client-C ---> MDS-2 MDS-1 -.- MDS-2 MDS-1 --> OSDs MDS-2 --> OSDs Client-A ---> OSDs Client-B ---> OSDs Client-C ---> OSDs Man Page : https://docs.ceph.com/en/latest/man/8/ceph-mds/ RGW - RADOS REST Gateway \u00b6 RGW can offer S3 and / or SWIFT compatible storage, allowing to use it as a \"replacement\" for AWS S3 object storage in some cases. An advantage to, e.g., the block storage (RBD) and CephFS is that the client itself does not need direct access to the MONs, OSDs, etc., though it depends on the use case and the performance required per client / application. graph LR Client-A --> RGW-1 Client-B --> RGW-1 Client-C --> RGW-2 RGW-1 --> OSDs RGW-2 --> OSDs Common scenario is that you have a load balancer in front of your RGWs, in itself the structure stays the same: graph LR Client-A --> Loadbalancer Client-B --> Loadbalancer Client-C --> Loadbalancer Loadbalancer --> RGW-1 Loadbalancer --> RGW-2 RGW-1 --> OSDs RGW-2 --> OSDs Man Page : https://docs.ceph.com/en/latest/man/8/radosgw/ This wonderful Ceph cheat sheet from @TheJJ on GitHub has much more insight into the processes, tricks and setup of a Ceph cluster and the moving parts. Old Diagrams \u00b6 The source for the diagrams, can be found as .graphml at the same path as the images. Basic Cluster with HDDs and SSDs \u00b6 Cluster with RGW for S3-compatible Object Storage \u00b6 No direct OSD access network is required by the consumers of the object storage. This can be seen as a \"advantage\" over RBD and CephFS, though it completely depends on your use case. Cluster with NVMe OSDs (+ Multi Datacenter Scenario) \u00b6","title":"Ceph"},{"location":"storage/ceph/#ceph","text":"Ceph offers block storage (RBD), network filesystem (CephFS), object storage (RGW, S3, SWIFT) and key value storage (librados).","title":"Ceph"},{"location":"storage/ceph/#glossary","text":"Daemons MON: Monitor Daemon MGR: Manager Daemon OSD: Object Storage Daemon RGW: Rados Gateway \"Things\" Pool: Group of PGs, separation . PG: Placement Group, group of objects in a pool. Object: A single object \"somewhere in the cluster\". Part of a PG.","title":"Glossary"},{"location":"storage/ceph/#pool-pg-object","text":"graph LR subgraph Pool subgraph PG-1[PG 1] Object-1[Object 1] Object-2[Object 2] end subgraph PG-2[PG 2] Object-3[Object 3] Object-4[Object 4] end end","title":"Pool -&gt; PG -&gt; Object"},{"location":"storage/ceph/#ceph-cluster-components","text":"","title":"Ceph Cluster Components"},{"location":"storage/ceph/#mon-monitor-daemon","text":"The MONs are keeping a map of the current MONs, OSDs, PGs, Pools and so on. MONs require a quorum to function, meaning that you should always run at least 3 mons for production 5 can be a good idea as well. graph LR MON-A MON-A -.- MON-B MON-A -.- MON-C MON-B MON-B -.- MON-A MON-B -.- MON-C MON-C MON-C -.- MON-B MON-C -.- MON-C Man Page : https://docs.ceph.com/en/latest/man/8/ceph-mon/","title":"MON - Monitor Daemon"},{"location":"storage/ceph/#quorum","text":"Simply put, if you run with 3 MONs you can lose 1 MON before the cluster will come to a halt as the quorum would be lost. MONs How many can be lost? 1 0 2 0 3 1 4 1 5 2 6 2 7 3 For more information regarding quorum, checkout the following link from the ETCD documentation: ETCD v3.3. - FAQ Why an odd number of cluster members? . ETCD is a good example of an infrastructure critical application (for Kubernetes) which requires a Quorum for (most; depending on your settings) operations.","title":"Quorum"},{"location":"storage/ceph/#mgr-manager-daemon","text":"The MGR daemon(s) \"provide additional monitoring and interfaces to external monitoring and management systems\". You need to have at least one running as otherwise certain status information in, e.g., ceph status ( ceph -s ), will not be shown. It is recommended to run at least 1. The MGR daemon(s) talk with the MON, OSD MDS, and even RGW. graph LR MGR --> MON MGR --> OSD MGR --> MDS MGR --> RGW The MGR daemon(s) have many modules to for example provide metrics for Prometheus, Zabbix and others. In addition to that a Ceph dashboard can be activated which contains some basic information and even an integration with Prometheus and Grafana. Man Page : https://docs.ceph.com/en/latest/mgr/index.html","title":"MGR - Manager Daemon"},{"location":"storage/ceph/#osd-object-storage-daemon","text":"All OSDs normally talk with each other for hearbeat checking and data replication (actual data and also for data \"scrubbing\" operations). graph LR OSD-0 OSD-0 -.- OSD-1 OSD-0 -.- OSD-2 OSD-0 -.- OSD-n OSD-1 OSD-1 -.- OSD-0 OSD-1 -.- OSD-2 OSD-1 -.- OSD-n OSD-2 OSD-2 -.- OSD-0 OSD-2 -.- OSD-1 OSD-2 -.- OSD-n OSD-n OSD-n -.- OSD-0 OSD-n -.- OSD-1 OSD-n -.- OSD-2 If a client writes data, the data is replicated by the OSD and not the client. If a client reads data, the data can be read from multiple OSDs at the same time (as long as they have a replica of the data). Side note : That is the reason why read speeds are so fast and writes can be so slow. One slow OSD can \"ruin your day\" because of that. Man Page : https://docs.ceph.com/en/latest/man/8/ceph-osd/","title":"OSD - Object Storage Daemon"},{"location":"storage/ceph/#mds-metadata-server-daemon","text":"The MDS is the metadata server for the CephFilesystem (CephFS). It talks with the OSDs and coordinates the filesystem access. Clients still need to access the OSDs, but the MDS is the \"gateway\" to know where to go so to say. graph LR Client-A --> MDS-1 Client-B --> MDS-1 Client-C ---> MDS-2 MDS-1 -.- MDS-2 MDS-1 --> OSDs MDS-2 --> OSDs Client-A ---> OSDs Client-B ---> OSDs Client-C ---> OSDs Man Page : https://docs.ceph.com/en/latest/man/8/ceph-mds/","title":"MDS - Metadata Server Daemon"},{"location":"storage/ceph/#rgw-rados-rest-gateway","text":"RGW can offer S3 and / or SWIFT compatible storage, allowing to use it as a \"replacement\" for AWS S3 object storage in some cases. An advantage to, e.g., the block storage (RBD) and CephFS is that the client itself does not need direct access to the MONs, OSDs, etc., though it depends on the use case and the performance required per client / application. graph LR Client-A --> RGW-1 Client-B --> RGW-1 Client-C --> RGW-2 RGW-1 --> OSDs RGW-2 --> OSDs Common scenario is that you have a load balancer in front of your RGWs, in itself the structure stays the same: graph LR Client-A --> Loadbalancer Client-B --> Loadbalancer Client-C --> Loadbalancer Loadbalancer --> RGW-1 Loadbalancer --> RGW-2 RGW-1 --> OSDs RGW-2 --> OSDs Man Page : https://docs.ceph.com/en/latest/man/8/radosgw/ This wonderful Ceph cheat sheet from @TheJJ on GitHub has much more insight into the processes, tricks and setup of a Ceph cluster and the moving parts.","title":"RGW - RADOS REST Gateway"},{"location":"storage/ceph/#old-diagrams","text":"The source for the diagrams, can be found as .graphml at the same path as the images.","title":"Old Diagrams"},{"location":"storage/ceph/#basic-cluster-with-hdds-and-ssds","text":"","title":"Basic Cluster with HDDs and SSDs"},{"location":"storage/ceph/#cluster-with-rgw-for-s3-compatible-object-storage","text":"No direct OSD access network is required by the consumers of the object storage. This can be seen as a \"advantage\" over RBD and CephFS, though it completely depends on your use case.","title":"Cluster with RGW for S3-compatible Object Storage"},{"location":"storage/ceph/#cluster-with-nvme-osds-multi-datacenter-scenario","text":"","title":"Cluster with NVMe OSDs (+ Multi Datacenter Scenario)"},{"location":"storage/ceph/common-issues/","text":"CephFS mount issues on Hosts \u00b6 Make sure you have a (active) Linux kernel of version 4.17 or higher. Tip In general it is recommended to have a very up-to-date version of the Linux kernel, as many improvements have been made to the Ceph kernel drivers in newer kernel versions ( 5.x or higher). HEALTH_WARN 1 large omap objects \u00b6 Issue \u00b6 1 2 3 HEALTH_WARN 1 large omap objects # and/or LARGE_OMAP_OBJECTS 1 large omap objects Solution \u00b6 The following command should fix the issue: 1 radosgw-admin reshard stale-instances rm","title":"Common Issues"},{"location":"storage/ceph/common-issues/#cephfs-mount-issues-on-hosts","text":"Make sure you have a (active) Linux kernel of version 4.17 or higher. Tip In general it is recommended to have a very up-to-date version of the Linux kernel, as many improvements have been made to the Ceph kernel drivers in newer kernel versions ( 5.x or higher).","title":"CephFS mount issues on Hosts"},{"location":"storage/ceph/common-issues/#health_warn-1-large-omap-objects","text":"","title":"HEALTH_WARN 1 large omap objects"},{"location":"storage/ceph/common-issues/#issue","text":"1 2 3 HEALTH_WARN 1 large omap objects # and/or LARGE_OMAP_OBJECTS 1 large omap objects","title":"Issue"},{"location":"storage/ceph/common-issues/#solution","text":"The following command should fix the issue: 1 radosgw-admin reshard stale-instances rm","title":"Solution"},{"location":"storage/ceph/osds/","text":"OSD Maintenance \u00b6 Gracefully remove OSD \u00b6 Tip If you are using Rook Ceph Operator to run a Ceph Cluster in Kubernetes, please follow the official documentation here: Rook Docs - Ceph OSD Management . First thing is to set the crush weight to zero, either instantly to 0.0 or a bit gracefully . ( gracefully should always be used when the cluster is in use, though any OSD weight change will cause data redistribution) 1 ceph osd crush reweight osd.<ID> 0 .0 or graceful: 1 2 3 4 5 for i in { 9 1 } ; do ceph osd crush reweight osd.<ID> 0 . $i # Wait five minutes each step or longer depending on your Ceph cluster recovery speed sleep 300 done After the reweight, set the OSD out and remove it (+ its credentials): 1 ceph osd out <ID> 1 2 3 ceph osd crush remove osd.<ID> ceph auth del osd.<ID> ceph osd rm <ID>","title":"OSDs"},{"location":"storage/ceph/osds/#osd-maintenance","text":"","title":"OSD Maintenance"},{"location":"storage/ceph/osds/#gracefully-remove-osd","text":"Tip If you are using Rook Ceph Operator to run a Ceph Cluster in Kubernetes, please follow the official documentation here: Rook Docs - Ceph OSD Management . First thing is to set the crush weight to zero, either instantly to 0.0 or a bit gracefully . ( gracefully should always be used when the cluster is in use, though any OSD weight change will cause data redistribution) 1 ceph osd crush reweight osd.<ID> 0 .0 or graceful: 1 2 3 4 5 for i in { 9 1 } ; do ceph osd crush reweight osd.<ID> 0 . $i # Wait five minutes each step or longer depending on your Ceph cluster recovery speed sleep 300 done After the reweight, set the OSD out and remove it (+ its credentials): 1 ceph osd out <ID> 1 2 3 ceph osd crush remove osd.<ID> ceph auth del osd.<ID> ceph osd rm <ID>","title":"Gracefully remove OSD"},{"location":"storage/ceph/rbd/","text":"RBD Performance Stats \u00b6 Source : Ceph Block Performance Monitoring: Putting noisy neighbors in their place with RBD top and QoS - Red Hat Blog 1 $ ceph ceph mgr module enable rbd_support 1 2 $ rbd perf image iotop $ rbd perf image iostat","title":"RBD (Block Storage)"},{"location":"storage/ceph/rbd/#rbd-performance-stats","text":"Source : Ceph Block Performance Monitoring: Putting noisy neighbors in their place with RBD top and QoS - Red Hat Blog 1 $ ceph ceph mgr module enable rbd_support 1 2 $ rbd perf image iotop $ rbd perf image iostat","title":"RBD Performance Stats"},{"location":"storage/gluster/common-issues/","text":"No File Locking \u00b6 Note The described case here is a define \"no-go\" to run on a GlusterFS, but it is worth to mention as some other applications (possibly git and others) might have issues in the long run as well. GlusterFS doesn't seem to have file locking, meaning that, e.g., a SQLite database will corrupt if multiple hosts try to access it.","title":"Common Issues"},{"location":"storage/gluster/common-issues/#no-file-locking","text":"Note The described case here is a define \"no-go\" to run on a GlusterFS, but it is worth to mention as some other applications (possibly git and others) might have issues in the long run as well. GlusterFS doesn't seem to have file locking, meaning that, e.g., a SQLite database will corrupt if multiple hosts try to access it.","title":"No File Locking"},{"location":"storage/rook/","text":"The source for the diagrams, can be found as .graphml at the same path as the images. Rook Ceph Components \u00b6 Where the \"basic\" components are the rook-ceph-agent and rook-discover DaemonSet. Note Rook Ceph Discovery DaemonSet is only started after at least one CephCluster has been created! The Rook Ceph Agent was used in earlier Rook Ceph versions for the \"Flex driver\" before the swaitch to use, e.g., Ceph's CSI driver.","title":"Rook"},{"location":"storage/rook/#rook-ceph-components","text":"Where the \"basic\" components are the rook-ceph-agent and rook-discover DaemonSet. Note Rook Ceph Discovery DaemonSet is only started after at least one CephCluster has been created! The Rook Ceph Agent was used in earlier Rook Ceph versions for the \"Flex driver\" before the swaitch to use, e.g., Ceph's CSI driver.","title":"Rook Ceph Components"},{"location":"storage/rook/architecture/","text":"The source for the diagrams, can be found as .graphml at the same path as the images. Rook Ceph Components \u00b6 Where the \"basic\" components are the rook-ceph-agent and rook-discover DaemonSet. Note Rook Ceph Discovery DaemonSet is only started after at least one CephCluster has been created! Rook Ceph Agent (\"Flex driver\") was used in earlier Rook Ceph versions for mounting storage, before it a switch to use Ceph CSI driver was made.","title":"Architecture"},{"location":"storage/rook/architecture/#rook-ceph-components","text":"Where the \"basic\" components are the rook-ceph-agent and rook-discover DaemonSet. Note Rook Ceph Discovery DaemonSet is only started after at least one CephCluster has been created! Rook Ceph Agent (\"Flex driver\") was used in earlier Rook Ceph versions for mounting storage, before it a switch to use Ceph CSI driver was made.","title":"Rook Ceph Components"},{"location":"storage/rook/cheat-sheet/","text":"Get PersistentVolume and PersistentVolumeClaim from csi-vol-... Name in Kubernetes \u00b6 Replace csi-vol-............ with the volume name: 1 $ kubectl get pv -o jsonpath = '{range .items[?(@.spec.csi.volumeAttributes.imageName==\"csi-vol-............\")]}{.metadata.name}{\"\\t\"}{.spec.claimRef.namespace}{\"/\"}{.spec.claimRef.name}{\"\\n\"}{end}'","title":"Cheat Sheet"},{"location":"storage/rook/cheat-sheet/#get-persistentvolume-and-persistentvolumeclaim-from-csi-vol-name-in-kubernetes","text":"Replace csi-vol-............ with the volume name: 1 $ kubectl get pv -o jsonpath = '{range .items[?(@.spec.csi.volumeAttributes.imageName==\"csi-vol-............\")]}{.metadata.name}{\"\\t\"}{.spec.claimRef.namespace}{\"/\"}{.spec.claimRef.name}{\"\\n\"}{end}'","title":"Get PersistentVolume and PersistentVolumeClaim from csi-vol-... Name in Kubernetes"},{"location":"storage/rook/common-issues/","text":"Be sure to checkout the Rook Ceph Common Issues page and that all prerequisites for the storage backend of your choice are met! General Rook prerequisites Ceph prerequisites Ceph \u00b6 Where did the rook-discover-* Pods go after a recent Rook Ceph update? \u00b6 A recent change in Rook Ceph has disabled the rook-discover DaemonSet by default. This behavior is controlled by the ROOK_ENABLE_DISCOVERY_DAEMON located in the operator.yaml or for Helm users enableDiscoveryDaemon: (false|true in your values file. It is a boolean, so false or true . When do you want to have rook-discover-* Pods / ROOK_ENABLE_DISCOVERY_DAEMON: true ? \u00b6 You are on (plain) bare metal and / or simply have \"some disks\" installed /attached to your server(s), that you want to use for the Rook Ceph cluster. If your cloud environment / provider does not provide PVCs with volumeMode: Block . Ceph requires block devices (Ceph's filestore is not available, through Rook, since a bunch of versions as bluestore is superior in certain ways). Crash Collector Pods are Pending / ContainerCreating \u00b6 Check the events of the Crash Collector Pod(s) using kubectl describe pod POD_NAME . If the Pod(s) is waiting for a Secret from the Ceph MONs (keyring for each crash collector), you need to wait a bit longer as the Ceph Cluster is probably still being bootsraped / started up. If they are stuck for more than 15-30 minutes, check the Rook Ceph Operator logs if it is stuck in the Ceph Cluster bootstrap / start up procedure. No rook-ceph-mon-* Pods are running \u00b6 First of all make sure your Kubernetes CNI is working fine! In what feels like 90% of the cases it is network related, e.g., some weird thing with the Kubernetes cluster CNI or other network environment issue. Can you talk to Cluster Service IPs from every node? Can you talk to Pod IPs from every node? Even to Pods not on the same node you are testing from? Check the docs of your CNI, most have a troubleshooting section, e.g., Cilium had some issues from systemd version 245 onwards with rp_filter , see here: rp_filter (default) strict mode breaks certain load balancing cases in kube-proxy-free mode \u00b7 Issue #13130 \u00b7 cilium/cilium Does your environment fit all the prerequisites? Check top of page for the links to some of the prerequisites and / or consult the Rook.io docs . Check the rook-ceph-operator Logs for any warnings, errors, etc. Disk(s) / Partition(s) not used for Ceph \u00b6 Does section When do you want to have rook-discover-* Pods / ROOK_ENABLE_DISCOVERY_DAEMON: true ? apply to you? If so, make sure the operator has the discovery daemon enabled in its (Pod) config! Is the disk empty? No leftover partitions on it? Make sure it is either \"empty\", e.g., nulled by shred , dd or similar, To make sure the disk is blank as the Rook docs and I recommend the following commands followed by a reboot of the server: 1 2 3 4 DISK=\"/dev/sdXYZ\" sgdisk --zap-all \"$DISK\" dd if=/dev/zero of=\"$DISK\" bs=1M count=100 oflag=direct,dsync blkdiscard \"$DISK\" Source: https://rook.io/docs/rook/v1.6/ceph-teardown.html#delete-the-data-on-hosts Was the disk previously used as a Ceph OSD? Make sure to follow the teardown steps, but make sure to only remove the LVM stuff from that one disk and not from all, see https://rook.io/docs/rook/v1.6/ceph-teardown.html#delete-the-data-on-hosts . A Pod can't mount its PersistentVolume after an \"unclean\" / \"undrained\" Node shutdown \u00b6 Check the events of the Pod using kubectl describe pod POD_NAME . Check the Node's dmesg logs. Check the kubelet logs for errors related to CSI connectivity and / or make sure the node can reach every other Kubernetes cluster node (at least the Rook Ceph cluster nodes (Ceph Mons, OSDs, MGRs, etc.)). Checkout the CSI Common Issues - Rook Docs . Ceph CSI: Provisioning, Mounting, Deletion or something doesn't work \u00b6 Make sure you have checked out the CSI Common Issues - Rook Docs . If you have some weird kernel and / or kubelet configuration, make sure Ceph CSI's config options in the Rook Ceph Operator config is correctly setup (e.g., LIB_MODULES_DIR_PATH , ROOK_CSI_KUBELET_DIR_PATH , AGENT_MOUNTS ). Can't run any Ceph Commands in the Toolbox / Ceph Commands timeout \u00b6 Are your rook-ceph-mon-* Pods all in Running state? Does a basic ceph -s work? Is your rook-ceph-mgr-* Pod(s) running as well? Check the rook-ceph-mon-* and rook-ceph-mgr-* logs for errors Try deleteing the toolbox Pod, \"maybe it is just a fluke in your Kubernetes cluster network / CNI. Also make sure you are using the latest Rook Ceph Toolbox YAML for the Rook Ceph version you are running on, see Rook Ceph Toolbox Pod not Creating / Stuck section . In case all these seem to indicate a loss of quorum, e.g., the rook-ceph-mon-* talk about probing for other mons only, you might need to follow the disaster recovery guide for your Rook Ceph version here: Rook v1.6 Docs - Ceph Disaster Recovery - Restoring Mon Quorum . A MON Pod is running on a Node which is down \u00b6 DO NOT EDIT THE MON DEPLOYMENT! A MON Deployment can't just be moved to another node without being failovered by the operator and / or if the MON is running using a PVC for its data. As long as the operator is running the operator should see the mon being down and fail it over after a configurable timeout. Env var ROOK_MON_OUT_TIMEOUT , by default 600s (10 minutes) Remove / Replace a failed disk \u00b6 Checkout the official Ceph OSD Management guide from Rook here: Rook v1.6 Docs - Ceph OSD Management . Rook Ceph Toolbox Pod not Creating / Stuck \u00b6 Make sure that you are not using an old version of the Rook Ceph Toolbox, grab the latest manifest here (make sure to switch to the release- branch of your Rook release): https://github.com/rook/rook/blob/master/cluster/examples/kubernetes/ceph/toolbox.yaml The Rook Ceph Toolbox can only fully startup after a Ceph Cluster has at least passed the initial setup by the Rook Ceph operator. Monitor the Rook Ceph Operator logs for errors. Check the events of the Toolbox Pod using kubectl describe pod POD_NAME . Ceph OSD Tree: Wrong Device Class \u00b6 Check device class, second column in ceph osd tree output. If you need to change the device class, you first must remove the current one (if it has one set): ceph osd crush rm-device-class osd.ID . Now you can set the device class for the OSD: ceph osd crush set-device-class CLASS osd.ID Default device classes (at the time of writing): hdd , ssd , nvme Source: Ceph Docs Latest - CRUSH Maps - Device Classes HEALTH_WARN: clients are using insecure global_id reclaim / HEALTH_WARN: mons are allowing insecure global_id reclaim \u00b6 Source : https://github.com/rook/rook/issues/7746 I can confirm this is happening in all clusters, whether a clean install or upgraded cluster, running at least versions: v14.2.20 , v15.2.11 or v16.2.1 . According to the CVE also previously mentioned , there is a security issue where clients need to be upgraded to the releases mentioned. Once all the clients are updated (e.g. the rook daemons and csi driver), a new setting needs to be applied to the cluster that will disable allowing the insecure mode. If you see both these health warnings, then either one of the rook or csi daemons has not been upgraded yet, or some other client is detected on the older version: 1 2 3 health: HEALTH_WARN client is using insecure global_id reclaim mon is allowing insecure global_id reclaim If you only see this one warning, then the insecure mode should be disabled: 1 2 health: HEALTH_WARN mon is allowing insecure global_id reclaim To disable the insecure mode from the toolbox after all the clients are upgraded: Make sure all clients have been upgraded, or else those clients will be blocked after this is set : 1 ceph config set mon auth_allow_insecure_global_id_reclaim false Rook could set this flag automatically after the clients have all been updated. Check which \"Object Store\" is used by an OSD \u00b6 1 2 $ ceph osd metadata 0 | grep osd_objectstore \"osd_objectstore\": \"bluestore\", To get a quick overview of the \"object stores\" ( bluestore , (don't use it) filestore ): 1 2 3 4 $ ceph osd count-metadata osd_objectstore { \"bluestore\": 6 } PersistentVolumeClaims / PersistentVolumes are not Resized \u00b6 Make sure the Ceph CSI driver for the storage (block or filesystem) is running (check the logs if you are unsure as well). Check if you use a StorageClass that has allowVolumeExpansion: false : 1 2 3 4 $ kubectl get storageclasses.storage.k8s.io NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE rook-ceph-block rook-ceph.rbd.csi.ceph.com Retain Immediate false 3d21h rook-ceph-fs rook-ceph.cephfs.csi.ceph.com Retain Immediate true 3d21h To fix this simply set allowVolumeExpansion: true in the StorageClass . Below is a StorageClass with this option set, it is at the top level of the object (not in .spec or similar): 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 allowVolumeExpansion : true apiVersion : storage.k8s.io/v1 kind : StorageClass metadata : name : rook-ceph-block parameters : clusterID : rook-ceph csi.storage.k8s.io/controller-expand-secret-name : rook-csi-rbd-provisioner [ ... ] imageFeatures : layering imageFormat : \"2\" pool : replicapool provisioner : rook-ceph.rbd.csi.ceph.com reclaimPolicy : Retain volumeBindingMode : Immediate [...] failed to reconcile cluster \"rook-ceph\": [...] failed to create servicemonitor. the server could not find the requested resource (post servicemonitors.monitoring.coreos.com) \u00b6 Set .spec.monitoring.enabled to false in your CephCluster object / yaml (and apply it). unable to get monitor info from DNS SRV with service name: ceph-mon / Can't run ceph and rbd commands in the Rook Ceph XYZ Pod \u00b6 You can only run ceph , rbd , etc. commands in the Rook Ceph toolbox. Regarding the Ceph toolbox checkout the Rook documentation here: Rook v1.6 Docs - Ceph Toolbox .","title":"Common Issues"},{"location":"storage/rook/common-issues/#ceph","text":"","title":"Ceph"},{"location":"storage/rook/common-issues/#where-did-the-rook-discover-pods-go-after-a-recent-rook-ceph-update","text":"A recent change in Rook Ceph has disabled the rook-discover DaemonSet by default. This behavior is controlled by the ROOK_ENABLE_DISCOVERY_DAEMON located in the operator.yaml or for Helm users enableDiscoveryDaemon: (false|true in your values file. It is a boolean, so false or true .","title":"Where did the rook-discover-* Pods go after a recent Rook Ceph update?"},{"location":"storage/rook/common-issues/#when-do-you-want-to-have-rook-discover-pods-rook_enable_discovery_daemon-true","text":"You are on (plain) bare metal and / or simply have \"some disks\" installed /attached to your server(s), that you want to use for the Rook Ceph cluster. If your cloud environment / provider does not provide PVCs with volumeMode: Block . Ceph requires block devices (Ceph's filestore is not available, through Rook, since a bunch of versions as bluestore is superior in certain ways).","title":"When do you want to have rook-discover-* Pods / ROOK_ENABLE_DISCOVERY_DAEMON: true?"},{"location":"storage/rook/common-issues/#crash-collector-pods-are-pending-containercreating","text":"Check the events of the Crash Collector Pod(s) using kubectl describe pod POD_NAME . If the Pod(s) is waiting for a Secret from the Ceph MONs (keyring for each crash collector), you need to wait a bit longer as the Ceph Cluster is probably still being bootsraped / started up. If they are stuck for more than 15-30 minutes, check the Rook Ceph Operator logs if it is stuck in the Ceph Cluster bootstrap / start up procedure.","title":"Crash Collector Pods are Pending / ContainerCreating"},{"location":"storage/rook/common-issues/#no-rook-ceph-mon-pods-are-running","text":"First of all make sure your Kubernetes CNI is working fine! In what feels like 90% of the cases it is network related, e.g., some weird thing with the Kubernetes cluster CNI or other network environment issue. Can you talk to Cluster Service IPs from every node? Can you talk to Pod IPs from every node? Even to Pods not on the same node you are testing from? Check the docs of your CNI, most have a troubleshooting section, e.g., Cilium had some issues from systemd version 245 onwards with rp_filter , see here: rp_filter (default) strict mode breaks certain load balancing cases in kube-proxy-free mode \u00b7 Issue #13130 \u00b7 cilium/cilium Does your environment fit all the prerequisites? Check top of page for the links to some of the prerequisites and / or consult the Rook.io docs . Check the rook-ceph-operator Logs for any warnings, errors, etc.","title":"No rook-ceph-mon-* Pods are running"},{"location":"storage/rook/common-issues/#disks-partitions-not-used-for-ceph","text":"Does section When do you want to have rook-discover-* Pods / ROOK_ENABLE_DISCOVERY_DAEMON: true ? apply to you? If so, make sure the operator has the discovery daemon enabled in its (Pod) config! Is the disk empty? No leftover partitions on it? Make sure it is either \"empty\", e.g., nulled by shred , dd or similar, To make sure the disk is blank as the Rook docs and I recommend the following commands followed by a reboot of the server: 1 2 3 4 DISK=\"/dev/sdXYZ\" sgdisk --zap-all \"$DISK\" dd if=/dev/zero of=\"$DISK\" bs=1M count=100 oflag=direct,dsync blkdiscard \"$DISK\" Source: https://rook.io/docs/rook/v1.6/ceph-teardown.html#delete-the-data-on-hosts Was the disk previously used as a Ceph OSD? Make sure to follow the teardown steps, but make sure to only remove the LVM stuff from that one disk and not from all, see https://rook.io/docs/rook/v1.6/ceph-teardown.html#delete-the-data-on-hosts .","title":"Disk(s) / Partition(s) not used for Ceph"},{"location":"storage/rook/common-issues/#a-pod-cant-mount-its-persistentvolume-after-an-unclean-undrained-node-shutdown","text":"Check the events of the Pod using kubectl describe pod POD_NAME . Check the Node's dmesg logs. Check the kubelet logs for errors related to CSI connectivity and / or make sure the node can reach every other Kubernetes cluster node (at least the Rook Ceph cluster nodes (Ceph Mons, OSDs, MGRs, etc.)). Checkout the CSI Common Issues - Rook Docs .","title":"A Pod can't mount its PersistentVolume after an \"unclean\" / \"undrained\" Node shutdown"},{"location":"storage/rook/common-issues/#ceph-csi-provisioning-mounting-deletion-or-something-doesnt-work","text":"Make sure you have checked out the CSI Common Issues - Rook Docs . If you have some weird kernel and / or kubelet configuration, make sure Ceph CSI's config options in the Rook Ceph Operator config is correctly setup (e.g., LIB_MODULES_DIR_PATH , ROOK_CSI_KUBELET_DIR_PATH , AGENT_MOUNTS ).","title":"Ceph CSI: Provisioning, Mounting, Deletion or something doesn't work"},{"location":"storage/rook/common-issues/#cant-run-any-ceph-commands-in-the-toolbox-ceph-commands-timeout","text":"Are your rook-ceph-mon-* Pods all in Running state? Does a basic ceph -s work? Is your rook-ceph-mgr-* Pod(s) running as well? Check the rook-ceph-mon-* and rook-ceph-mgr-* logs for errors Try deleteing the toolbox Pod, \"maybe it is just a fluke in your Kubernetes cluster network / CNI. Also make sure you are using the latest Rook Ceph Toolbox YAML for the Rook Ceph version you are running on, see Rook Ceph Toolbox Pod not Creating / Stuck section . In case all these seem to indicate a loss of quorum, e.g., the rook-ceph-mon-* talk about probing for other mons only, you might need to follow the disaster recovery guide for your Rook Ceph version here: Rook v1.6 Docs - Ceph Disaster Recovery - Restoring Mon Quorum .","title":"Can't run any Ceph Commands in the Toolbox / Ceph Commands timeout"},{"location":"storage/rook/common-issues/#a-mon-pod-is-running-on-a-node-which-is-down","text":"DO NOT EDIT THE MON DEPLOYMENT! A MON Deployment can't just be moved to another node without being failovered by the operator and / or if the MON is running using a PVC for its data. As long as the operator is running the operator should see the mon being down and fail it over after a configurable timeout. Env var ROOK_MON_OUT_TIMEOUT , by default 600s (10 minutes)","title":"A MON Pod is running on a Node which is down"},{"location":"storage/rook/common-issues/#remove-replace-a-failed-disk","text":"Checkout the official Ceph OSD Management guide from Rook here: Rook v1.6 Docs - Ceph OSD Management .","title":"Remove / Replace a failed disk"},{"location":"storage/rook/common-issues/#rook-ceph-toolbox-pod-not-creating-stuck","text":"Make sure that you are not using an old version of the Rook Ceph Toolbox, grab the latest manifest here (make sure to switch to the release- branch of your Rook release): https://github.com/rook/rook/blob/master/cluster/examples/kubernetes/ceph/toolbox.yaml The Rook Ceph Toolbox can only fully startup after a Ceph Cluster has at least passed the initial setup by the Rook Ceph operator. Monitor the Rook Ceph Operator logs for errors. Check the events of the Toolbox Pod using kubectl describe pod POD_NAME .","title":"Rook Ceph Toolbox Pod not Creating / Stuck"},{"location":"storage/rook/common-issues/#ceph-osd-tree-wrong-device-class","text":"Check device class, second column in ceph osd tree output. If you need to change the device class, you first must remove the current one (if it has one set): ceph osd crush rm-device-class osd.ID . Now you can set the device class for the OSD: ceph osd crush set-device-class CLASS osd.ID Default device classes (at the time of writing): hdd , ssd , nvme Source: Ceph Docs Latest - CRUSH Maps - Device Classes","title":"Ceph OSD Tree: Wrong Device Class"},{"location":"storage/rook/common-issues/#health_warn-clients-are-using-insecure-global_id-reclaim-health_warn-mons-are-allowing-insecure-global_id-reclaim","text":"Source : https://github.com/rook/rook/issues/7746 I can confirm this is happening in all clusters, whether a clean install or upgraded cluster, running at least versions: v14.2.20 , v15.2.11 or v16.2.1 . According to the CVE also previously mentioned , there is a security issue where clients need to be upgraded to the releases mentioned. Once all the clients are updated (e.g. the rook daemons and csi driver), a new setting needs to be applied to the cluster that will disable allowing the insecure mode. If you see both these health warnings, then either one of the rook or csi daemons has not been upgraded yet, or some other client is detected on the older version: 1 2 3 health: HEALTH_WARN client is using insecure global_id reclaim mon is allowing insecure global_id reclaim If you only see this one warning, then the insecure mode should be disabled: 1 2 health: HEALTH_WARN mon is allowing insecure global_id reclaim To disable the insecure mode from the toolbox after all the clients are upgraded: Make sure all clients have been upgraded, or else those clients will be blocked after this is set : 1 ceph config set mon auth_allow_insecure_global_id_reclaim false Rook could set this flag automatically after the clients have all been updated.","title":"HEALTH_WARN: clients are using insecure global_id reclaim / HEALTH_WARN: mons are allowing insecure global_id reclaim"},{"location":"storage/rook/common-issues/#check-which-object-store-is-used-by-an-osd","text":"1 2 $ ceph osd metadata 0 | grep osd_objectstore \"osd_objectstore\": \"bluestore\", To get a quick overview of the \"object stores\" ( bluestore , (don't use it) filestore ): 1 2 3 4 $ ceph osd count-metadata osd_objectstore { \"bluestore\": 6 }","title":"Check which \"Object Store\" is used by an OSD"},{"location":"storage/rook/common-issues/#persistentvolumeclaims-persistentvolumes-are-not-resized","text":"Make sure the Ceph CSI driver for the storage (block or filesystem) is running (check the logs if you are unsure as well). Check if you use a StorageClass that has allowVolumeExpansion: false : 1 2 3 4 $ kubectl get storageclasses.storage.k8s.io NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE rook-ceph-block rook-ceph.rbd.csi.ceph.com Retain Immediate false 3d21h rook-ceph-fs rook-ceph.cephfs.csi.ceph.com Retain Immediate true 3d21h To fix this simply set allowVolumeExpansion: true in the StorageClass . Below is a StorageClass with this option set, it is at the top level of the object (not in .spec or similar): 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 allowVolumeExpansion : true apiVersion : storage.k8s.io/v1 kind : StorageClass metadata : name : rook-ceph-block parameters : clusterID : rook-ceph csi.storage.k8s.io/controller-expand-secret-name : rook-csi-rbd-provisioner [ ... ] imageFeatures : layering imageFormat : \"2\" pool : replicapool provisioner : rook-ceph.rbd.csi.ceph.com reclaimPolicy : Retain volumeBindingMode : Immediate","title":"PersistentVolumeClaims / PersistentVolumes are not Resized"},{"location":"storage/rook/common-issues/#failed-to-reconcile-cluster-rook-ceph-failed-to-create-servicemonitor-the-server-could-not-find-the-requested-resource-post-servicemonitorsmonitoringcoreoscom","text":"Set .spec.monitoring.enabled to false in your CephCluster object / yaml (and apply it).","title":"[...] failed to reconcile cluster \"rook-ceph\": [...] failed to create servicemonitor. the server could not find the requested resource (post servicemonitors.monitoring.coreos.com)"},{"location":"storage/rook/common-issues/#unable-to-get-monitor-info-from-dns-srv-with-service-name-ceph-mon-cant-run-ceph-and-rbd-commands-in-the-rook-ceph-xyz-pod","text":"You can only run ceph , rbd , etc. commands in the Rook Ceph toolbox. Regarding the Ceph toolbox checkout the Rook documentation here: Rook v1.6 Docs - Ceph Toolbox .","title":"unable to get monitor info from DNS SRV with service name: ceph-mon / Can't run ceph and rbd commands in the Rook Ceph XYZ Pod"}]}